{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 01:55:51.398280: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 01:55:51.477680: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-17 01:55:51.477699: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-17 01:55:51.862562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-17 01:55:51.862620: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-17 01:55:51.862626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'tem'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 01:55:57.423430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-17 01:55:57.423472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-17 01:55:57.424087: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 103ms/step - loss: 1.7698 - val_loss: 1.1339\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 1.0576 - val_loss: 0.7953\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7769 - val_loss: 0.5888\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.6224 - val_loss: 0.5063\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5501 - val_loss: 0.4828\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5168 - val_loss: 0.4894\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5015 - val_loss: 0.4661\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4948 - val_loss: 0.4786\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4878 - val_loss: 0.4622\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4979 - val_loss: 0.4675\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4817 - val_loss: 0.4591\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4744 - val_loss: 0.4638\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4758 - val_loss: 0.4532\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4700 - val_loss: 0.4714\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4711 - val_loss: 0.4510\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4621 - val_loss: 0.5031\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4771 - val_loss: 0.4597\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4568 - val_loss: 0.4733\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4589 - val_loss: 0.4480\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4567 - val_loss: 0.4477\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4537 - val_loss: 0.4592\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4440 - val_loss: 0.4623\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4504 - val_loss: 0.4526\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4507 - val_loss: 0.4592\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4414 - val_loss: 0.4429\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4366 - val_loss: 0.4490\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4359 - val_loss: 0.4539\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4368 - val_loss: 0.4715\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4370 - val_loss: 0.4354\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4381 - val_loss: 0.4534\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4292 - val_loss: 0.4574\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4272 - val_loss: 0.4460\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4213 - val_loss: 0.4491\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4209 - val_loss: 0.5117\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4148 - val_loss: 0.4610\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4249 - val_loss: 0.4423\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4208 - val_loss: 0.4648\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4180 - val_loss: 0.4468\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4269Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4269 - val_loss: 0.4476\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 110ms/step - loss: 1.6328 - val_loss: 1.0636\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.9765 - val_loss: 0.7339\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.7280 - val_loss: 0.5918\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.5965 - val_loss: 0.4967\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5467 - val_loss: 0.4732\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.5117 - val_loss: 0.4957\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5068 - val_loss: 0.4508\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.5016 - val_loss: 0.4639\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4910 - val_loss: 0.4644\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4889 - val_loss: 0.5190\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4786 - val_loss: 0.4601\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4744 - val_loss: 0.4614\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4779 - val_loss: 0.4600\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4787 - val_loss: 0.4483\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4756 - val_loss: 0.4644\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4625 - val_loss: 0.4614\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4668 - val_loss: 0.4420\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4700 - val_loss: 0.4748\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4816 - val_loss: 0.4443\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4640 - val_loss: 0.4530\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4582 - val_loss: 0.4499\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4563 - val_loss: 0.4394\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4615 - val_loss: 0.4438\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4556 - val_loss: 0.4380\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4571 - val_loss: 0.4521\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4529 - val_loss: 0.4497\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4445 - val_loss: 0.4705\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4478 - val_loss: 0.4493\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4465 - val_loss: 0.4405\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4548 - val_loss: 0.4446\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4437 - val_loss: 0.4416\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4348 - val_loss: 0.4489\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.4411 - val_loss: 0.4455\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4373Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4373 - val_loss: 0.4507\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 110ms/step - loss: 1.7617 - val_loss: 1.1562\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 1.0633 - val_loss: 0.7806\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.7813 - val_loss: 0.5960\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.6313 - val_loss: 0.5196\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5634 - val_loss: 0.5363\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.5413 - val_loss: 0.4701\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.5012 - val_loss: 0.4817\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 0.4927 - val_loss: 0.4495\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4937 - val_loss: 0.4707\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4744 - val_loss: 0.4442\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4799 - val_loss: 0.4989\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4782 - val_loss: 0.4683\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4803 - val_loss: 0.4562\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4735 - val_loss: 0.4916\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4667 - val_loss: 0.4804\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4730 - val_loss: 0.4465\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4661 - val_loss: 0.4556\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4609 - val_loss: 0.4440\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4591 - val_loss: 0.4369\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4558 - val_loss: 0.4454\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4537 - val_loss: 0.4334\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4563 - val_loss: 0.4358\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4547 - val_loss: 0.4531\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4510 - val_loss: 0.4336\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4461 - val_loss: 0.4474\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4430 - val_loss: 0.4488\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4323 - val_loss: 0.4733\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4418 - val_loss: 0.4468\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4282 - val_loss: 0.4434\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4385 - val_loss: 0.4463\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4395Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4395 - val_loss: 0.4466\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 109ms/step - loss: 1.6913 - val_loss: 1.0607\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 1.0124 - val_loss: 0.7506\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.7500 - val_loss: 0.5728\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.6258 - val_loss: 0.5174\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5419 - val_loss: 0.4800\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5260 - val_loss: 0.4645\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5108 - val_loss: 0.5026\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4853 - val_loss: 0.5330\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4848 - val_loss: 0.4584\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4847 - val_loss: 0.4408\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4714 - val_loss: 0.4658\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4700 - val_loss: 0.4514\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4696 - val_loss: 0.4524\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4677 - val_loss: 0.4448\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4660 - val_loss: 0.4546\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4633 - val_loss: 0.4725\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4672 - val_loss: 0.4622\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4557 - val_loss: 0.4437\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4566 - val_loss: 0.4294\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4478 - val_loss: 0.4541\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4497 - val_loss: 0.4441\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4534 - val_loss: 0.4445\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4427 - val_loss: 0.4341\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4447 - val_loss: 0.4335\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4375 - val_loss: 0.4223\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4388 - val_loss: 0.4981\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4410 - val_loss: 0.4397\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4302 - val_loss: 0.4503\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4322 - val_loss: 0.4353\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4391 - val_loss: 0.4634\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4337 - val_loss: 0.4374\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4372 - val_loss: 0.4408\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4276 - val_loss: 0.4239\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4254 - val_loss: 0.4561\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4286Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4286 - val_loss: 0.4406\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 111ms/step - loss: 1.7326 - val_loss: 1.1484\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 1.0811 - val_loss: 0.8004\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7931 - val_loss: 0.6154\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.6284 - val_loss: 0.5136\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5623 - val_loss: 0.4856\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5189 - val_loss: 0.4613\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5114 - val_loss: 0.4764\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4941 - val_loss: 0.4692\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4984 - val_loss: 0.4611\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4833 - val_loss: 0.4779\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4821 - val_loss: 0.4561\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4769 - val_loss: 0.4481\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4688 - val_loss: 0.4457\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4750 - val_loss: 0.4440\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4729 - val_loss: 0.4627\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4614 - val_loss: 0.4575\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4622 - val_loss: 0.4557\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4598 - val_loss: 0.4515\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4601 - val_loss: 0.4386\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4533 - val_loss: 0.4443\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4590 - val_loss: 0.4564\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4549 - val_loss: 0.4534\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4602 - val_loss: 0.5021\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4518 - val_loss: 0.4456\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4482 - val_loss: 0.4544\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4369 - val_loss: 0.4512\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4526 - val_loss: 0.4563\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4404 - val_loss: 0.4569\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4349Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4349 - val_loss: 0.4506\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 112ms/step - loss: 1.6649 - val_loss: 1.0728\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.9775 - val_loss: 0.7228\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.7344 - val_loss: 0.5846\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5943 - val_loss: 0.5339\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5452 - val_loss: 0.4675\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5163 - val_loss: 0.4672\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.5072 - val_loss: 0.4656\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4945 - val_loss: 0.4709\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4816 - val_loss: 0.4602\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4818 - val_loss: 0.4524\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4838 - val_loss: 0.4535\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4738 - val_loss: 0.4633\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4748 - val_loss: 0.4485\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4696 - val_loss: 0.4435\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4651 - val_loss: 0.4665\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4710 - val_loss: 0.4578\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4659 - val_loss: 0.4519\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4772 - val_loss: 0.4566\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4664 - val_loss: 0.4454\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4590 - val_loss: 0.4441\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4547 - val_loss: 0.4738\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4525 - val_loss: 0.4541\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4550 - val_loss: 0.4432\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4475 - val_loss: 0.4594\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4458 - val_loss: 0.4361\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4588 - val_loss: 0.4630\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4579 - val_loss: 0.4410\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4432 - val_loss: 0.4458\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4379 - val_loss: 0.4421\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4332 - val_loss: 0.4472\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4456 - val_loss: 0.4483\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4363 - val_loss: 0.4861\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4410 - val_loss: 0.4488\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4314 - val_loss: 0.4348\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4362 - val_loss: 0.4555\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4270 - val_loss: 0.4457\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4249 - val_loss: 0.4415\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4207 - val_loss: 0.4593\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4138 - val_loss: 0.4445\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4119 - val_loss: 0.4441\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4076 - val_loss: 0.4721\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4159 - val_loss: 0.4624\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4052 - val_loss: 0.4493\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4021Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4021 - val_loss: 0.4558\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 116ms/step - loss: 1.7201 - val_loss: 1.1185\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 1.0341 - val_loss: 0.7722\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.7599 - val_loss: 0.6003\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.6139 - val_loss: 0.5098\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5523 - val_loss: 0.4954\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5200 - val_loss: 0.4940\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5020 - val_loss: 0.4619\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4973 - val_loss: 0.4886\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4822 - val_loss: 0.4726\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4814 - val_loss: 0.4575\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.4807 - val_loss: 0.4638\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4709 - val_loss: 0.4830\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4892 - val_loss: 0.4586\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4768 - val_loss: 0.4561\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4702 - val_loss: 0.4439\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4673 - val_loss: 0.4396\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4654 - val_loss: 0.4374\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4577 - val_loss: 0.4374\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4652 - val_loss: 0.4389\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4477 - val_loss: 0.4355\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.4481 - val_loss: 0.4456\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4445 - val_loss: 0.4557\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4471 - val_loss: 0.4874\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4405 - val_loss: 0.4393\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4366 - val_loss: 0.4612\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4346 - val_loss: 0.4890\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4327 - val_loss: 0.4500\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4434 - val_loss: 0.4335\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4212 - val_loss: 0.4682\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4260 - val_loss: 0.4490\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.4225 - val_loss: 0.4507\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.4344 - val_loss: 0.4517\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.4375 - val_loss: 0.4409\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4231 - val_loss: 0.4442\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4132 - val_loss: 0.5063\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4168 - val_loss: 0.4595\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4099 - val_loss: 0.4770\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3969Restoring model weights from the end of the best epoch: 28.\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3969 - val_loss: 0.4670\n",
      "Epoch 38: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 106ms/step - loss: 1.7895 - val_loss: 1.2542\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 1.1430 - val_loss: 0.8410\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.8251 - val_loss: 0.6472\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.6495 - val_loss: 0.5451\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5758 - val_loss: 0.4859\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5288 - val_loss: 0.5158\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5197 - val_loss: 0.4656\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4990 - val_loss: 0.4441\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4900 - val_loss: 0.4546\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4934 - val_loss: 0.4681\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4835 - val_loss: 0.4485\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4803 - val_loss: 0.4543\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4764 - val_loss: 0.4478\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4721 - val_loss: 0.4540\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4703 - val_loss: 0.4575\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4702 - val_loss: 0.4441\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4613 - val_loss: 0.4834\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4671 - val_loss: 0.4537\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4604 - val_loss: 0.4643\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4555 - val_loss: 0.4505\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4575 - val_loss: 0.4466\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4571 - val_loss: 0.4662\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4494 - val_loss: 0.4421\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4511 - val_loss: 0.4423\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4402 - val_loss: 0.4371\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4505 - val_loss: 0.4451\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4465 - val_loss: 0.4682\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4375 - val_loss: 0.4444\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4291 - val_loss: 0.4365\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4256 - val_loss: 0.4476\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4308 - val_loss: 0.4628\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4200 - val_loss: 0.4691\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.4199 - val_loss: 0.4731\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4265 - val_loss: 0.4736\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4201 - val_loss: 0.4532\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4170 - val_loss: 0.4731\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4090 - val_loss: 0.4621\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.4124 - val_loss: 0.4847\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4012Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4012 - val_loss: 0.4515\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 105ms/step - loss: 1.8229 - val_loss: 1.2086\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 1.1163 - val_loss: 0.8243\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.7968 - val_loss: 0.6132\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.6385 - val_loss: 0.5278\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.5579 - val_loss: 0.4805\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5186 - val_loss: 0.4607\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5052 - val_loss: 0.4743\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5007 - val_loss: 0.4593\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4983 - val_loss: 0.4587\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4891 - val_loss: 0.4573\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4823 - val_loss: 0.4721\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4912 - val_loss: 0.4621\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4753 - val_loss: 0.4562\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4823 - val_loss: 0.4480\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4692 - val_loss: 0.4366\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4707 - val_loss: 0.4657\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4763 - val_loss: 0.4491\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4614 - val_loss: 0.4587\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4592 - val_loss: 0.4666\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4545 - val_loss: 0.4421\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4500 - val_loss: 0.4356\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4500 - val_loss: 0.4521\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4424 - val_loss: 0.4506\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4439 - val_loss: 0.4451\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4348 - val_loss: 0.4700\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4377 - val_loss: 0.4539\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4323 - val_loss: 0.4452\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4286 - val_loss: 0.4512\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4337 - val_loss: 0.4379\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4240 - val_loss: 0.4342\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4266 - val_loss: 0.4632\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.4240 - val_loss: 0.4336\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.4133 - val_loss: 0.4748\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4355 - val_loss: 0.4373\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4373 - val_loss: 0.4958\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4230 - val_loss: 0.4546\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4154 - val_loss: 0.4344\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4133 - val_loss: 0.4642\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4159 - val_loss: 0.4625\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4102 - val_loss: 0.4496\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4142 - val_loss: 0.4614\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3964Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.3964 - val_loss: 0.4516\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 103ms/step - loss: 1.6460 - val_loss: 1.0642\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 1.0116 - val_loss: 0.7521\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.7544 - val_loss: 0.5818\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.6123 - val_loss: 0.5219\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5451 - val_loss: 0.4914\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.5285 - val_loss: 0.4663\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5066 - val_loss: 0.4972\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5022 - val_loss: 0.4686\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4872 - val_loss: 0.4610\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4797 - val_loss: 0.4574\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4910 - val_loss: 0.4619\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4790 - val_loss: 0.4746\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4797 - val_loss: 0.4501\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4736 - val_loss: 0.5145\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4720 - val_loss: 0.4729\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4634 - val_loss: 0.4536\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4669 - val_loss: 0.4932\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4675 - val_loss: 0.4460\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4557 - val_loss: 0.4564\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4548 - val_loss: 0.4574\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4536 - val_loss: 0.4641\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4485 - val_loss: 0.4520\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4551 - val_loss: 0.4611\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4600 - val_loss: 0.4424\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4450 - val_loss: 0.4485\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4446 - val_loss: 0.4708\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4467 - val_loss: 0.4510\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4807 - val_loss: 0.4579\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4536 - val_loss: 0.4586\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4396 - val_loss: 0.4539\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4412 - val_loss: 0.4727\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4368 - val_loss: 0.4524\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4309 - val_loss: 0.4621\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4291Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4291 - val_loss: 0.4484\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 107ms/step - loss: 610592.0625 - val_loss: 184701.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 320064.2500 - val_loss: 931533.4375\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 265759.7500 - val_loss: 108076.3047\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 217399.0469 - val_loss: 110314.9688\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 121154.4062 - val_loss: 40841.1211\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 70471.5938 - val_loss: 138514.2969\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 82296.4531 - val_loss: 248386.7031\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 115029.6641 - val_loss: 27537.5195\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 46428.2344 - val_loss: 71747.3281\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 75ms/step - loss: 34050.6602 - val_loss: 44037.6016\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 42662.6289 - val_loss: 56325.4609\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 41154.9844 - val_loss: 27033.0781\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 46677.9844 - val_loss: 18453.0840\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 27526.3281 - val_loss: 16202.9795\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 33548.6445 - val_loss: 86652.7266\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 53214.8555 - val_loss: 28198.3848\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 34368.4375 - val_loss: 12570.9961\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 15375.3047 - val_loss: 12667.6836\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 14025.3779 - val_loss: 9769.1807\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 13377.2617 - val_loss: 28857.2480\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 21402.6250 - val_loss: 32101.6719\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 25682.7930 - val_loss: 36497.1016\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 20486.3691 - val_loss: 6712.7251\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 20316.4473 - val_loss: 1761.5125\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 17713.4961 - val_loss: 18730.9473\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 17369.7520 - val_loss: 14342.1885\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 17755.7793 - val_loss: 35193.3086\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 29687.3594 - val_loss: 20758.4980\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 41536.4414 - val_loss: 30163.2695\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 33697.2031 - val_loss: 18539.5605\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 18355.4512 - val_loss: 13541.4238\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 10961.1650 - val_loss: 12854.0400\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 12933.2021 - val_loss: 6751.4795\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 7161.7495Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 7161.7495 - val_loss: 22026.4785\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 695320.3125 - val_loss: 237241.5781\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 572583.6250 - val_loss: 295525.6562\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 274677.8750 - val_loss: 158742.9219\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 243824.1406 - val_loss: 324887.4062\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 170431.9062 - val_loss: 244118.9219\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 198519.1406 - val_loss: 33169.7500\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 88455.9922 - val_loss: 35228.5625\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 93662.4609 - val_loss: 203129.5312\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 135888.5625 - val_loss: 184699.1250\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 125103.7500 - val_loss: 90528.9688\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 68214.1328 - val_loss: 114632.1016\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 80922.2266 - val_loss: 61268.1719\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 67416.2656 - val_loss: 125881.3672\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 78061.0156 - val_loss: 5142.3164\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 39476.0938 - val_loss: 96192.4453\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 37738.5664 - val_loss: 19731.0820\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33258.7500 - val_loss: 7189.9365\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 24043.5840 - val_loss: 20245.3906\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 24560.1211 - val_loss: 15579.0068\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 22506.6738 - val_loss: 46385.2734\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 28412.1016 - val_loss: 20510.7910\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 19251.4395 - val_loss: 44735.9648\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 23562.1738 - val_loss: 11995.3115\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 10881.6475Restoring model weights from the end of the best epoch: 14.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10881.6475 - val_loss: 16966.9238\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 106ms/step - loss: 582844.8750 - val_loss: 154058.6719\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 347917.5312 - val_loss: 448571.9688\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 238562.7969 - val_loss: 401657.4375\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 161303.3906 - val_loss: 88737.8438\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 54791.5117 - val_loss: 179782.5156\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 289589.7500 - val_loss: 197732.9688\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 75871.6250 - val_loss: 106706.6797\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 58523.8008 - val_loss: 52441.8789\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 89205.0703 - val_loss: 52260.9609\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 51416.5312 - val_loss: 29188.9590\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 43950.3516 - val_loss: 12195.3193\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 31202.2500 - val_loss: 16382.3193\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 34295.7930 - val_loss: 49987.4609\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 23111.8516 - val_loss: 60060.8906\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 28243.7637 - val_loss: 17536.0801\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 13701.1562 - val_loss: 68391.4062\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 39064.5312 - val_loss: 11000.5518\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 16119.9170 - val_loss: 36274.2773\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 27644.7109 - val_loss: 51655.7227\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 23060.5586 - val_loss: 25358.6113\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 14929.5850 - val_loss: 40465.3711\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 22400.2109 - val_loss: 2024.2159\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 18541.7188 - val_loss: 8076.2173\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 13698.0156 - val_loss: 13237.1699\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 16672.7852 - val_loss: 31701.9453\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 16337.1123 - val_loss: 17893.9062\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 19106.2578 - val_loss: 13807.3740\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 21190.8301 - val_loss: 13341.6133\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 12628.1885 - val_loss: 31509.2031\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 12600.3955 - val_loss: 15572.7383\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 11304.9688 - val_loss: 4098.2407\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 10577.4775Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 10577.4775 - val_loss: 23288.1836\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 105ms/step - loss: 710635.6875 - val_loss: 315380.1562\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 399404.7500 - val_loss: 158663.8281\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 248698.5156 - val_loss: 89038.9141\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 217591.2969 - val_loss: 96049.4375\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 98871.6172 - val_loss: 268632.0938\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 127827.1172 - val_loss: 29413.9863\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 57033.9219 - val_loss: 109482.2656\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 108656.3359 - val_loss: 58371.0703\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 79856.9375 - val_loss: 64304.8633\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 53891.2578 - val_loss: 138852.4688\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 90120.2109 - val_loss: 5803.4863\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 55036.7109 - val_loss: 43479.5781\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 30492.6953 - val_loss: 60114.9023\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 49810.4766 - val_loss: 47480.8789\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 65776.2344 - val_loss: 184401.1250\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 64238.7422 - val_loss: 26015.1699\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33184.9961 - val_loss: 32858.9414\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 38341.3477 - val_loss: 51437.2812\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 41197.6914 - val_loss: 11115.1973\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 22432.4766 - val_loss: 36871.4180\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 27067.9434Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 27067.9434 - val_loss: 11200.1582\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 107ms/step - loss: 1189625.7500 - val_loss: 674403.1875\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 395120.6250 - val_loss: 579058.1250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 403727.6250 - val_loss: 351918.9375\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 171938.9219 - val_loss: 80201.2422\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 171713.9844 - val_loss: 123905.4453\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 169100.4062 - val_loss: 180176.7969\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 80668.3906 - val_loss: 111612.3281\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 114260.1875 - val_loss: 171068.8750\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 137781.9219 - val_loss: 79083.6172\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 36967.4102 - val_loss: 123705.8203\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 70823.9062 - val_loss: 62791.7812\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 52013.7969 - val_loss: 117501.2656\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 60497.6836 - val_loss: 80722.6953\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 27291.9629 - val_loss: 214235.5938\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 91796.7578 - val_loss: 52754.0156\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 46461.3789 - val_loss: 28706.0938\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 63106.6992 - val_loss: 18582.4219\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 39981.0625 - val_loss: 55835.2617\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 28518.6641 - val_loss: 21597.7734\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 19886.5762 - val_loss: 61200.6055\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 31299.7539 - val_loss: 48034.5156\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 18500.9453 - val_loss: 19634.9902\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12583.4443 - val_loss: 35292.2969\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 26196.5234 - val_loss: 18252.6152\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 19801.0801 - val_loss: 134218.9375\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 30737.7051 - val_loss: 75089.9141\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 22366.7031 - val_loss: 6681.2227\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 14042.7861 - val_loss: 20477.7539\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 16771.0840 - val_loss: 30090.2070\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 13206.1670 - val_loss: 27414.6797\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 11752.2295 - val_loss: 28357.3809\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 18278.4590 - val_loss: 20516.6230\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 8435.8770 - val_loss: 20753.0059\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 16142.1904 - val_loss: 17768.1797\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 24317.4590 - val_loss: 17694.6191\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 16963.5879 - val_loss: 15716.4170\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 15508.8994Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 15508.8994 - val_loss: 26225.9023\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 106ms/step - loss: 503283.9375 - val_loss: 887140.7500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 454866.0938 - val_loss: 1001346.3125\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 320924.8125 - val_loss: 72181.6797\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 150613.3750 - val_loss: 146229.2969\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 159644.3125 - val_loss: 14120.2324\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 131864.3438 - val_loss: 80793.2344\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 64771.3203 - val_loss: 99848.2500\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 56128.6094 - val_loss: 172625.0156\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 82022.2031 - val_loss: 87523.5547\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 75034.1328 - val_loss: 49072.6016\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 30658.3398 - val_loss: 12501.8184\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 106039.1797 - val_loss: 35530.8047\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 52790.5625 - val_loss: 82450.7344\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 31829.8418 - val_loss: 53453.4102\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 34826.4023 - val_loss: 112201.5938\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 57444.2461 - val_loss: 93595.2500\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 37324.8672 - val_loss: 29104.1387\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 41871.5430 - val_loss: 8562.8623\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 25755.3262 - val_loss: 10904.3848\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 32272.1289 - val_loss: 35484.6484\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 24740.7266 - val_loss: 29015.0234\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 17336.0820 - val_loss: 16423.7852\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 34807.0586 - val_loss: 50598.0000\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 22868.8203 - val_loss: 48926.4062\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 18001.7812 - val_loss: 18889.1289\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 16332.3828 - val_loss: 53557.1094\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 29097.8613 - val_loss: 68992.8516\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 19891.3184Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 19891.3184 - val_loss: 14593.3945\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 108ms/step - loss: 680556.0625 - val_loss: 248281.4062\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 290076.1875 - val_loss: 223944.0156\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 222647.6562 - val_loss: 41105.7773\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 173254.2969 - val_loss: 494135.8750\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 254128.8281 - val_loss: 428079.0938\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 115053.3438 - val_loss: 83901.7109\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 101616.8438 - val_loss: 97090.2656\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 62332.7461 - val_loss: 73049.4297\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 77627.7891 - val_loss: 58387.7305\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 33341.7695 - val_loss: 32121.1523\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 59560.3789 - val_loss: 71455.2422\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 34893.8008 - val_loss: 37091.5977\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38836.7539 - val_loss: 69696.3828\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35017.3633 - val_loss: 19435.7578\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 17652.4297 - val_loss: 20534.8438\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 20472.6836 - val_loss: 50321.8008\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 24041.7246 - val_loss: 13732.6992\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 21307.8125 - val_loss: 9303.1836\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13489.8594 - val_loss: 21768.2070\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 19332.6953 - val_loss: 84618.1484\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 27939.7363 - val_loss: 16689.2832\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 15895.3633 - val_loss: 5938.6816\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 15445.1865 - val_loss: 12337.7939\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 18958.9648 - val_loss: 16419.3477\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 25203.2402 - val_loss: 18491.6660\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 17653.6719 - val_loss: 40198.5391\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 23575.1992 - val_loss: 30672.7109\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 35303.4336 - val_loss: 40264.2852\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 45077.4102 - val_loss: 36507.8867\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 23982.6074 - val_loss: 30832.2109\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 13251.9521 - val_loss: 5932.0908\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11784.8877 - val_loss: 19053.4023\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10563.9961 - val_loss: 12865.4902\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10711.3340 - val_loss: 34689.0430\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 13619.2500 - val_loss: 6751.0435\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 12822.2764 - val_loss: 15375.6289\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 14683.8594 - val_loss: 14641.8398\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 15992.9863 - val_loss: 6170.9395\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8364.3447 - val_loss: 8410.3467\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7114.9590 - val_loss: 8585.0859\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 4974.0815 - val_loss: 2047.4563\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 8312.7500 - val_loss: 9071.0791\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 5952.3096 - val_loss: 15052.5176\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 7464.3687 - val_loss: 13672.8496\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9079.5635 - val_loss: 3289.7275\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 5005.0459 - val_loss: 5917.2842\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7211.0547 - val_loss: 11307.4688\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10044.4844 - val_loss: 7798.5605\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14605.7617 - val_loss: 6754.1782\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13109.6230 - val_loss: 20537.2539\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 11618.5225Restoring model weights from the end of the best epoch: 41.\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 11618.5225 - val_loss: 5064.9204\n",
      "Epoch 51: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 103ms/step - loss: 628314.8125 - val_loss: 122978.5547\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 257179.6250 - val_loss: 171363.0625\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 194838.1406 - val_loss: 44197.9023\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 107253.9219 - val_loss: 232708.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 224551.1250 - val_loss: 37696.7578\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 51791.7969 - val_loss: 62367.8906\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 56108.9727 - val_loss: 184450.8750\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 84540.2109 - val_loss: 158990.4375\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 73082.6562 - val_loss: 30788.3145\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 31294.1133 - val_loss: 115343.6406\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 47682.3828 - val_loss: 22137.5547\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 56126.2344 - val_loss: 50303.5781\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 32129.4023 - val_loss: 59452.8633\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 24978.4023 - val_loss: 19550.1992\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 13633.9736 - val_loss: 22704.3730\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 42734.2773 - val_loss: 117364.2656\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 31738.7168 - val_loss: 13388.7334\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 29774.5508 - val_loss: 78869.5859\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 18557.8555 - val_loss: 39188.9609\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 38350.0156 - val_loss: 80354.1641\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 23210.9922 - val_loss: 13475.0801\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 18703.2949 - val_loss: 35824.8203\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 18654.2227 - val_loss: 12119.5273\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 12177.4385 - val_loss: 7671.8569\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 10094.6270 - val_loss: 11845.7100\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 21890.0195 - val_loss: 52497.8047\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 24835.0801 - val_loss: 24144.5957\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 13193.2988 - val_loss: 10820.0518\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 14493.6680 - val_loss: 7606.4429\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8803.6357 - val_loss: 23034.1855\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 11394.9014 - val_loss: 25701.7871\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 12586.2432 - val_loss: 53750.7773\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 36753.5664 - val_loss: 20749.2324\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 11991.8066 - val_loss: 47097.7773\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 22310.6738 - val_loss: 21913.1406\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 11104.4619 - val_loss: 20656.1699\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 10856.2109 - val_loss: 17606.6152\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 7847.9072 - val_loss: 16652.7441\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 10402.5693 - val_loss: 3845.1284\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11521.4600 - val_loss: 18753.2441\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 8558.0488 - val_loss: 8782.7969\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 5257.3198 - val_loss: 9793.9961\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 8649.2217 - val_loss: 29595.9043\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 10940.5938 - val_loss: 5707.0806\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 5346.0864 - val_loss: 10817.9023\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 7730.9688 - val_loss: 28055.7285\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 8809.6133 - val_loss: 6121.6670\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 8291.2432 - val_loss: 10175.1787\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8740.7412Restoring model weights from the end of the best epoch: 39.\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 8740.7412 - val_loss: 10147.0283\n",
      "Epoch 49: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 109ms/step - loss: 697809.1875 - val_loss: 275976.1875\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 373341.1250 - val_loss: 552193.4375\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 395516.7188 - val_loss: 256428.4062\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 118944.8203 - val_loss: 157760.4688\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 87499.8516 - val_loss: 45749.0430\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 70524.3438 - val_loss: 58204.9258\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 68108.1328 - val_loss: 11815.0303\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 29771.4355 - val_loss: 26823.9531\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 53029.9531 - val_loss: 50937.9062\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 47926.9492 - val_loss: 96704.6172\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 44527.8594 - val_loss: 102161.0000\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 28061.5918 - val_loss: 154015.4062\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 51713.2539 - val_loss: 8951.4766\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 31853.4844 - val_loss: 43525.8906\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 21655.4590 - val_loss: 15770.6514\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 17891.1660 - val_loss: 70476.2031\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 29864.2598 - val_loss: 41269.1797\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 22720.2812 - val_loss: 25687.6953\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 17355.6484 - val_loss: 28670.8008\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 19393.6309 - val_loss: 28668.2910\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 17147.7090 - val_loss: 18199.5176\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 23768.1367 - val_loss: 58419.5781\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 16799.9570Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16799.9570 - val_loss: 21825.0449\n",
      "Epoch 23: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 881848.8125 - val_loss: 534885.8125\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 404177.3750 - val_loss: 577518.8750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 236378.1562 - val_loss: 298065.5938\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 99805.8203 - val_loss: 811723.3125\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 216101.2812 - val_loss: 26173.7344\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 83831.3750 - val_loss: 44201.6172\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 48466.6562 - val_loss: 188480.4688\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 89540.1250 - val_loss: 154855.2031\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 77429.9688 - val_loss: 150236.7812\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 65203.2539 - val_loss: 112542.5078\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 66423.3359 - val_loss: 54094.5898\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 38084.8125 - val_loss: 88467.2812\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 45685.3672 - val_loss: 17105.7188\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 22317.9062 - val_loss: 46723.9375\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 25844.9473 - val_loss: 18171.3223\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 20722.7734 - val_loss: 69689.8359\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 36580.2305 - val_loss: 14981.9463\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 11268.7051 - val_loss: 10748.1426\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 21267.9121 - val_loss: 12792.9658\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 17041.2676 - val_loss: 40532.1797\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 20413.2090 - val_loss: 16877.6094\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 20545.2168 - val_loss: 10749.6289\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 13301.0186 - val_loss: 26832.6191\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 13171.8232 - val_loss: 17232.5449\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 19384.4121 - val_loss: 37614.6133\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 14688.1572 - val_loss: 36593.5078\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 13525.5488 - val_loss: 31227.7617\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 14040.7168Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 14040.7168 - val_loss: 25849.1934\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 186.7334 - val_loss: 190.3920\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 177.7125 - val_loss: 190.9779\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 177.2609 - val_loss: 190.5332\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.8076 - val_loss: 189.0215\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.6109 - val_loss: 189.2786\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 176.5782 - val_loss: 189.9502\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.4353 - val_loss: 188.7172\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.2052 - val_loss: 189.5557\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.7498 - val_loss: 188.6894\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.5103 - val_loss: 189.3885\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.0086 - val_loss: 189.6718\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.1439 - val_loss: 189.7492\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.2114 - val_loss: 190.3498\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.1698 - val_loss: 189.4332\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 176.1335 - val_loss: 190.1225\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 176.0582 - val_loss: 189.6287\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.0267 - val_loss: 190.3952\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 174.4606 - val_loss: 189.4169\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 174.2902Restoring model weights from the end of the best epoch: 9.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 174.2902 - val_loss: 189.5101\n",
      "Epoch 19: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 85.5543 - val_loss: 58.1354\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 57.4983 - val_loss: 45.2053\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 49.1092 - val_loss: 42.5796\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.6662 - val_loss: 36.1521\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 40.0069 - val_loss: 36.5442\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 38.6130 - val_loss: 33.8548\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 37.5546 - val_loss: 34.4920\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 36.6200 - val_loss: 33.1185\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.7616 - val_loss: 33.4092\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 36.2853 - val_loss: 33.3969\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.5580 - val_loss: 34.3719\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.6173 - val_loss: 32.8116\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.1990 - val_loss: 32.4587\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 35.2663 - val_loss: 34.6575\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.2753 - val_loss: 32.8534\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.0482 - val_loss: 32.6800\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8384 - val_loss: 32.6777\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 34.6689 - val_loss: 32.1961\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.4724 - val_loss: 32.1362\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.0099 - val_loss: 32.6617\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 34.2282 - val_loss: 32.2379\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.2700 - val_loss: 32.2914\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.2362 - val_loss: 32.3989\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 34.6065 - val_loss: 33.4619\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 34.1134 - val_loss: 32.7952\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.5693 - val_loss: 32.6269\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 34.6169 - val_loss: 32.6711\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.2458 - val_loss: 32.6281\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.8472Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 33.8472 - val_loss: 32.7799\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 90.6394 - val_loss: 67.5068\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 72.9331 - val_loss: 61.5036\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 64.8638 - val_loss: 48.5760\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 48.0972 - val_loss: 39.1392\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 40.9965 - val_loss: 35.1686\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 39.1889 - val_loss: 33.9662\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 37.9061 - val_loss: 34.1567\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 36.8078 - val_loss: 32.6461\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.2349 - val_loss: 32.5633\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.8431 - val_loss: 32.7296\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.8247 - val_loss: 33.3797\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.6311 - val_loss: 33.0455\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.4036 - val_loss: 31.7854\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.9764 - val_loss: 32.4014\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.5413 - val_loss: 32.3775\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.4908 - val_loss: 32.0897\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8111 - val_loss: 32.4844\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 36.2519 - val_loss: 33.7966\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.6931 - val_loss: 33.9308\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8673 - val_loss: 31.8671\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.7485 - val_loss: 32.9274\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.3343 - val_loss: 31.4254\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.0876 - val_loss: 32.0424\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.5715 - val_loss: 33.2856\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.0845 - val_loss: 32.3877\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.6738 - val_loss: 32.3950\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.5220 - val_loss: 33.4933\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.6696 - val_loss: 31.7426\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.6815 - val_loss: 32.0074\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.7499 - val_loss: 31.3665\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.1282 - val_loss: 32.1431\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.2148 - val_loss: 33.0394\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.6699 - val_loss: 33.6366\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.0995 - val_loss: 32.5106\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.3478 - val_loss: 31.9276\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.6028 - val_loss: 31.7707\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.3562 - val_loss: 32.6835\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.7364 - val_loss: 31.5763\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.4049 - val_loss: 31.0820\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.1792 - val_loss: 31.8449\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.4791 - val_loss: 31.7776\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.8088 - val_loss: 32.8631\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5586 - val_loss: 32.6121\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 33.5983 - val_loss: 31.9137\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.4715 - val_loss: 32.4079\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5906 - val_loss: 32.5832\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5709 - val_loss: 31.5783\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.1795 - val_loss: 30.8480\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.3425 - val_loss: 31.7236\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.4028 - val_loss: 31.8138\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.1413 - val_loss: 33.0293\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.0585 - val_loss: 32.0408\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.0399 - val_loss: 31.8398\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.6772 - val_loss: 31.8898\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.1393 - val_loss: 31.8017\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 33.1435 - val_loss: 31.4262\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.0446 - val_loss: 31.6799\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.0292Restoring model weights from the end of the best epoch: 48.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.0292 - val_loss: 31.6751\n",
      "Epoch 58: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 189.3027 - val_loss: 193.2064\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 181.8638 - val_loss: 145.1211\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 83.7613 - val_loss: 57.4420\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 52.3715 - val_loss: 50.7124\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 48.1046 - val_loss: 48.4489\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 46.6694 - val_loss: 48.5390\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 46.2661 - val_loss: 49.1881\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 45.8575 - val_loss: 47.5875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 45.5683 - val_loss: 47.2017\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.9611 - val_loss: 46.6966\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.7834 - val_loss: 47.0864\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 45.1064 - val_loss: 46.6480\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.6107 - val_loss: 47.1849\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.5187 - val_loss: 47.0884\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.6654 - val_loss: 46.7411\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 44.7497 - val_loss: 47.2239\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.3082 - val_loss: 46.6653\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.2352 - val_loss: 46.8840\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.4469 - val_loss: 46.7856\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 44.1848 - val_loss: 46.8274\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.1608 - val_loss: 46.6930\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.2036Restoring model weights from the end of the best epoch: 12.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 44.2036 - val_loss: 46.6849\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 187.6235 - val_loss: 188.0084\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 182.2438 - val_loss: 189.9893\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 181.3188 - val_loss: 188.4177\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180.6543 - val_loss: 188.8433\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 178.0618 - val_loss: 189.9920\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 176.6879 - val_loss: 189.1789\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 176.3417 - val_loss: 190.0824\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 177.8651 - val_loss: 188.3315\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 179.8111 - val_loss: 188.7592\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 179.7115 - val_loss: 188.7921\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 177.3290Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 177.3290 - val_loss: 190.4141\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 186.2569 - val_loss: 188.4527\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 182.6284 - val_loss: 189.2862\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 181.5237 - val_loss: 188.9244\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180.9736 - val_loss: 189.4283\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180.8064 - val_loss: 189.2638\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180.5245 - val_loss: 189.4243\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 180.4250 - val_loss: 188.4422\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 177.1040 - val_loss: 191.4477\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 176.9138 - val_loss: 190.6359\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.5356 - val_loss: 189.9224\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.3526 - val_loss: 189.9092\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.3062 - val_loss: 189.9564\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 176.6810 - val_loss: 190.7910\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.4168 - val_loss: 189.9503\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 176.4380 - val_loss: 189.7175\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 175.6989 - val_loss: 189.4049\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 113.7453 - val_loss: 63.0846\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 58.0034 - val_loss: 55.8300\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 138.1694 - val_loss: 183.1169\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 165.5685 - val_loss: 181.5877\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 165.0658 - val_loss: 181.9835\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 164.5873 - val_loss: 180.1971\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 162.2060 - val_loss: 180.2849\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 86.9231 - val_loss: 51.8768\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 48.2566 - val_loss: 49.8102\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 46.5513 - val_loss: 47.8771\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 45.5969 - val_loss: 47.6618\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.9550 - val_loss: 47.3434\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 45.1596 - val_loss: 47.2392\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.6442 - val_loss: 47.2270\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.8972 - val_loss: 47.7582\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.3994 - val_loss: 47.5927\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.7586 - val_loss: 48.2974\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.5360 - val_loss: 47.4697\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.5114 - val_loss: 47.4728\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.4260 - val_loss: 47.5143\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 44.4706 - val_loss: 47.5187\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.1462 - val_loss: 47.3821\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.4359 - val_loss: 46.9988\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.5901 - val_loss: 47.1243\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.3725 - val_loss: 47.1387\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.4058 - val_loss: 47.2017\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.1977 - val_loss: 46.9653\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.1420 - val_loss: 47.2708\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.2311 - val_loss: 47.6503\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.3579 - val_loss: 46.9359\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 43.8759 - val_loss: 47.3702\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 44.0223 - val_loss: 47.4033\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.1914 - val_loss: 47.5107\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 43.9822 - val_loss: 47.4636\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.0514 - val_loss: 47.0565\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.1385 - val_loss: 47.2705\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 43.9190 - val_loss: 47.0861\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 43.8750 - val_loss: 47.4149\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 43.7689 - val_loss: 47.3856\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.0844Restoring model weights from the end of the best epoch: 46.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.0844 - val_loss: 47.5602\n",
      "Epoch 56: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 80.5825 - val_loss: 53.5056\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 52.0159 - val_loss: 41.9592\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 44.4097 - val_loss: 37.0976\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 40.4010 - val_loss: 35.9159\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 38.4120 - val_loss: 34.3427\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 37.3023 - val_loss: 33.3458\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.0345 - val_loss: 33.7453\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.0760 - val_loss: 32.8102\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.6717 - val_loss: 32.4882\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.5520 - val_loss: 32.5437\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.4070 - val_loss: 33.7836\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.7753 - val_loss: 33.0268\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2314 - val_loss: 32.7606\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.0031 - val_loss: 33.1543\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.0046 - val_loss: 33.7914\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.3865 - val_loss: 32.8687\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.1982 - val_loss: 32.7187\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.6856 - val_loss: 32.9494\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.6981 - val_loss: 32.1560\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.3726 - val_loss: 32.9907\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.5325 - val_loss: 32.3677\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4042 - val_loss: 32.8675\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.3229 - val_loss: 32.4609\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.1602 - val_loss: 32.2242\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.0834 - val_loss: 32.4287\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.9873 - val_loss: 34.6495\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4773 - val_loss: 32.3469\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4803 - val_loss: 32.5925\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 34.2677Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.2677 - val_loss: 32.4309\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 88.6425 - val_loss: 57.9374\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 57.2345 - val_loss: 45.3803\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 47.5258 - val_loss: 40.3772\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 42.0669 - val_loss: 37.6310\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 40.3491 - val_loss: 37.3096\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 38.6386 - val_loss: 34.6250\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 37.4241 - val_loss: 34.5408\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.4937 - val_loss: 33.6919\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.7049 - val_loss: 33.0743\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 35.7912 - val_loss: 32.9424\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.5549 - val_loss: 32.9106\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.0789 - val_loss: 34.3530\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.6871 - val_loss: 33.8815\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.7900 - val_loss: 32.7339\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8577 - val_loss: 32.6398\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.7334 - val_loss: 32.7060\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.9147 - val_loss: 32.6104\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.5109 - val_loss: 33.0609\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.7179 - val_loss: 33.7669\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.9546 - val_loss: 32.9295\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.0670 - val_loss: 32.5902\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.7399 - val_loss: 33.0980\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.9088 - val_loss: 32.5870\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.3558 - val_loss: 32.9225\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.2150 - val_loss: 32.1937\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.4138 - val_loss: 33.9811\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.7140 - val_loss: 33.2190\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.0792 - val_loss: 32.6317\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 34.2082 - val_loss: 32.2506\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.8505 - val_loss: 32.8631\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.2743 - val_loss: 33.3941\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.1581 - val_loss: 32.5424\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.0901 - val_loss: 32.2710\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.8860 - val_loss: 31.9377\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5016 - val_loss: 32.4768\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.4302 - val_loss: 32.4155\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.1598 - val_loss: 32.9095\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 33.0353 - val_loss: 32.5674\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.6581 - val_loss: 33.2021\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5358 - val_loss: 32.8467\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 32.9008 - val_loss: 32.2763\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 32.8167 - val_loss: 32.4723\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 32.8176 - val_loss: 32.3598\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32.8201 - val_loss: 31.6560\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32.5825 - val_loss: 31.8724\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.5382 - val_loss: 33.1556\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 32.5817 - val_loss: 32.8757\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32.7716 - val_loss: 32.6781\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 32.7248 - val_loss: 32.8739\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32.9246 - val_loss: 32.7875\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 32.2805 - val_loss: 33.2912\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 32.8587 - val_loss: 34.4485\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32.4052 - val_loss: 32.5883\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 32.6533Restoring model weights from the end of the best epoch: 44.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 32.6533 - val_loss: 35.2281\n",
      "Epoch 54: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 90.7044 - val_loss: 60.8465\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 58.9558 - val_loss: 49.4256\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 49.3245 - val_loss: 42.8067\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 43.7512 - val_loss: 37.5541\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 40.8310 - val_loss: 35.2612\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 39.0917 - val_loss: 34.3029\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 37.6151 - val_loss: 33.6772\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 36.8750 - val_loss: 33.8005\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.4021 - val_loss: 33.3094\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.9549 - val_loss: 32.9731\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.6032 - val_loss: 33.0223\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.4327 - val_loss: 32.6294\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.2123 - val_loss: 33.5545\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.3875 - val_loss: 33.1607\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.3876 - val_loss: 32.8920\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.1111 - val_loss: 32.9717\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.9721 - val_loss: 33.5695\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 35.0368 - val_loss: 32.4336\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.9420 - val_loss: 33.1841\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4057 - val_loss: 32.1706\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.0267 - val_loss: 32.5675\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4233 - val_loss: 32.5394\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.4582 - val_loss: 33.5995\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.1420 - val_loss: 33.2317\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.2352 - val_loss: 31.9557\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4006 - val_loss: 32.2341\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.9590 - val_loss: 32.3106\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.2955 - val_loss: 33.0354\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.0802 - val_loss: 32.3082\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.7453 - val_loss: 33.0154\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.9811 - val_loss: 32.4365\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.8907 - val_loss: 32.4686\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1137 - val_loss: 32.5754\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.7237 - val_loss: 32.2591\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.5012Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.5012 - val_loss: 32.7655\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 83.3267 - val_loss: 53.6870\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 52.2225 - val_loss: 41.6754\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 43.7677 - val_loss: 37.2291\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 40.2285 - val_loss: 35.1280\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 38.1294 - val_loss: 33.4570\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 36.8969 - val_loss: 33.4164\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.3409 - val_loss: 33.0347\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 35.9787 - val_loss: 33.5311\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.6156 - val_loss: 32.8648\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 35.6982 - val_loss: 35.0968\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.3217 - val_loss: 33.5957\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.6949 - val_loss: 32.5958\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.2039 - val_loss: 32.5921\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34.8834 - val_loss: 32.8851\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.1651 - val_loss: 32.3853\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.8453 - val_loss: 32.7143\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.7029 - val_loss: 32.4196\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.3941 - val_loss: 34.7800\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 35.0563 - val_loss: 32.6463\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.9127 - val_loss: 32.7652\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.4309 - val_loss: 32.2043\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.4680 - val_loss: 32.1954\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.0935 - val_loss: 32.8106\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34.1279 - val_loss: 33.2198\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.1448 - val_loss: 31.9510\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.0994 - val_loss: 33.4784\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34.1167 - val_loss: 31.8396\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.7470 - val_loss: 32.1406\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.9440 - val_loss: 33.4559\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.5916 - val_loss: 31.6229\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 33.3778 - val_loss: 32.3656\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.2985 - val_loss: 32.3954\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.8046 - val_loss: 32.3347\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.1235 - val_loss: 32.9938\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.8387 - val_loss: 32.3516\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.2287 - val_loss: 32.2468\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 32.5031 - val_loss: 32.0637\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 33.1371 - val_loss: 32.7578\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.7186 - val_loss: 32.4320\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 32.8984Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.8984 - val_loss: 31.9115\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 8.1280 - val_loss: 5.0824\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.8053 - val_loss: 3.6477\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.6059 - val_loss: 2.8738\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.0636 - val_loss: 2.6587\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.8050 - val_loss: 2.6143\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.6666 - val_loss: 2.4124\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5800 - val_loss: 2.4430\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.6063 - val_loss: 2.3818\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5450 - val_loss: 2.3389\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5078 - val_loss: 2.3901\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5105 - val_loss: 2.3239\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5109 - val_loss: 2.5846\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5212 - val_loss: 2.4375\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4535 - val_loss: 2.3986\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4268 - val_loss: 2.3428\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4165 - val_loss: 2.4258\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4138 - val_loss: 2.3938\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3945 - val_loss: 2.2859\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4027 - val_loss: 2.2983\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3889 - val_loss: 2.3860\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3754 - val_loss: 2.4410\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3765 - val_loss: 2.4835\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4242 - val_loss: 2.2863\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3807 - val_loss: 2.2884\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3499 - val_loss: 2.3648\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3930 - val_loss: 2.3438\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3402 - val_loss: 2.3541\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2743Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2743 - val_loss: 2.3384\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 8.2132 - val_loss: 5.1987\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.9069 - val_loss: 3.6745\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.6197 - val_loss: 3.0523\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.1143 - val_loss: 2.5570\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.7711 - val_loss: 2.4875\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.6683 - val_loss: 2.5011\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.6300 - val_loss: 2.3294\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 2.6005 - val_loss: 2.3680\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.5322 - val_loss: 2.3865\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.5006 - val_loss: 2.3625\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.4603 - val_loss: 2.2955\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.4500 - val_loss: 2.2989\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.4728 - val_loss: 2.3579\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.3978 - val_loss: 2.3261\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.4232 - val_loss: 2.3614\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.4105 - val_loss: 2.4529\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.4003 - val_loss: 2.3095\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.3789 - val_loss: 2.2647\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3314 - val_loss: 2.3416\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.3597 - val_loss: 2.2276\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.3081 - val_loss: 2.4986\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.3190 - val_loss: 2.4181\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.3200 - val_loss: 2.3180\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.2870 - val_loss: 2.4121\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.3161 - val_loss: 2.2513\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.2307 - val_loss: 2.3617\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.2402 - val_loss: 2.3027\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.3559 - val_loss: 2.2930\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.2206 - val_loss: 2.3726\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2013Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.2013 - val_loss: 2.3355\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 8.7581 - val_loss: 5.8070\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.3947 - val_loss: 4.4549\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 4.0127 - val_loss: 3.3242\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.2717 - val_loss: 2.7238\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.9027 - val_loss: 2.5040\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.7235 - val_loss: 2.7080\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.6159 - val_loss: 2.3942\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5886 - val_loss: 2.4826\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.6644 - val_loss: 2.4535\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.5149 - val_loss: 2.3619\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5235 - val_loss: 2.3690\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4421 - val_loss: 2.5282\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4777 - val_loss: 2.6209\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4937 - val_loss: 2.3448\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4289 - val_loss: 2.3713\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4007 - val_loss: 2.3038\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3987 - val_loss: 2.3572\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3775 - val_loss: 2.3841\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4485 - val_loss: 2.4596\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4330 - val_loss: 2.3651\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3718 - val_loss: 2.3152\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3454 - val_loss: 2.3275\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3828 - val_loss: 2.4409\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3753 - val_loss: 2.3442\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 2.3395 - val_loss: 2.3896\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3466 - val_loss: 2.2946\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3034 - val_loss: 2.2450\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3058 - val_loss: 2.2980\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2684 - val_loss: 2.3365\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2840 - val_loss: 2.3091\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2513 - val_loss: 2.2944\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2867 - val_loss: 2.3415\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2554 - val_loss: 2.5400\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2632 - val_loss: 2.3392\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2133 - val_loss: 2.3007\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2410 - val_loss: 2.3797\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2261Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2261 - val_loss: 2.4458\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 8.6616 - val_loss: 5.6304\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 5.2117 - val_loss: 3.8102\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.8279 - val_loss: 3.1462\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.2017 - val_loss: 2.7570\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.8286 - val_loss: 2.4710\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.7225 - val_loss: 2.5738\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.6243 - val_loss: 2.6664\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.6411 - val_loss: 2.6417\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.5759 - val_loss: 2.3864\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.5257 - val_loss: 2.4386\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.4925 - val_loss: 2.3817\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.4908 - val_loss: 2.3584\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.4451 - val_loss: 2.3089\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.4598 - val_loss: 2.7129\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.4703 - val_loss: 2.4231\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.4304 - val_loss: 2.3668\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.4337 - val_loss: 2.4400\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3837 - val_loss: 2.3523\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3374 - val_loss: 2.2568\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3242 - val_loss: 2.2910\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3281 - val_loss: 2.2563\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3601 - val_loss: 2.3399\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.3346 - val_loss: 2.2474\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3019 - val_loss: 2.3296\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3166 - val_loss: 2.2101\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2866 - val_loss: 2.2287\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.2679 - val_loss: 2.2580\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2601 - val_loss: 2.2410\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2377 - val_loss: 2.3664\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.2254 - val_loss: 2.3337\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2241 - val_loss: 2.4181\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2837 - val_loss: 2.2771\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.1658 - val_loss: 2.2723\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.1991 - val_loss: 2.4114\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2182Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.2182 - val_loss: 2.2820\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 8.9243 - val_loss: 5.8096\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4505 - val_loss: 4.0198\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.0163 - val_loss: 3.1849\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2375 - val_loss: 2.6509\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.8627 - val_loss: 2.4532\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.6954 - val_loss: 2.6409\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.6918 - val_loss: 2.3812\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5572 - val_loss: 2.3823\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.5791 - val_loss: 2.4601\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5548 - val_loss: 2.4377\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4750 - val_loss: 2.3544\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4860 - val_loss: 2.3489\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4640 - val_loss: 2.7517\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.6200 - val_loss: 2.3997\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4703 - val_loss: 2.4591\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4707 - val_loss: 2.3703\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.4133 - val_loss: 2.4053\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4190 - val_loss: 2.6841\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5030 - val_loss: 2.2965\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4286 - val_loss: 2.4417\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4599 - val_loss: 2.3090\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4127 - val_loss: 2.2780\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3840 - val_loss: 2.3104\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3509 - val_loss: 2.3668\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3554 - val_loss: 2.4889\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3575 - val_loss: 2.2938\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.3258 - val_loss: 2.3050\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3126 - val_loss: 2.3754\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.2743 - val_loss: 2.3810\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2767 - val_loss: 2.3491\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2394 - val_loss: 2.2857\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.3259Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3259 - val_loss: 2.3205\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 8.7985 - val_loss: 5.8604\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 5.4524 - val_loss: 4.1223\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 3.9906 - val_loss: 3.1843\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.2428 - val_loss: 2.8752\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.9088 - val_loss: 2.7909\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.7516 - val_loss: 2.6270\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.6644 - val_loss: 2.5056\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.6051 - val_loss: 2.6712\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.5586 - val_loss: 2.4898\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.5141 - val_loss: 2.4133\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4997 - val_loss: 2.4585\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4965 - val_loss: 2.3208\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4590 - val_loss: 2.3173\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4035 - val_loss: 2.4489\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4433 - val_loss: 2.6243\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4223 - val_loss: 2.4545\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3875 - val_loss: 2.3253\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4304 - val_loss: 2.3173\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3745 - val_loss: 2.2961\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4081 - val_loss: 2.3959\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3996 - val_loss: 2.3300\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3874 - val_loss: 2.3514\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3462 - val_loss: 2.3295\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3055 - val_loss: 2.2996\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3075 - val_loss: 2.3514\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2640 - val_loss: 2.3720\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2720 - val_loss: 2.3187\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2389 - val_loss: 2.6172\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3192 - val_loss: 2.2502\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2523 - val_loss: 2.2844\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2256 - val_loss: 2.3147\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2421 - val_loss: 2.3990\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2386 - val_loss: 2.4056\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1699 - val_loss: 2.2776\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2205 - val_loss: 2.3185\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.1731 - val_loss: 2.4972\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1300 - val_loss: 2.2952\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1743 - val_loss: 2.3083\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1058Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1058 - val_loss: 2.3865\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 8.7357 - val_loss: 5.4868\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.1764 - val_loss: 3.8051\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.8223 - val_loss: 3.0030\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 3.1296 - val_loss: 2.6655\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.8089 - val_loss: 2.5656\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.7170 - val_loss: 2.7456\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.6291 - val_loss: 2.5984\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.5869 - val_loss: 2.3434\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5117 - val_loss: 2.4385\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5716 - val_loss: 2.4254\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5232 - val_loss: 2.3727\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5174 - val_loss: 2.5205\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5046 - val_loss: 2.3314\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.4405 - val_loss: 2.3387\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.4032 - val_loss: 2.3290\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4265 - val_loss: 2.4536\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5072 - val_loss: 2.6335\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4215 - val_loss: 2.3317\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3991 - val_loss: 2.3427\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3932 - val_loss: 2.3048\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3868 - val_loss: 2.5085\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3981 - val_loss: 2.3546\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3120 - val_loss: 2.3844\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3398 - val_loss: 2.2643\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3278 - val_loss: 2.4550\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3548 - val_loss: 2.5023\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3437 - val_loss: 2.3286\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2917 - val_loss: 2.3006\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3214 - val_loss: 2.2835\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3066 - val_loss: 2.3118\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2851 - val_loss: 2.3813\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2170 - val_loss: 2.3547\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2692 - val_loss: 2.2530\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.2280 - val_loss: 2.3231\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.1941 - val_loss: 2.3060\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.1773 - val_loss: 2.3047\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1750 - val_loss: 2.3418\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1228 - val_loss: 2.4611\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.1496 - val_loss: 2.5277\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1382 - val_loss: 2.3099\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1784 - val_loss: 2.3844\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.0997 - val_loss: 2.3894\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.0913Restoring model weights from the end of the best epoch: 33.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.0913 - val_loss: 2.3167\n",
      "Epoch 43: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 8.9824 - val_loss: 5.8577\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4584 - val_loss: 4.1811\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.9972 - val_loss: 3.1342\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.2899 - val_loss: 2.6965\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.8717 - val_loss: 2.6972\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.6820 - val_loss: 2.4151\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.6015 - val_loss: 2.4106\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.5989 - val_loss: 2.3640\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.5108 - val_loss: 2.4731\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5297 - val_loss: 2.3966\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.5597 - val_loss: 2.4629\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4930 - val_loss: 2.3989\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4782 - val_loss: 2.3624\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4588 - val_loss: 2.4162\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4711 - val_loss: 2.5370\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3884 - val_loss: 2.3836\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.4065 - val_loss: 2.3071\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3937 - val_loss: 2.3989\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3941 - val_loss: 2.5336\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4037 - val_loss: 2.3980\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3763 - val_loss: 2.4042\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3734 - val_loss: 2.3977\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3286 - val_loss: 2.4470\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3454 - val_loss: 2.2961\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.3271 - val_loss: 2.3419\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.2798 - val_loss: 2.3756\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2909 - val_loss: 2.3020\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3067 - val_loss: 2.2840\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2955 - val_loss: 2.2811\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2469 - val_loss: 2.3196\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2213 - val_loss: 2.3398\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2481 - val_loss: 2.4232\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2170 - val_loss: 2.2613\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2002 - val_loss: 2.2952\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1910 - val_loss: 2.3632\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.1449 - val_loss: 2.3340\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.2065 - val_loss: 2.3084\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.1417 - val_loss: 2.4126\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2062 - val_loss: 2.4942\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1469 - val_loss: 2.4804\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1141 - val_loss: 2.5385\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1496 - val_loss: 2.3746\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1245Restoring model weights from the end of the best epoch: 33.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1245 - val_loss: 2.3231\n",
      "Epoch 43: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 8.9500 - val_loss: 5.9203\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 5.6032 - val_loss: 4.2129\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 4.1857 - val_loss: 3.2414\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.3051 - val_loss: 2.6582\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.8991 - val_loss: 2.5489\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.7195 - val_loss: 2.4975\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.7149 - val_loss: 2.4103\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.6084 - val_loss: 2.4002\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.5623 - val_loss: 2.7196\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.5849 - val_loss: 2.3345\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4743 - val_loss: 2.3918\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4640 - val_loss: 2.3779\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4624 - val_loss: 2.6370\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4829 - val_loss: 2.3158\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4152 - val_loss: 2.3424\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.4378 - val_loss: 2.2992\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3838 - val_loss: 2.3763\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3595 - val_loss: 2.3001\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3863 - val_loss: 2.3239\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3913 - val_loss: 2.2807\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3484 - val_loss: 2.3947\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.3587 - val_loss: 2.3520\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2970 - val_loss: 2.5706\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3489 - val_loss: 2.3078\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3054 - val_loss: 2.2619\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3326 - val_loss: 2.2851\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.3356 - val_loss: 2.2891\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2806 - val_loss: 2.2513\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2741 - val_loss: 2.4232\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2753 - val_loss: 2.2334\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.2550 - val_loss: 2.2313\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2269 - val_loss: 2.1606\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2278 - val_loss: 2.6259\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2469 - val_loss: 2.2560\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2401 - val_loss: 2.3291\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2464 - val_loss: 2.2581\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2129 - val_loss: 2.2932\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.1815 - val_loss: 2.3662\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.1863 - val_loss: 2.2830\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2071 - val_loss: 2.2682\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1323 - val_loss: 2.3310\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1142Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1142 - val_loss: 2.3125\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 102ms/step - loss: 9.0123 - val_loss: 6.2150\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 5.6576 - val_loss: 4.2886\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 4.1220 - val_loss: 3.2999\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.3235 - val_loss: 2.7276\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.9003 - val_loss: 2.5086\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.7587 - val_loss: 2.4040\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.6430 - val_loss: 2.4359\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.6234 - val_loss: 2.4343\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.5285 - val_loss: 2.3757\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.5317 - val_loss: 2.3258\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.5091 - val_loss: 2.3216\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.5300 - val_loss: 2.3974\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.4721 - val_loss: 2.4377\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 2.4582 - val_loss: 2.3920\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.4311 - val_loss: 2.3096\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.4306 - val_loss: 2.3484\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.4042 - val_loss: 2.3145\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.3686 - val_loss: 2.3892\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3662 - val_loss: 2.2795\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.4068 - val_loss: 2.4057\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3839 - val_loss: 2.2680\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3986 - val_loss: 2.3202\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3342 - val_loss: 2.3634\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3413 - val_loss: 2.3113\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.2983 - val_loss: 2.3373\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.2914 - val_loss: 2.2877\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.2685 - val_loss: 2.4049\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.2956 - val_loss: 2.3047\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 2.3351 - val_loss: 2.3647\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.3322 - val_loss: 2.4404\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2759Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.2759 - val_loss: 2.3963\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 111.3141 - val_loss: 47.5728\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 45.4292 - val_loss: 27.3282\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 28.5345 - val_loss: 18.0930\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 20.2794 - val_loss: 15.1203\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 16.6473 - val_loss: 13.1122\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 15.2020 - val_loss: 13.0204\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14.0350 - val_loss: 11.4401\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.2609 - val_loss: 11.3392\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.3912 - val_loss: 10.9843\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 12.3020 - val_loss: 10.4377\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.6852 - val_loss: 10.1587\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.2369 - val_loss: 10.3280\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.9420 - val_loss: 11.0893\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.7204 - val_loss: 10.6021\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.8421 - val_loss: 9.6914\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.5627 - val_loss: 9.8465\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.2950 - val_loss: 11.5021\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.1284 - val_loss: 10.5055\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.0109 - val_loss: 10.2725\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9418 - val_loss: 9.6262\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.6518 - val_loss: 9.3209\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.8501 - val_loss: 9.4867\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.0216 - val_loss: 9.9803\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.6280 - val_loss: 10.9934\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5564 - val_loss: 10.1394\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.5174 - val_loss: 9.7622\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.3026 - val_loss: 9.9499\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.3361 - val_loss: 9.7637\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.3509 - val_loss: 9.3448\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1854 - val_loss: 9.8641\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.8411Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.8411 - val_loss: 9.5528\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 129.0344 - val_loss: 57.9993\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 54.2957 - val_loss: 32.4199\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.2955 - val_loss: 20.3210\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 22.1367 - val_loss: 15.5491\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 17.7458 - val_loss: 14.9942\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 15.2080 - val_loss: 11.7016\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.6543 - val_loss: 10.9357\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.9899 - val_loss: 10.9939\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.5360 - val_loss: 15.6522\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.4777 - val_loss: 10.5510\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.6701 - val_loss: 10.5168\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.7492 - val_loss: 11.2318\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.2113 - val_loss: 10.8403\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.9165 - val_loss: 9.9943\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.9032 - val_loss: 10.0483\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.8716 - val_loss: 9.7335\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.4662 - val_loss: 10.7778\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2616 - val_loss: 9.4579\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.2157 - val_loss: 11.3550\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2336 - val_loss: 10.3887\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.1650 - val_loss: 9.2968\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.5993 - val_loss: 8.8315\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.5976 - val_loss: 9.5219\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.6681 - val_loss: 11.0206\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.7248 - val_loss: 8.9660\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.3581 - val_loss: 9.8050\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.4126 - val_loss: 9.4800\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.3316 - val_loss: 9.5727\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1022 - val_loss: 10.1388\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.0974 - val_loss: 9.2206\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1596 - val_loss: 10.8799\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.3871Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.3871 - val_loss: 9.5447\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 129.9468 - val_loss: 62.3425\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 58.4573 - val_loss: 34.9524\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 35.9443 - val_loss: 22.0784\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 24.9543 - val_loss: 17.1985\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 19.1485 - val_loss: 15.4257\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 16.1909 - val_loss: 12.7330\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 14.3407 - val_loss: 12.9999\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 13.5664 - val_loss: 11.8754\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 12.8664 - val_loss: 10.7991\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 12.2390 - val_loss: 12.1219\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 12.3568 - val_loss: 10.3954\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.8942 - val_loss: 10.0283\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.2925 - val_loss: 10.6101\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.1552 - val_loss: 10.2273\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.8958 - val_loss: 10.5669\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.7167 - val_loss: 11.6219\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 10.5656 - val_loss: 9.5740\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 10.8548 - val_loss: 10.3385\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.2437 - val_loss: 9.7522\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.2799 - val_loss: 9.4877\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.1010 - val_loss: 9.6554\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 9.8921 - val_loss: 9.7346\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.2303 - val_loss: 11.0659\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.1396 - val_loss: 10.2194\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.6800 - val_loss: 9.0604\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.8173 - val_loss: 10.4209\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.7307 - val_loss: 10.4660\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.5659 - val_loss: 9.6996\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.3376 - val_loss: 11.2545\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.2105 - val_loss: 9.2379\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.3177 - val_loss: 10.2056\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.1325 - val_loss: 9.9084\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.6610 - val_loss: 8.8702\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.5210 - val_loss: 9.2094\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.2585 - val_loss: 9.9445\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.8219 - val_loss: 10.0299\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.7034 - val_loss: 9.7333\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.2962 - val_loss: 9.4471\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.4841 - val_loss: 9.7557\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 7.9704 - val_loss: 10.3834\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.5987 - val_loss: 10.6313\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.5799 - val_loss: 9.6243\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.9059Restoring model weights from the end of the best epoch: 33.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.9059 - val_loss: 9.8988\n",
      "Epoch 43: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 148.7419 - val_loss: 68.1945\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 61.6468 - val_loss: 36.8784\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 36.8804 - val_loss: 22.8875\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 25.3080 - val_loss: 16.9972\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 19.2912 - val_loss: 14.9795\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 17.2205 - val_loss: 11.9613\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 14.1525 - val_loss: 12.3830\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 13.5397 - val_loss: 11.8779\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.7835 - val_loss: 11.4231\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.3890 - val_loss: 10.4225\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.8017 - val_loss: 12.4235\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11.5986 - val_loss: 10.1579\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.6890 - val_loss: 11.0557\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.8965 - val_loss: 9.6702\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.8589 - val_loss: 9.9685\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.8152 - val_loss: 10.4166\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.8132 - val_loss: 10.1219\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.3909 - val_loss: 10.2100\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.3473 - val_loss: 9.2902\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0141 - val_loss: 9.8322\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.8759 - val_loss: 12.6843\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0652 - val_loss: 11.2164\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.3467 - val_loss: 9.4048\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.8075 - val_loss: 13.1524\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.4820 - val_loss: 9.5828\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9465 - val_loss: 10.2663\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.9493 - val_loss: 10.0281\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.4893 - val_loss: 10.7145\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.3816 - val_loss: 9.1577\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.2978 - val_loss: 9.1035\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.3068 - val_loss: 10.2548\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1523 - val_loss: 11.2473\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.4010 - val_loss: 9.6459\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.5169 - val_loss: 8.8406\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0267 - val_loss: 11.2136\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.9019 - val_loss: 10.0805\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.7519 - val_loss: 9.2610\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.3511 - val_loss: 10.3736\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0594 - val_loss: 9.0235\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.7711 - val_loss: 11.3208\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.4686 - val_loss: 8.9908\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.7538 - val_loss: 9.7801\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.4095 - val_loss: 11.1027\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.6993Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.6993 - val_loss: 10.4613\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 130.2036 - val_loss: 62.1348\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 58.6045 - val_loss: 34.9376\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35.7571 - val_loss: 22.2150\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 24.5806 - val_loss: 16.5116\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 18.7941 - val_loss: 13.6921\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 16.1113 - val_loss: 12.2308\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14.6362 - val_loss: 11.5224\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.4462 - val_loss: 11.9424\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12.4263 - val_loss: 11.0283\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.1768 - val_loss: 10.6399\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.6498 - val_loss: 10.8938\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.0640 - val_loss: 12.6529\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.6364 - val_loss: 11.5105\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.4992 - val_loss: 11.5599\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.5488 - val_loss: 10.2839\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12.0090 - val_loss: 10.8815\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.5227 - val_loss: 11.0656\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.5582 - val_loss: 10.4865\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.9226 - val_loss: 10.3623\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.5811 - val_loss: 10.2428\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.6571 - val_loss: 10.5543\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.4795 - val_loss: 10.5079\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.4263 - val_loss: 10.1186\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2400 - val_loss: 11.2994\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.3711 - val_loss: 10.1155\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.0697 - val_loss: 10.4319\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.2762 - val_loss: 10.4678\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9440 - val_loss: 9.7893\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.8807 - val_loss: 10.0522\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0485 - val_loss: 9.8365\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.1066 - val_loss: 10.9271\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.8943 - val_loss: 9.7854\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5629 - val_loss: 9.5638\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.5823 - val_loss: 9.9786\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5192 - val_loss: 9.5872\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5170 - val_loss: 10.2887\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.3034 - val_loss: 10.3527\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.4489 - val_loss: 8.9029\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5720 - val_loss: 9.4367\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.3591 - val_loss: 10.2845\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.9551 - val_loss: 10.1133\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.2242 - val_loss: 9.0393\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.1024 - val_loss: 8.4119\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.9298 - val_loss: 9.1998\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.6293 - val_loss: 9.7969\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8883 - val_loss: 11.1637\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8828 - val_loss: 9.9719\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.8477 - val_loss: 9.9700\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8759 - val_loss: 9.2976\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.4601 - val_loss: 9.2891\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.1266 - val_loss: 9.4350\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.4237 - val_loss: 10.9998\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.1949Restoring model weights from the end of the best epoch: 43.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.1949 - val_loss: 9.2866\n",
      "Epoch 53: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 130.7495 - val_loss: 56.9709\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 52.9781 - val_loss: 31.8633\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.1625 - val_loss: 20.8260\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 22.6905 - val_loss: 16.1443\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 18.1830 - val_loss: 13.1048\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 15.4169 - val_loss: 12.0655\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.8835 - val_loss: 11.4432\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.8610 - val_loss: 10.5302\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.3130 - val_loss: 10.7894\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.6168 - val_loss: 10.8530\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.8286 - val_loss: 12.1822\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.4532 - val_loss: 9.9036\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.3053 - val_loss: 11.7707\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.0339 - val_loss: 10.0974\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.7684 - val_loss: 10.6699\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.5478 - val_loss: 9.6532\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.9135 - val_loss: 10.0404\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.7168 - val_loss: 10.3249\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.1982 - val_loss: 12.1409\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.1644 - val_loss: 9.9814\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.4909 - val_loss: 10.9640\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.0138 - val_loss: 9.5055\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.0833 - val_loss: 10.9373\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9118 - val_loss: 10.3225\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.8024 - val_loss: 9.7072\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.3148 - val_loss: 9.8787\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.9373 - val_loss: 9.7733\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.3456 - val_loss: 9.8690\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.4529 - val_loss: 10.3941\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.2294 - val_loss: 9.4663\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.0367 - val_loss: 9.7722\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.3054 - val_loss: 9.5388\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.8486 - val_loss: 12.0756\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.9893 - val_loss: 9.9604\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.7933 - val_loss: 9.6062\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.6333 - val_loss: 9.3288\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8553 - val_loss: 9.9950\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.7262 - val_loss: 9.6158\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.8144 - val_loss: 13.0906\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8480 - val_loss: 9.9343\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.1473 - val_loss: 10.7379\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.8487 - val_loss: 11.0187\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.0541 - val_loss: 11.2855\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.2582 - val_loss: 10.2514\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.2196 - val_loss: 9.5124\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.0586Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.0586 - val_loss: 10.2946\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 123.2324 - val_loss: 58.2987\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 53.9529 - val_loss: 32.5229\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 33.8477 - val_loss: 25.7654\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 25.0417 - val_loss: 16.3662\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 19.3012 - val_loss: 13.8128\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16.2208 - val_loss: 12.3602\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 14.2967 - val_loss: 13.6071\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 13.3837 - val_loss: 11.3955\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12.8250 - val_loss: 11.2675\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12.4234 - val_loss: 10.7514\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.7175 - val_loss: 10.2275\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.4617 - val_loss: 10.3095\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.3684 - val_loss: 10.6849\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.1651 - val_loss: 10.1213\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.8126 - val_loss: 9.6235\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.3247 - val_loss: 9.9610\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.5162 - val_loss: 12.4033\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.4344 - val_loss: 9.3850\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.2047 - val_loss: 9.7595\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2387 - val_loss: 9.8491\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.1342 - val_loss: 9.7545\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.8989 - val_loss: 10.4330\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.7228 - val_loss: 9.4138\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.7713 - val_loss: 9.4316\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.8312 - val_loss: 9.6891\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.7206 - val_loss: 9.3057\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 9.7315 - val_loss: 9.7323\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.5522 - val_loss: 9.3974\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.5792 - val_loss: 9.5938\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.3229 - val_loss: 8.9541\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.2463 - val_loss: 10.0754\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.0899 - val_loss: 9.6538\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.9964 - val_loss: 9.5412\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.9517 - val_loss: 9.5356\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.8614 - val_loss: 8.9043\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.7892 - val_loss: 9.8999\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.4705 - val_loss: 9.3963\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.4688 - val_loss: 8.7529\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.0215 - val_loss: 9.9055\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.2019 - val_loss: 9.5617\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.2540 - val_loss: 9.3165\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.5454 - val_loss: 9.7941\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.2209 - val_loss: 8.6835\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.1587 - val_loss: 8.4673\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.8779 - val_loss: 8.7995\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.0176 - val_loss: 9.1205\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 7.8850 - val_loss: 9.7262\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.8946 - val_loss: 9.5572\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 8.0559 - val_loss: 10.3503\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.7385 - val_loss: 8.9289\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.5819 - val_loss: 9.2333\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 6.9759 - val_loss: 9.5120\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.1270 - val_loss: 11.1706\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 7.2162Restoring model weights from the end of the best epoch: 44.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.2162 - val_loss: 10.7167\n",
      "Epoch 54: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 136.9767 - val_loss: 66.8917\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 61.7893 - val_loss: 38.0101\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 38.0705 - val_loss: 24.3356\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 25.8931 - val_loss: 17.5156\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 19.8162 - val_loss: 14.7029\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 16.8593 - val_loss: 13.4338\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 14.5625 - val_loss: 11.8499\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 13.6311 - val_loss: 12.4338\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 12.9544 - val_loss: 11.3690\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 12.2109 - val_loss: 11.9499\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.0451 - val_loss: 10.3584\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.0557 - val_loss: 11.7664\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.5210 - val_loss: 10.2036\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.1970 - val_loss: 10.6516\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11.2911 - val_loss: 9.9931\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.6356 - val_loss: 11.3457\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.7762 - val_loss: 10.0445\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.9830 - val_loss: 9.8929\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.3933 - val_loss: 10.6271\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.2635 - val_loss: 9.8329\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.9965 - val_loss: 9.4975\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.7220 - val_loss: 10.0469\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9837 - val_loss: 10.0174\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.8791 - val_loss: 10.6743\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.6583 - val_loss: 10.6841\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.6108 - val_loss: 10.1518\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 9.3573 - val_loss: 10.0211\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.4260 - val_loss: 12.0494\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.5801 - val_loss: 10.6890\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.4461 - val_loss: 9.9523\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.2402Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.2402 - val_loss: 10.8979\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 122.3509 - val_loss: 55.9412\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 53.3238 - val_loss: 31.6156\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.6049 - val_loss: 20.0445\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 23.1818 - val_loss: 15.6597\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 18.2753 - val_loss: 13.0449\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 15.3885 - val_loss: 11.6375\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.9489 - val_loss: 12.5907\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 13.0955 - val_loss: 13.7008\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.2248 - val_loss: 10.2425\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.8960 - val_loss: 10.3603\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.5319 - val_loss: 10.1125\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.5875 - val_loss: 11.2639\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.1055 - val_loss: 11.0532\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 10.9382 - val_loss: 11.4294\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.6122 - val_loss: 10.0772\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.9126 - val_loss: 9.4668\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.5158 - val_loss: 11.4395\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.3188 - val_loss: 9.6513\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0541 - val_loss: 9.8222\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0416 - val_loss: 9.1568\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.2046 - val_loss: 10.5068\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.8575 - val_loss: 9.9052\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.7161 - val_loss: 9.5904\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.4207 - val_loss: 9.8194\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.7832 - val_loss: 10.3230\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.7072 - val_loss: 10.6103\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.0550 - val_loss: 11.8381\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.7621 - val_loss: 10.6006\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.5518 - val_loss: 9.7359\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.4098 - val_loss: 8.9380\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.2356 - val_loss: 10.2702\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0955 - val_loss: 8.9872\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 9.0718 - val_loss: 9.0836\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.5063 - val_loss: 10.4115\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0738 - val_loss: 10.6563\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.1109 - val_loss: 8.5873\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.7877 - val_loss: 9.2456\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1805 - val_loss: 8.9275\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.4624 - val_loss: 10.0048\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.0545 - val_loss: 9.6425\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8596 - val_loss: 9.8249\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.9718 - val_loss: 9.9663\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.8157 - val_loss: 9.6107\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.6173 - val_loss: 10.8821\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.4875 - val_loss: 9.7281\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.5988Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.5988 - val_loss: 8.9239\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 118.0280 - val_loss: 54.2475\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 51.3316 - val_loss: 30.3373\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 31.5658 - val_loss: 21.0724\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 21.9372 - val_loss: 15.8757\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 17.7089 - val_loss: 13.1441\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 15.2491 - val_loss: 12.0192\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 13.7662 - val_loss: 13.3911\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 13.7906 - val_loss: 12.7206\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 13.0267 - val_loss: 10.9049\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 12.2088 - val_loss: 10.7189\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 11.9014 - val_loss: 11.0723\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.8112 - val_loss: 10.5925\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.9054 - val_loss: 10.4513\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 11.8723 - val_loss: 10.9047\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.3242 - val_loss: 11.9929\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.1361 - val_loss: 11.3829\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 11.3817 - val_loss: 10.2839\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.9347 - val_loss: 9.9694\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.8607 - val_loss: 10.8725\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 10.6457 - val_loss: 10.1266\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.6859 - val_loss: 10.6856\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.3480 - val_loss: 9.7510\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 9.9783 - val_loss: 9.6255\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.4167 - val_loss: 11.0408\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 10.0379 - val_loss: 10.5271\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.0256 - val_loss: 12.2913\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.1669 - val_loss: 9.9873\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.1848 - val_loss: 9.9669\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 9.5433 - val_loss: 10.0480\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.1079 - val_loss: 12.0714\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.5434 - val_loss: 10.2019\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.4593 - val_loss: 10.6389\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 10.5710Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 10.5710 - val_loss: 11.0741\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 33ms/step\n",
      "5/5 [==============================] - 0s 33ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 33ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.93121, 3.10456, 3.072, 16.99985, 3.30394)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n",
    "mae_predictions = bagging_predict2(pred4, X_train_val)\n",
    "mse_predictions =  bagging_predict2(pred5, X_train_val)\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "sMAPE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAPE= np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MASE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MSE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "\n",
    "MSE, MASE, MAE, MAPE, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed0c7ce-ad6f-47e0-b18a-7e5a1e0a7f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "exp 3.14525\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions2 = bagging_predict2(pred1, test_X)\n",
    "smape_predictions2 = np.median(np.concatenate([smape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mase_predictions2 =bagging_predict2(pred2, test_X)\n",
    "mase_predictions2 = np.median(np.concatenate([mase_predictions2],axis=0),axis=0)\n",
    "\n",
    "mape_predictions2 =bagging_predict2(pred3, test_X)\n",
    "mape_predictions2 = np.median(np.concatenate([mape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mae_predictions2 = bagging_predict2(pred4,test_X)\n",
    "mae_predictions2 = np.median(np.concatenate([mae_predictions2],axis=0),axis=0)\n",
    "\n",
    "mse_predictions2 =bagging_predict2(pred5,test_X)\n",
    "mse_predictions2 = np.median(np.concatenate([mse_predictions2],axis=0),axis=0)\n",
    "\n",
    "\n",
    "#concat_mase = np.concatenate([np.nan_to_num(np.array(mase_predictions2), nan=0)])\n",
    "#fin_pred_mase = np.median(concat_mase,axis=1)\n",
    "\n",
    "#concat_mape = np.concatenate([np.nan_to_num(np.array(mape_predictions2), nan=0)])\n",
    "#fin_pred_mape = np.median(concat_mape,axis=1)\n",
    "\n",
    "#concat_smape = np.concatenate([np.nan_to_num(np.array(smape_predictions2), nan=0)])\n",
    "#fin_pred_smape = np.median(concat_smape,axis=1)\n",
    "\n",
    "#concat_mae = np.concatenate([np.nan_to_num(np.array(mae_predictions2), nan=0)])\n",
    "#fin_pred_mae = np.median(concat_mae,axis=1)\n",
    "\n",
    "#concat_mse = np.concatenate([np.nan_to_num(np.array(mse_predictions2), nan=0)])\n",
    "#fin_pred_mse = np.median(concat_mse,axis=1)\n",
    "\n",
    "performance = np.array([MAE, MAPE,sMAPE,MSE,MASE])\n",
    "beta = 3 # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([mae_predictions2,\n",
    "                    mape_predictions2,\n",
    "                   smape_predictions2,\n",
    "                   mse_predictions2,\n",
    "                   mase_predictions2],axis=0)\n",
    "#gd=np.median(gd,axis=2)\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "pd.DataFrame(ensemble_prediction.flatten()).to_csv('exp7/LSTM.csv')\n",
    "\n",
    "\n",
    "print('exp',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93fd3e88-6523-4042-a996-ad1f65d9fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "all 3.15118\n",
      "original 3.21648\n",
      "best 3.14744\n",
      "mse 3.16214\n",
      "mase 3.17097\n",
      "mae 3.17264\n",
      "mape 18.02861\n",
      "smape 3.32071\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "mae_predictions_G = bagging_predict2(pred4, test_X)\n",
    "mse_predictions_G = bagging_predict2(pred5,test_X)\n",
    "\n",
    "\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G,mae_predictions_G,mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('all',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5)) \n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G, mase_predictions_G,mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('best',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mse',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mae',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce6054d-bd3d-4981-90dc-91c0ab363196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3.152945),\n",
       " (2, 3.1484854),\n",
       " (3, 3.1452515),\n",
       " (4, 3.143071),\n",
       " (5, 3.1417692),\n",
       " (6, 3.141185),\n",
       " (7, 3.141174),\n",
       " (8, 3.1416101),\n",
       " (9, 3.1423838),\n",
       " (10, 3.1434019),\n",
       " (11, 3.1445856),\n",
       " (12, 3.1458688),\n",
       " (13, 3.1471977),\n",
       " (14, 3.148529),\n",
       " (15, 3.1498303),\n",
       " (16, 3.151077),\n",
       " (17, 3.1522524),\n",
       " (18, 3.1533453),\n",
       " (19, 3.1543508)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eee = []\n",
    "for i in range(1,20):\n",
    "    weights = np.exp(-i* performance)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "    eee.append((i,np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten().round(5)))))\n",
    "    #print(f'exp_beta{i}',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))\n",
    "\n",
    "eee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
