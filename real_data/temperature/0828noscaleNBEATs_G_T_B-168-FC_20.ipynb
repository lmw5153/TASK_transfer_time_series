{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1c4e45-4282-4654-91cf-273b2ddc4d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((578, 168), (145, 168), (358, 168))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_X.shape,target_X_val.shape,test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87e935f-ff85-4b32-9115-faea5af569a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((578, 24), (145, 24), (358, 24))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_y.shape,target_y_val.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3692c3-3a1e-4c06-817f-8dfb583638e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11560, 168), (11560, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebab801-077c-43dc-9f56-b335c1d5fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 00:25:14.865981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-20 00:25:15.026690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-20 00:25:15.026715: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-20 00:25:15.703168: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-20 00:25:15.703269: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-20 00:25:15.703279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape,LSTM\n",
    "#import pandas as pd\n",
    "#####################################################################################\n",
    "data = 'tem'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "\n",
    "target_X_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "target_y_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "target_X = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "target_y = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "np.random.seed(2)\n",
    "random_indices1 = np.random.choice(pd.read_csv(\"../data/M4_train.csv\").iloc[:,(1):].index, size=target_X.shape[0]*20, replace=False)\n",
    "X_train = pd.read_csv(\"../data/M4_train.csv\").iloc[:,1+(24*0):].loc[random_indices1].values\n",
    "y_train = pd.read_csv(\"../data/M4_test.csv\").iloc[:,1:].loc[random_indices1].values\n",
    "X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "target_X.shape,test_X.shape\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1], y_train.shape[1],1,1,256\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 생성 함수\n",
    "def bulid_model(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK)\n",
    "                   ,nb_blocks_per_stack=1, thetas_dim=(1,2,2,4,4,4),\n",
    "                   share_weights_in_stack=True, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + G모델 생성 함수    \n",
    "def bulid_model_G(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.GENERIC_BLOCK,NBeatsKeras.GENERIC_BLOCK)\n",
    "                   ,nb_blocks_per_stack=5, thetas_dim=(24,24),\n",
    "                   share_weights_in_stack=False, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models_G(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model_G(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=False)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[np.newaxis, ...]\n",
    "\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "def create_model1(lossf, d_model, nlayers, nhead, dropout, iw, ow, lr, pretrained_output_reshaped, inputs):\n",
    "    \"\"\"\n",
    "    Create an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - lossf (callable): Loss function to use.\n",
    "    - d_model (int): Dimension of the model.\n",
    "    - nlayers (int): Number of LSTM layers.\n",
    "    - nhead (int): Number of heads (not used in LSTM).\n",
    "    - dropout (float): Dropout rate.\n",
    "    - iw (int): Input width (number of features).\n",
    "    - ow (int): Output width (number of classes).\n",
    "    - lr (float): Learning rate for the optimizer.\n",
    "    - pretrained_output_reshaped: Pre-trained model output reshaped.\n",
    "    - inputs: Input layer.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    # LSTM input layer\n",
    "    x = LSTM(d_model, return_sequences=False)(pretrained_output_reshaped)\n",
    "    \n",
    "    for _ in range(nlayers - 1):\n",
    "        x = LSTM(d_model, return_sequences=False)(x)\n",
    "       # x = Dropout(dropout)(x)  # Add dropout after each LSTM layer\n",
    "\n",
    "    # Final Dense layer for output\n",
    "    x = Dense(ow, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=lossf, metrics=['mae'])  # Adjust metrics as needed\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def create_model(fn,d_model, nlayers, nhead, dropout, iw, ow,lr,pretrained_output_reshaped,inputs):\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(pretrained_output_reshaped)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "    \n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    \n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    target_model = Model(inputs=inputs, outputs=outputs)\n",
    "    target_model.compile(optimizer=optimizer, loss=fn)\n",
    "    \n",
    "    return target_model\n",
    "########################################################################################################################\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# 예측\n",
    "def transfer_FC(model_num, models, trainable, lossf, epochs_, batch_size_, pt, lr_):\n",
    "    history_mapes_G = []\n",
    "    model_pred_val = []\n",
    "    model_pred_test = []\n",
    "    for i in range(1, model_num + 1):\n",
    "        K.clear_session()\n",
    "        model_name = f'model_{i}'\n",
    "        m, _ = models\n",
    "        model1 = m[model_name]\n",
    "        \n",
    "        # 모든 레이어를 학습 불가능하게 설정\n",
    "        for layer in model1.layers[:-1]:\n",
    "            layer.trainable = trainable\n",
    "            \n",
    "        pretrained_layers = model1.layers[:-1]\n",
    "        pretrained_model = Model(inputs=model1.input, outputs=pretrained_layers[-1].output)\n",
    "\n",
    "        # LSTM 입력을 위한 입력 레이어\n",
    "        inputs = Input(shape=(target_X.shape[1], 1))  # X_train의 shape에 맞게 설정\n",
    "        pretrained_output = pretrained_model(inputs)\n",
    "        pretrained_output_reshaped = layers.Reshape((y_train.shape[1], -1))(pretrained_output)\n",
    "        # LSTM 레이어 추가\n",
    "        lstm_output = layers.Dense(128, activation='linear')(pretrained_output_reshaped ) \n",
    "        lstm_output = layers.Dropout(0.2)(lstm_output)\n",
    "        lstm_output = layers.Dense(64, activation='linear')(lstm_output)# LSTM 레이어 추가\n",
    "        lstm_output = layers.Dropout(0.2)(lstm_output)\n",
    "        #lstm_output = layers.Dense(32, activation='linear')(lstm_output)# LSTM 레이어 추가\n",
    "        #lstm_output = layers.Dropout(0.2)(lstm_output)\n",
    "       #lstm_output = layers.Dense(y_train.shape[1], activation='linear')(lstm_output)  # 최종 출력 레이어\n",
    "        #lstm_output = layers.Lambda(lambda x: x[:, -24:, :])(lstm_output)\n",
    "\n",
    "        lstm_output = layers.Dense(1, activation='linear')(lstm_output)\n",
    "        model_instance = Model(inputs=inputs, outputs=lstm_output)\n",
    "\n",
    "        model_instance.compile(optimizer='adam', loss=lossf)  # 모델 컴파일\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=pt, verbose=0, restore_best_weights=True)\n",
    "    \n",
    "        # 모델 학습\n",
    "        history = model_instance.fit(target_X, target_y, batch_size=batch_size_,\n",
    "                                      epochs=epochs_, verbose=0, \n",
    "                                      callbacks=[early_stop],\n",
    "                                      validation_data=[target_X_val,target_y_val])\n",
    "        \n",
    "        # 예측\n",
    "        pred_val = model_instance.predict(target_X_val)\n",
    "        pred_val = pred_val.reshape(-1, y_train.shape[1])\n",
    "        model_pred_val.append(pred_val)\n",
    "        \n",
    "        pred_test = model_instance.predict(test_X)\n",
    "        pred_test = pred_test.reshape(-1, y_train.shape[1])\n",
    "        model_pred_test.append(pred_test)\n",
    "\n",
    "        history_mapes_G.append(history)\n",
    "        print(f\"########################################################fitted {i}\")\n",
    "    \n",
    "    return model_pred_val,model_pred_test\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27e2cd6-1f6f-4c32-94ef-a199729c9e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 00:25:17.735732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-20 00:25:17.735768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-20 00:25:17.736233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "num=10\n",
    "mape_models = train_bagging_models_G(num,'mape',2000,10,256,0.001)\n",
    "smape_models = train_bagging_models_G(num,SMAPE(),2000,10,256,0.001)\n",
    "mase_models = train_bagging_models_G(num,MASE(y_train,y_train.shape[1]),2000,10,256,0.001)\n",
    "mae_models = train_bagging_models_G(num,'mae',2000,10,256,0.001)\n",
    "mse_models = train_bagging_models_G(num,'mse',2000,10,256,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5bf679-acab-48b5-b52f-1a1d4c6634ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "########################################################fitted 1\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 2\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "########################################################fitted 3\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 4\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 5\n",
      "5/5 [==============================] - 1s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 6\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 7\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 8\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 9\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 10\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "########################################################fitted 1\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 2\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 3\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 4\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 5\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 6\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 7\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 8\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 9\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 10\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 1\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 2\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 3\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 4\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 5\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 6\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 7\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 8\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 9\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 10\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 1\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "########################################################fitted 2\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 3\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 4\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 5\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 6\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 7\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 8\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 9\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 10\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 1\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 2\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 3\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 4\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 5\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 6\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 7\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 8\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 9\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "########################################################fitted 10\n"
     ]
    }
   ],
   "source": [
    "mape_pred , mape_pred2 = transfer_FC(num,mape_models,True, 'mape',2000,8,10,0.001)\n",
    "mase_pred ,mase_pred2 = transfer_FC(num,mase_models,True, MASE(target_y,y_train.shape[1]),2000,8,10,0.001)\n",
    "smape_pred,smape_pred2 = transfer_FC(num,smape_models,True, SMAPE(),2000,8,10,0.001)\n",
    "mae_pred ,mae_pred2= transfer_FC(num,mape_models,True, 'mae',2000,8,10,0.001)\n",
    "mse_pred ,mse_pred2= transfer_FC(num,mape_models,True, 'mse',2000,8,10,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec188940-5830-44af-aa6d-6cbc11281d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp7/NBEATs_G+FC\n",
      "##############################\n",
      "all 2.56411\n",
      "best 2.60534\n",
      "mse 2.64794\n",
      "mase 2.60593\n",
      "mae 2.62321\n",
      "mape 16.9919\n",
      "mape 2.57136\n",
      "exp 2.56647\n",
      "#############var##############\n",
      "##############################\n",
      "#############test#############\n",
      "all 2.60726\n",
      "best 2.60907\n",
      "org 2.64168\n",
      "mse 2.63419\n",
      "mase 2.61878\n",
      "mae 2.64531\n",
      "mape 18.02079\n",
      "smape 2.62291\n",
      "exp 2.58697\n",
      "############save##############\n",
      "############exp_beta##########\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2.5881755),\n",
       " (2, 2.5875592),\n",
       " (3, 2.5869694),\n",
       " (4, 2.5864081),\n",
       " (5, 2.5858777),\n",
       " (6, 2.5853796),\n",
       " (7, 2.5849156),\n",
       " (8, 2.5844877),\n",
       " (9, 2.5840971)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'exp7/NBEATs_G+FC'\n",
    "\n",
    "print(name)\n",
    "print(\"##############################\")\n",
    "\n",
    "concat_G = np.concatenate([mape_pred,mase_pred,smape_pred,mse_pred,mae_pred])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('all',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mae_pred, mase_pred,mse_pred])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('best',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.nan_to_num(np.array(mse_pred), nan=0)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mse',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(mase_pred)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mase',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.nan_to_num(np.array(mae_pred), nan=0)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mae',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(mape_pred)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mape',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(smape_pred)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mape',np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_mase = np.concatenate([np.nan_to_num(np.array(mase_pred), nan=0)])\n",
    "fin_pred_mase = np.median(concat_mase,axis=0)\n",
    "MASE= np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_mase .flatten())).round(5)\n",
    "\n",
    "concat_mape = np.concatenate([np.nan_to_num(np.array(mape_pred), nan=0)])\n",
    "fin_pred_mape = np.median(concat_mape,axis=0)\n",
    "MAPE= np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_mape .flatten())).round(5)\n",
    "\n",
    "concat_smape = np.concatenate([np.nan_to_num(np.array(smape_pred), nan=0)])\n",
    "fin_pred_smape = np.median(concat_smape,axis=0)\n",
    "sMAPE= np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_smape .flatten())).round(5)\n",
    "\n",
    "concat_mae = np.concatenate([np.nan_to_num(np.array(mae_pred), nan=0)])\n",
    "fin_pred_mae = np.median(concat_mae,axis=0)\n",
    "MAE= np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_mae .flatten())).round(5)\n",
    "\n",
    "concat_mse = np.concatenate([np.nan_to_num(np.array(mse_pred), nan=0)])\n",
    "fin_pred_mse = np.median(concat_mse,axis=0)\n",
    "MSE= np.sqrt(mean_squared_error(target_y_val.flatten(),fin_pred_mse .flatten())).round(5)\n",
    "\n",
    "performance = np.array([MASE, MAPE,sMAPE,MAE,MSE])\n",
    "beta = 3 # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([fin_pred_mase.flatten().reshape(1,-1),\n",
    "                    fin_pred_mape.flatten().reshape(1,-1),\n",
    "                   fin_pred_smape.flatten().reshape(1,-1),\n",
    "                   fin_pred_mae.flatten().reshape(1,-1),\n",
    "                   fin_pred_mse.flatten().reshape(1,-1)],axis=0)\n",
    "\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd)\n",
    "print('exp',np.sqrt(mean_squared_error(target_y_val.flatten(),ensemble_prediction)).round(5))\n",
    "print(\"#############var##############\")\n",
    "print(\"##############################\")\n",
    "print(\"#############test#############\")\n",
    "concat_G = np.concatenate([mape_pred2,mase_pred2,smape_pred2,mse_pred2,mae_pred2])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('all',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mae_pred2, mase_pred2,mse_pred2])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('best',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "concat_G = np.concatenate([mape_pred2, mase_pred2,smape_pred2])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('org',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "concat_G = np.concatenate([np.array(mse_pred2)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mse',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(mase_pred2)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mase',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(mae_pred2)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mae',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(mape_pred2)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('mape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.array(smape_pred2)])\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "print('smape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_mase = np.concatenate([np.nan_to_num(np.array(mase_pred2), nan=0)])\n",
    "fin_pred_mase = np.median(concat_mase,axis=0)\n",
    "\n",
    "concat_mape = np.concatenate([np.nan_to_num(np.array(mape_pred2), nan=0)])\n",
    "fin_pred_mape = np.median(concat_mape,axis=0)\n",
    "\n",
    "concat_smape = np.concatenate([np.nan_to_num(np.array(smape_pred2), nan=0)])\n",
    "fin_pred_smape = np.median(concat_smape,axis=0)\n",
    "\n",
    "concat_mae = np.concatenate([np.nan_to_num(np.array(mae_pred2), nan=0)])\n",
    "fin_pred_mae = np.median(concat_mae,axis=0)\n",
    "\n",
    "concat_mse = np.concatenate([np.nan_to_num(np.array(mse_pred2), nan=0)])\n",
    "fin_pred_mse = np.median(concat_mse,axis=0)\n",
    "\n",
    "#performance = np.array([MASE, MAPE,sMAPE,MAE,MSE])\n",
    "beta = 3 # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([fin_pred_mase.flatten().reshape(1,-1),\n",
    "                    fin_pred_mape.flatten().reshape(1,-1),\n",
    "                   fin_pred_smape.flatten().reshape(1,-1),\n",
    "                   fin_pred_mae.flatten().reshape(1,-1),\n",
    "                   fin_pred_mse.flatten().reshape(1,-1)],axis=0)\n",
    "\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd)\n",
    "print('exp',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction)).round(5))\n",
    "pd.DataFrame(ensemble_prediction.flatten()).to_csv(f'{name}.csv')\n",
    "print(\"############save##############\")\n",
    "print(\"############exp_beta##########\")\n",
    "\n",
    "eee = []\n",
    "for i in range(1,10):\n",
    "    weights = np.exp(-i* performance)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "    eee.append((i,np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten().round(5)))))\n",
    "    #print(f'exp_beta{i}',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))\n",
    "\n",
    "eee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
