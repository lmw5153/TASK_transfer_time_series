{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 14:05:00.274249: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 14:05:00.349252: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-18 14:05:00.349268: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-18 14:05:00.714586: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-18 14:05:00.714636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-18 14:05:00.714642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = 'solor'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 14:05:04.146979: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-18 14:05:04.147013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-18 14:05:04.147484: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 85ms/step - loss: 1.0891 - val_loss: 0.7985\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.8713 - val_loss: 0.6445\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.8008 - val_loss: 0.6320\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.7955 - val_loss: 0.5987\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.7860 - val_loss: 0.6276\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.7828 - val_loss: 0.5968\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7610 - val_loss: 0.5878\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7590 - val_loss: 0.6205\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7655 - val_loss: 0.5867\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.7606 - val_loss: 0.5983\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7447 - val_loss: 0.5849\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7450 - val_loss: 0.6067\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7631 - val_loss: 0.5927\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7464 - val_loss: 0.5801\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7503 - val_loss: 0.5813\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7424 - val_loss: 0.5750\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7343 - val_loss: 0.5677\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7317 - val_loss: 0.5926\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7326 - val_loss: 0.5931\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7333 - val_loss: 0.5771\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7304 - val_loss: 0.5812\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7303 - val_loss: 0.5761\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7340 - val_loss: 0.5688\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.7280 - val_loss: 0.5848\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7344 - val_loss: 0.5702\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7428 - val_loss: 0.5786\n",
      "Epoch 27/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7258Restoring model weights from the end of the best epoch: 17.\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7275 - val_loss: 0.6187\n",
      "Epoch 27: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 90ms/step - loss: 1.1542 - val_loss: 0.7484\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.8437 - val_loss: 0.7459\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8146 - val_loss: 0.6941\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.8061 - val_loss: 0.6248\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7705 - val_loss: 0.6062\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7822 - val_loss: 0.6170\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7680 - val_loss: 0.6422\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7643 - val_loss: 0.5954\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7556 - val_loss: 0.5885\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7599 - val_loss: 0.5809\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7404 - val_loss: 0.5716\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7470 - val_loss: 0.5933\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7447 - val_loss: 0.5893\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7517 - val_loss: 0.5895\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7420 - val_loss: 0.6133\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7453 - val_loss: 0.5933\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7435 - val_loss: 0.5767\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7376 - val_loss: 0.5721\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7395 - val_loss: 0.6046\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7265 - val_loss: 0.6083\n",
      "Epoch 21/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7346Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7337 - val_loss: 0.6222\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 88ms/step - loss: 1.1607 - val_loss: 0.7488\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.8539 - val_loss: 0.6704\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.8175 - val_loss: 0.7451\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.8189 - val_loss: 0.6329\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7740 - val_loss: 0.5837\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7799 - val_loss: 0.6811\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7767 - val_loss: 0.5974\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7581 - val_loss: 0.6863\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7591 - val_loss: 0.6137\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7733 - val_loss: 0.5993\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7414 - val_loss: 0.6120\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7466 - val_loss: 0.5963\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7467 - val_loss: 0.6155\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7555 - val_loss: 0.5855\n",
      "Epoch 15/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7430Restoring model weights from the end of the best epoch: 5.\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7433 - val_loss: 0.5849\n",
      "Epoch 15: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 1.1567 - val_loss: 0.6958\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8231 - val_loss: 0.6533\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8163 - val_loss: 0.6221\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7780 - val_loss: 0.6924\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7892 - val_loss: 0.6018\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7817 - val_loss: 0.6088\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7664 - val_loss: 0.6258\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7618 - val_loss: 0.5785\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7632 - val_loss: 0.5892\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7450 - val_loss: 0.5860\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7513 - val_loss: 0.5812\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7436 - val_loss: 0.5948\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7501 - val_loss: 0.6557\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7600 - val_loss: 0.5799\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7371 - val_loss: 0.5998\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7393 - val_loss: 0.5973\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7539 - val_loss: 0.5833\n",
      "Epoch 18/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7409Restoring model weights from the end of the best epoch: 8.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7410 - val_loss: 0.5906\n",
      "Epoch 18: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 91ms/step - loss: 1.1413 - val_loss: 0.7461\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8375 - val_loss: 0.7468\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8204 - val_loss: 0.6314\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.8062 - val_loss: 0.6146\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7896 - val_loss: 0.6564\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7923 - val_loss: 0.6591\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7845 - val_loss: 0.6140\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7624 - val_loss: 0.6499\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7723 - val_loss: 0.6490\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7588 - val_loss: 0.5946\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7431 - val_loss: 0.5663\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7500 - val_loss: 0.6268\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7507 - val_loss: 0.6265\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7606 - val_loss: 0.5869\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7392 - val_loss: 0.6089\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7569 - val_loss: 0.5881\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7433 - val_loss: 0.5929\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7464 - val_loss: 0.5643\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7397 - val_loss: 0.5873\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7343 - val_loss: 0.6011\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7457 - val_loss: 0.5737\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7356 - val_loss: 0.5847\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7339 - val_loss: 0.5727\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7361 - val_loss: 0.5970\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7321 - val_loss: 0.5682\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7384 - val_loss: 0.6519\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7430 - val_loss: 0.5900\n",
      "Epoch 28/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7437Restoring model weights from the end of the best epoch: 18.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7430 - val_loss: 0.5750\n",
      "Epoch 28: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 91ms/step - loss: 1.2238 - val_loss: 0.7656\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.8323 - val_loss: 0.6533\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.8111 - val_loss: 0.6398\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7890 - val_loss: 0.6082\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7934 - val_loss: 0.5994\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7852 - val_loss: 0.5872\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7606 - val_loss: 0.6115\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7640 - val_loss: 0.6015\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7668 - val_loss: 0.6133\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7500 - val_loss: 0.5982\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7593 - val_loss: 0.6040\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7531 - val_loss: 0.6157\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7543 - val_loss: 0.5726\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7387 - val_loss: 0.5649\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7566 - val_loss: 0.5957\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7418 - val_loss: 0.5555\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7354 - val_loss: 0.5776\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7369 - val_loss: 0.6079\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7510 - val_loss: 0.5706\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7244 - val_loss: 0.5623\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7322 - val_loss: 0.5685\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7275 - val_loss: 0.5610\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7270 - val_loss: 0.5611\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7350 - val_loss: 0.5680\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7327 - val_loss: 0.5598\n",
      "Epoch 26/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7320Restoring model weights from the end of the best epoch: 16.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7318 - val_loss: 0.5805\n",
      "Epoch 26: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 89ms/step - loss: 1.0934 - val_loss: 0.7053\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8343 - val_loss: 0.6579\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.8368 - val_loss: 0.6471\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7862 - val_loss: 0.6081\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7811 - val_loss: 0.6058\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7801 - val_loss: 0.6565\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7676 - val_loss: 0.6450\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7692 - val_loss: 0.6113\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7691 - val_loss: 0.6101\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7461 - val_loss: 0.6284\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7612 - val_loss: 0.5773\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7516 - val_loss: 0.5856\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7436 - val_loss: 0.5801\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7372 - val_loss: 0.5934\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7373 - val_loss: 0.5794\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7418 - val_loss: 0.5781\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7311 - val_loss: 0.6348\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7483 - val_loss: 0.5941\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7394 - val_loss: 0.5631\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7278 - val_loss: 0.5651\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7349 - val_loss: 0.5955\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7434 - val_loss: 0.5987\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7357 - val_loss: 0.5962\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7298 - val_loss: 0.5757\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7389 - val_loss: 0.5746\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7300 - val_loss: 0.5910\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7379 - val_loss: 0.5738\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7273 - val_loss: 0.5864\n",
      "Epoch 29/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7314Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7325 - val_loss: 0.5868\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 91ms/step - loss: 1.1705 - val_loss: 0.7206\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.8509 - val_loss: 0.8097\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.8153 - val_loss: 0.6050\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7857 - val_loss: 0.5952\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7689 - val_loss: 0.6119\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7636 - val_loss: 0.5926\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7796 - val_loss: 0.6003\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7591 - val_loss: 0.6154\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7605 - val_loss: 0.5874\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7440 - val_loss: 0.6036\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7449 - val_loss: 0.5664\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7480 - val_loss: 0.5822\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7423 - val_loss: 0.5821\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7353 - val_loss: 0.5748\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7298 - val_loss: 0.5836\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7409 - val_loss: 0.5690\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7316 - val_loss: 0.6712\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7465 - val_loss: 0.5924\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7365 - val_loss: 0.6674\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7516 - val_loss: 0.5616\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7363 - val_loss: 0.5737\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7301 - val_loss: 0.6220\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7267 - val_loss: 0.6080\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7290 - val_loss: 0.5738\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7389 - val_loss: 0.5804\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7226 - val_loss: 0.5959\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7243 - val_loss: 0.5639\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7266 - val_loss: 0.5671\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7280 - val_loss: 0.5821\n",
      "Epoch 30/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7349Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.7345 - val_loss: 0.5852\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 1.1805 - val_loss: 0.7302\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.8466 - val_loss: 0.6909\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7924 - val_loss: 0.6256\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8205 - val_loss: 0.6452\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7794 - val_loss: 0.6517\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7699 - val_loss: 0.6804\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7636 - val_loss: 0.6212\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7589 - val_loss: 0.6273\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7541 - val_loss: 0.5826\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7538 - val_loss: 0.5695\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7535 - val_loss: 0.5918\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7478 - val_loss: 0.5668\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7403 - val_loss: 0.5892\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7487 - val_loss: 0.5806\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7417 - val_loss: 0.5729\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7422 - val_loss: 0.5758\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7423 - val_loss: 0.6224\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7344 - val_loss: 0.6025\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7410 - val_loss: 0.5736\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7493 - val_loss: 0.5945\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7337 - val_loss: 0.5626\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7342 - val_loss: 0.5741\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7353 - val_loss: 0.5808\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7331 - val_loss: 0.5711\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7322 - val_loss: 0.6034\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7332 - val_loss: 0.5802\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7274 - val_loss: 0.5897\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7319 - val_loss: 0.5794\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7255 - val_loss: 0.5856\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.7345 - val_loss: 0.5662\n",
      "Epoch 31/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7293Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7311 - val_loss: 0.5976\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 1.1217 - val_loss: 1.1096\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8812 - val_loss: 0.6594\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8082 - val_loss: 0.6318\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7930 - val_loss: 0.6155\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7960 - val_loss: 0.6265\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7786 - val_loss: 0.6242\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7645 - val_loss: 0.6044\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7788 - val_loss: 0.6093\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7684 - val_loss: 0.6101\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7521 - val_loss: 0.6039\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7558 - val_loss: 0.6061\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7509 - val_loss: 0.6377\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7465 - val_loss: 0.5716\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7440 - val_loss: 0.6262\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7525 - val_loss: 0.6207\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7433 - val_loss: 0.5665\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7401 - val_loss: 0.5839\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7346 - val_loss: 0.5761\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7319 - val_loss: 0.5690\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7307 - val_loss: 0.5733\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7315 - val_loss: 0.5924\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7398 - val_loss: 0.6248\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7385 - val_loss: 0.5745\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7273 - val_loss: 0.5657\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7339 - val_loss: 0.5653\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7281 - val_loss: 0.5599\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7234 - val_loss: 0.5658\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7380 - val_loss: 0.5921\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7314 - val_loss: 0.5662\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7296 - val_loss: 0.5952\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7315 - val_loss: 0.5784\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7273 - val_loss: 0.5597\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7234 - val_loss: 0.5686\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7204 - val_loss: 0.5654\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7263 - val_loss: 0.5685\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7395 - val_loss: 0.5809\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7311 - val_loss: 0.5757\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7276 - val_loss: 0.5880\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7221 - val_loss: 0.5614\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7266 - val_loss: 0.5697\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7254 - val_loss: 0.5743\n",
      "Epoch 42/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.7254Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7248 - val_loss: 0.5707\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 3647543.2500 - val_loss: 906968.9375\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 641938.0625 - val_loss: 601278.7500\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 494669.6562 - val_loss: 211724.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 310202.7188 - val_loss: 297901.3125\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 358754.9375 - val_loss: 328065.8438\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 360594.7500 - val_loss: 706233.3750\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 367127.9375 - val_loss: 216409.4844\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 410126.6562 - val_loss: 465728.3750\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 309645.6250 - val_loss: 195658.2344\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 183762.7656 - val_loss: 134754.5312\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 240157.3906 - val_loss: 120368.0156\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 152185.1719 - val_loss: 150526.6875\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 244006.9531 - val_loss: 305353.5312\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 192960.8906 - val_loss: 44308.9883\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 153867.3750 - val_loss: 98997.2812\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 139851.6250 - val_loss: 309861.1250\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 178856.6406 - val_loss: 355931.0938\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 312763.0000 - val_loss: 149006.9219\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 172764.7969 - val_loss: 302453.6875\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 131983.6875 - val_loss: 219102.8125\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125198.3672 - val_loss: 127428.5703\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 155458.8906 - val_loss: 160824.0000\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 169718.6250 - val_loss: 253897.2812\n",
      "Epoch 24/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 140444.9688Restoring model weights from the end of the best epoch: 14.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 140394.8906 - val_loss: 131374.5469\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 3927406.0000 - val_loss: 610133.2500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 691039.2500 - val_loss: 567089.2500\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 506402.4375 - val_loss: 540679.1250\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 457961.4375 - val_loss: 1672920.1250\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 477917.5000 - val_loss: 217711.5625\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 382097.5938 - val_loss: 369407.0625\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 426366.6562 - val_loss: 382343.8438\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 275430.5938 - val_loss: 855895.1875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 490529.0938 - val_loss: 460999.3438\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 256872.8125 - val_loss: 325924.6250\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 332599.3438 - val_loss: 80430.4609\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 207353.3750 - val_loss: 139787.7812\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 324787.8125 - val_loss: 174482.4375\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 238476.6094 - val_loss: 110157.3984\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 156675.0312 - val_loss: 259455.9219\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 198152.9375 - val_loss: 118391.9062\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 176272.5469 - val_loss: 443218.2812\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 352575.8125 - val_loss: 323642.7812\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 138541.2031 - val_loss: 146730.1875\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 239767.0938 - val_loss: 328192.4688\n",
      "Epoch 21/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 179342.5156Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 179161.0000 - val_loss: 142082.6875\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 4299135.5000 - val_loss: 891917.8750\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 1097779.7500 - val_loss: 302376.3750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 454015.8438 - val_loss: 298276.2188\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 628791.7500 - val_loss: 240343.4219\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 329836.4062 - val_loss: 447168.0938\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 366354.7188 - val_loss: 645796.9375\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 446621.9062 - val_loss: 147080.9531\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 304177.3750 - val_loss: 175475.9531\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 339382.7500 - val_loss: 360231.4062\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 296897.7812 - val_loss: 739814.9375\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 315587.3125 - val_loss: 1067013.6250\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 310080.5000 - val_loss: 453014.0625\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 332646.8438 - val_loss: 811685.7500\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 278755.1250 - val_loss: 275719.0625\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 188945.1250 - val_loss: 238290.2188\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 230620.2969 - val_loss: 62143.4844\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 235848.8125 - val_loss: 504803.7812\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 356960.5938 - val_loss: 378655.6250\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 266572.7188 - val_loss: 38328.6094\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 325531.2500 - val_loss: 289037.4062\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 248052.0000 - val_loss: 241796.9375\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 152906.4062 - val_loss: 345224.0625\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 217549.2344 - val_loss: 103260.7422\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 148974.8281 - val_loss: 401651.4688\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 143024.2656 - val_loss: 74240.2734\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 146763.7969 - val_loss: 123600.9141\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 166707.8125 - val_loss: 159265.4531\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 157051.9062 - val_loss: 280121.6875\n",
      "Epoch 29/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 219173.2500Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 219030.5000 - val_loss: 175331.1094\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 5134283.0000 - val_loss: 536734.5000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 541897.6250 - val_loss: 1366822.6250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 839272.8750 - val_loss: 634900.2500\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 391665.5625 - val_loss: 360960.8438\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 391065.5312 - val_loss: 515513.7812\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 393544.7188 - val_loss: 287617.3125\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 241149.1719 - val_loss: 398709.8750\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 266930.9688 - val_loss: 798863.7500\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 320633.8125 - val_loss: 263045.8125\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 274394.3750 - val_loss: 224427.7031\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 270794.7188 - val_loss: 65239.1602\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 182118.9844 - val_loss: 468081.1562\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 250894.5781 - val_loss: 201576.0000\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 267470.3750 - val_loss: 67637.7266\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 151252.3125 - val_loss: 137989.0156\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 123337.8281 - val_loss: 282543.8438\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122493.9062 - val_loss: 293998.3125\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 207431.7031 - val_loss: 235510.6719\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 146772.1250 - val_loss: 180955.5000\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 203446.4688 - val_loss: 336638.1875\n",
      "Epoch 21/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 153631.5938Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 153758.4688 - val_loss: 167741.8750\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 92ms/step - loss: 7577610.5000 - val_loss: 669425.6875\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 677940.6250 - val_loss: 830038.2500\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 803184.0000 - val_loss: 682657.6875\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 569473.1875 - val_loss: 529201.0625\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 487310.4062 - val_loss: 447751.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 275869.9688 - val_loss: 106858.6562\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 408370.5312 - val_loss: 1058133.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 436024.1875 - val_loss: 208698.6875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 431853.6250 - val_loss: 92908.8359\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 306940.4062 - val_loss: 180508.9062\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 210178.9531 - val_loss: 213191.0781\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 249294.0000 - val_loss: 127931.3203\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 202980.5156 - val_loss: 78600.0156\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 174036.6250 - val_loss: 145435.3125\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 358889.6250 - val_loss: 61023.5352\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 240853.6875 - val_loss: 432332.9062\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 346635.0312 - val_loss: 38737.1914\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 299703.4688 - val_loss: 192280.1406\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 213501.6094 - val_loss: 45153.1133\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 249684.6875 - val_loss: 33765.2266\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 103087.9609 - val_loss: 228086.8750\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 255057.7812 - val_loss: 214509.9844\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 222007.0781 - val_loss: 292914.1875\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 229594.6250 - val_loss: 68125.9297\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 247603.1406 - val_loss: 605193.3750\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 223377.8906 - val_loss: 35487.8594\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 136215.6562 - val_loss: 326113.2188\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 208405.9062 - val_loss: 155771.6406\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 206951.5312 - val_loss: 377742.5938\n",
      "Epoch 30/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 96305.4688Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 96246.5469 - val_loss: 63480.7578\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 4457907.5000 - val_loss: 636118.1875\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 889949.9375 - val_loss: 326126.3438\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 873538.8750 - val_loss: 337163.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 439100.0000 - val_loss: 452642.0938\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 369570.3438 - val_loss: 321707.1250\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 324626.1875 - val_loss: 240753.8594\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 228001.0469 - val_loss: 141269.2969\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 481287.8750 - val_loss: 365435.5625\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 273818.5000 - val_loss: 1179390.6250\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 507460.3125 - val_loss: 595031.7500\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 339872.2188 - val_loss: 119214.5859\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 208203.1719 - val_loss: 348279.9688\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 271511.9375 - val_loss: 358952.1875\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 230460.8594 - val_loss: 521787.3438\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 291528.9375 - val_loss: 600328.3125\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 267462.9688 - val_loss: 133071.6094\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 152514.6094 - val_loss: 72477.1016\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 194222.7500 - val_loss: 231068.6406\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 209038.5781 - val_loss: 427121.2188\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 166704.2812 - val_loss: 293772.1250\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 161568.8906 - val_loss: 195820.7656\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 144885.4531 - val_loss: 140772.1562\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 167265.1875 - val_loss: 426309.0312\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 245648.7969 - val_loss: 248585.5312\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 142836.9375 - val_loss: 75410.5469\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 110968.3281 - val_loss: 161871.7031\n",
      "Epoch 27/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 158068.5781Restoring model weights from the end of the best epoch: 17.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 158368.9844 - val_loss: 412533.7812\n",
      "Epoch 27: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 4293595.0000 - val_loss: 330478.2812\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 746496.5625 - val_loss: 104780.5312\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 687681.8125 - val_loss: 148448.9531\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 374077.5938 - val_loss: 641374.4375\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 387558.4062 - val_loss: 535291.5625\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 476016.6875 - val_loss: 195595.8125\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 458585.4688 - val_loss: 681451.7500\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 392940.1562 - val_loss: 195596.1250\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 249856.1406 - val_loss: 136480.3438\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 254068.5000 - val_loss: 346625.4688\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 264703.8750 - val_loss: 510969.2188\n",
      "Epoch 12/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 228611.8438Restoring model weights from the end of the best epoch: 2.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 228669.8438 - val_loss: 535835.5625\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 96ms/step - loss: 3621800.0000 - val_loss: 1503089.6250\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 598398.9375 - val_loss: 471729.2812\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 528947.2500 - val_loss: 492205.1562\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 431613.4062 - val_loss: 346955.7500\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 326337.1562 - val_loss: 256369.6406\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 304676.0938 - val_loss: 304122.1875\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 275812.6875 - val_loss: 181996.4375\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 259195.1250 - val_loss: 63385.7969\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 262391.0938 - val_loss: 461936.8125\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 305256.4062 - val_loss: 481658.6562\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 221391.3281 - val_loss: 260106.1406\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 408255.5625 - val_loss: 286708.1562\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 169212.6250 - val_loss: 339775.2812\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 286494.3750 - val_loss: 446532.6875\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 176377.9844 - val_loss: 220115.5625\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 183166.7500 - val_loss: 270834.0625\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 191188.8594 - val_loss: 196303.5938\n",
      "Epoch 18/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 144912.6094Restoring model weights from the end of the best epoch: 8.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 144807.5625 - val_loss: 218219.1719\n",
      "Epoch 18: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 3729671.0000 - val_loss: 3287066.5000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 1161885.0000 - val_loss: 555700.5000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 550546.7500 - val_loss: 819302.8750\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 373959.5312 - val_loss: 377798.7812\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 421801.8438 - val_loss: 396487.3438\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 269758.7500 - val_loss: 983326.3125\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 295872.4375 - val_loss: 379438.2812\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 272497.6250 - val_loss: 574383.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 249898.0938 - val_loss: 236985.3594\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 304614.0312 - val_loss: 387941.6875\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 243298.3594 - val_loss: 352210.2812\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 231877.1562 - val_loss: 501006.0625\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 334230.0625 - val_loss: 412989.4375\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 263778.0938 - val_loss: 138653.2812\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 176112.5312 - val_loss: 222810.9531\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 221550.5625 - val_loss: 247631.6719\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 116853.5859 - val_loss: 208991.6719\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 146948.4062 - val_loss: 573828.4375\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 277367.3438 - val_loss: 51017.0234\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 141032.4375 - val_loss: 220158.8125\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 158170.6094 - val_loss: 438491.5000\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 185169.2500 - val_loss: 202821.4531\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 142015.9688 - val_loss: 283691.9375\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 135662.2500 - val_loss: 493717.6562\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 186496.7656 - val_loss: 202018.6094\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 195555.0938 - val_loss: 242164.7500\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 157440.5469 - val_loss: 43600.7422\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123793.7578 - val_loss: 242093.8594\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 143850.6094 - val_loss: 375293.8125\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121216.6250 - val_loss: 194645.4375\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 105914.0391 - val_loss: 99483.2812\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 117818.1641 - val_loss: 42496.6445\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 107723.6797 - val_loss: 103799.3281\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 113261.9844 - val_loss: 131733.4688\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 98548.5234 - val_loss: 351384.8438\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 105576.3828 - val_loss: 145809.4219\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 179882.5938 - val_loss: 228322.3125\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124125.0625 - val_loss: 62110.8672\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 166342.0156 - val_loss: 125122.9141\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 70324.6172 - val_loss: 54378.6250\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 102063.5000 - val_loss: 128259.6406\n",
      "Epoch 42/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121260.5547Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121191.2109 - val_loss: 43660.5898\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 3340464.5000 - val_loss: 847303.7500\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 750023.2500 - val_loss: 1291351.1250\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 695337.6250 - val_loss: 227800.6719\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 441173.1875 - val_loss: 711542.2500\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 337013.6250 - val_loss: 173552.8125\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 272124.1250 - val_loss: 578484.3125\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 307943.4062 - val_loss: 157473.2188\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 320614.0625 - val_loss: 379329.8750\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 325158.8750 - val_loss: 95693.8047\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 351136.9375 - val_loss: 225517.9375\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 223692.6094 - val_loss: 144939.0625\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 237848.2031 - val_loss: 358197.2188\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 239736.6250 - val_loss: 273579.9062\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 218620.1875 - val_loss: 152963.0312\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 155758.2031 - val_loss: 229890.7344\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 200682.3750 - val_loss: 409272.0000\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 396018.0938 - val_loss: 346552.4062\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 209658.0312 - val_loss: 101280.9453\n",
      "Epoch 19/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 218896.1875Restoring model weights from the end of the best epoch: 9.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 218984.3438 - val_loss: 285110.7188\n",
      "Epoch 19: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 137.9999 - val_loss: 137.6760\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 131.4473 - val_loss: 136.8807\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 128.8197 - val_loss: 132.1757\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 127.5546 - val_loss: 135.6911\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 127.6551 - val_loss: 133.2424\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 126.4924 - val_loss: 130.8888\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.2650 - val_loss: 129.2421\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.5154 - val_loss: 130.3502\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.8597 - val_loss: 129.8122\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.2363 - val_loss: 127.7919\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.9710 - val_loss: 129.3590\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.7742 - val_loss: 130.8072\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.2423 - val_loss: 130.4270\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.1143 - val_loss: 128.6522\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.4300 - val_loss: 129.1511\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.5300 - val_loss: 128.3400\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.9585 - val_loss: 129.3219\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.4686 - val_loss: 129.2178\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.2412 - val_loss: 128.7753\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.0088 - val_loss: 126.3299\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.6863 - val_loss: 125.9590\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.5223 - val_loss: 127.5500\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.7599 - val_loss: 126.6437\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.5891 - val_loss: 126.1138\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.3397 - val_loss: 127.1570\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.3237 - val_loss: 126.9437\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.4303 - val_loss: 125.6190\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.4833 - val_loss: 128.3813\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.7275 - val_loss: 127.0067\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9837 - val_loss: 127.8533\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8131 - val_loss: 127.7979\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.7332 - val_loss: 127.5495\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.8027 - val_loss: 127.1260\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.3287 - val_loss: 127.4845\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.1637 - val_loss: 126.3642\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.3170 - val_loss: 128.2909\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.9790 - val_loss: 125.3458\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.0778 - val_loss: 125.7613\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.3542 - val_loss: 125.8122\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.8443 - val_loss: 125.1623\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.7253 - val_loss: 125.0565\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.6859 - val_loss: 125.9931\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.0343 - val_loss: 126.1949\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8047 - val_loss: 125.7097\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5572 - val_loss: 124.9679\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2524 - val_loss: 125.3210\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.3441 - val_loss: 125.1631\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.4096 - val_loss: 124.8912\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.1691 - val_loss: 126.1157\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8474 - val_loss: 124.9604\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2887 - val_loss: 127.6274\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.2217 - val_loss: 124.9908\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.1205 - val_loss: 124.7899\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.3477 - val_loss: 125.9083\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.2486 - val_loss: 126.0685\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.1580 - val_loss: 125.7338\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.7516 - val_loss: 124.4927\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.6208 - val_loss: 127.5484\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.3668 - val_loss: 125.1224\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.1515 - val_loss: 125.1348\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.3986 - val_loss: 125.9573\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.1987 - val_loss: 125.5949\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.1926 - val_loss: 125.4475\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.0287 - val_loss: 125.2250\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.3946 - val_loss: 125.0608\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.1641 - val_loss: 126.8034\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.1445 - val_loss: 124.3746\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.8363 - val_loss: 125.3248\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.8541 - val_loss: 124.8101\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.8504 - val_loss: 126.6804\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.7817 - val_loss: 125.1515\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.6717 - val_loss: 123.8972\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 119.9181 - val_loss: 124.7061\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.6693 - val_loss: 124.9507\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.6862 - val_loss: 125.1254\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.5710 - val_loss: 125.9667\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.5043 - val_loss: 124.6662\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.3663 - val_loss: 123.9582\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.1812 - val_loss: 124.7992\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.5695 - val_loss: 123.7408\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.1036 - val_loss: 125.5372\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.3113 - val_loss: 124.9892\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.5776 - val_loss: 124.2934\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.9660 - val_loss: 125.7987\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.2361 - val_loss: 124.7062\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 119.3232 - val_loss: 124.6615\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 119.8416 - val_loss: 124.8687\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.7671 - val_loss: 126.1594\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.6574 - val_loss: 126.8692\n",
      "Epoch 90/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 119.5621Restoring model weights from the end of the best epoch: 80.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 119.5565 - val_loss: 126.5645\n",
      "Epoch 90: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 140.4792 - val_loss: 137.6028\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 130.6021 - val_loss: 137.4132\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 129.4831 - val_loss: 133.5539\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 128.8671 - val_loss: 132.8553\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 127.4606 - val_loss: 137.3532\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 128.5816 - val_loss: 132.8652\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 126.1267 - val_loss: 131.1970\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 125.2696 - val_loss: 129.9984\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 125.8883 - val_loss: 130.0834\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.6092 - val_loss: 130.8761\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 125.2534 - val_loss: 129.4805\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.8395 - val_loss: 128.4654\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.6704 - val_loss: 132.4217\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.5902 - val_loss: 129.3507\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.7722 - val_loss: 128.0455\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.3246 - val_loss: 128.6465\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.5619 - val_loss: 128.2697\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.8351 - val_loss: 127.9028\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.7524 - val_loss: 126.3943\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.0998 - val_loss: 127.3382\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.0774 - val_loss: 125.9226\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.8702 - val_loss: 127.1022\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.4523 - val_loss: 129.4787\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.3916 - val_loss: 127.0101\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.3655 - val_loss: 130.3676\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.1164 - val_loss: 129.5697\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.8787 - val_loss: 128.3626\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.8309 - val_loss: 126.5537\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9998 - val_loss: 125.9192\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.7239 - val_loss: 127.4107\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9059 - val_loss: 126.8047\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.8135 - val_loss: 127.3466\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.9021 - val_loss: 125.2000\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.1057 - val_loss: 126.4202\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9633 - val_loss: 126.2181\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8898 - val_loss: 126.5072\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8129 - val_loss: 124.8425\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8151 - val_loss: 126.3595\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8232 - val_loss: 127.2947\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.1581 - val_loss: 125.3730\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7329 - val_loss: 125.5160\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.4899 - val_loss: 124.6448\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.9855 - val_loss: 124.5773\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.6371 - val_loss: 125.3191\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2138 - val_loss: 125.5646\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.4003 - val_loss: 126.5758\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.2663 - val_loss: 125.3158\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.3537 - val_loss: 125.8294\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.3679 - val_loss: 124.8803\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.3312 - val_loss: 125.1261\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.0619 - val_loss: 126.6711\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.2199 - val_loss: 126.3342\n",
      "Epoch 53/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.7256Restoring model weights from the end of the best epoch: 43.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7255 - val_loss: 126.4582\n",
      "Epoch 53: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 93ms/step - loss: 141.4466 - val_loss: 138.4299\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 132.5311 - val_loss: 137.4106\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 130.8420 - val_loss: 134.9639\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 128.1317 - val_loss: 133.4969\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 127.2828 - val_loss: 131.1304\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.7270 - val_loss: 132.4931\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.9112 - val_loss: 129.3421\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.6448 - val_loss: 130.7398\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.5287 - val_loss: 130.5194\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.2220 - val_loss: 128.2090\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.1175 - val_loss: 127.1571\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.1344 - val_loss: 130.6408\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.7397 - val_loss: 126.8881\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.6774 - val_loss: 130.0574\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.1813 - val_loss: 131.3429\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.5171 - val_loss: 128.4728\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.0125 - val_loss: 131.6314\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.3434 - val_loss: 129.8245\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.4092 - val_loss: 130.2666\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.9160 - val_loss: 128.2478\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.6575 - val_loss: 130.4506\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.4434 - val_loss: 129.1232\n",
      "Epoch 23/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 123.5551Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.6454 - val_loss: 131.3512\n",
      "Epoch 23: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 199.0263 - val_loss: 146.1225\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 135.2566 - val_loss: 135.1376\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 131.7126 - val_loss: 132.7461\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 127.5770 - val_loss: 130.6507\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 128.0153 - val_loss: 133.6669\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 127.1088 - val_loss: 131.6542\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.6206 - val_loss: 130.6624\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125.7533 - val_loss: 130.2150\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.4870 - val_loss: 130.1150\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.3544 - val_loss: 129.3695\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 125.2746 - val_loss: 131.9621\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 125.3100 - val_loss: 134.8374\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.4466 - val_loss: 128.8544\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.9375 - val_loss: 130.2985\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.8175 - val_loss: 128.6638\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.4068 - val_loss: 128.7593\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.6254 - val_loss: 126.9556\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.6098 - val_loss: 129.4820\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.4424 - val_loss: 129.2977\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.8388 - val_loss: 130.6983\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.2834 - val_loss: 130.5582\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.6316 - val_loss: 126.5533\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.7057 - val_loss: 128.6891\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.3681 - val_loss: 129.0767\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.0807 - val_loss: 127.8656\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.4749 - val_loss: 127.7411\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.2044 - val_loss: 126.7715\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.6847 - val_loss: 128.0036\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1848 - val_loss: 127.7430\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.5769 - val_loss: 126.5449\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5446 - val_loss: 128.0860\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.3448 - val_loss: 127.0730\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1212 - val_loss: 126.3302\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9232 - val_loss: 127.6974\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5685 - val_loss: 126.4745\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.9000 - val_loss: 126.2205\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5952 - val_loss: 125.6424\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5954 - val_loss: 125.8405\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.7866 - val_loss: 125.4879\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.7184 - val_loss: 127.7005\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.6261 - val_loss: 127.7436\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.2932 - val_loss: 126.0274\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5279 - val_loss: 128.0900\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.6700 - val_loss: 125.7327\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.4859 - val_loss: 127.1393\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9910 - val_loss: 125.9876\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2545 - val_loss: 125.5249\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.0131 - val_loss: 124.4390\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2981 - val_loss: 126.2191\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0348 - val_loss: 125.4652\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.2261 - val_loss: 125.5977\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.3066 - val_loss: 125.5332\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 119.8872 - val_loss: 126.8308\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.6943 - val_loss: 125.0031\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.6903 - val_loss: 126.7759\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.2728 - val_loss: 126.0805\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5100 - val_loss: 125.7583\n",
      "Epoch 58/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.1497Restoring model weights from the end of the best epoch: 48.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.1045 - val_loss: 125.5507\n",
      "Epoch 58: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 137.9803 - val_loss: 136.9707\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 135.5605 - val_loss: 138.0611\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 130.8198 - val_loss: 133.9578\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 129.6201 - val_loss: 135.6554\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 128.0919 - val_loss: 133.3698\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 128.9242 - val_loss: 132.9285\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.8255 - val_loss: 132.0999\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 125.5275 - val_loss: 129.9648\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.7425 - val_loss: 130.4955\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 125.1537 - val_loss: 132.2783\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 125.2843 - val_loss: 129.9062\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.5059 - val_loss: 132.3629\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 126.0375 - val_loss: 130.0505\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.5461 - val_loss: 128.3778\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.8284 - val_loss: 128.0613\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.8067 - val_loss: 130.0916\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.2285 - val_loss: 130.4774\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.5551 - val_loss: 129.0989\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.8027 - val_loss: 127.9308\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.3531 - val_loss: 127.2969\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.3381 - val_loss: 128.0087\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.4332 - val_loss: 128.2639\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.0393 - val_loss: 128.8980\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 126.7792 - val_loss: 130.2473\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.5269 - val_loss: 129.3201\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.6031 - val_loss: 128.4485\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.7994 - val_loss: 128.5984\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.9233 - val_loss: 129.7583\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.5221 - val_loss: 128.3048\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.2375 - val_loss: 126.5491\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.1814 - val_loss: 128.5478\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.9588 - val_loss: 126.0353\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0714 - val_loss: 126.6786\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.4316 - val_loss: 127.5332\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.6353 - val_loss: 126.4735\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.4358 - val_loss: 126.5001\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8835 - val_loss: 127.0250\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7598 - val_loss: 126.7301\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.9329 - val_loss: 125.1057\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7636 - val_loss: 126.1187\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.2173 - val_loss: 125.8445\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0966 - val_loss: 127.6404\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.6237 - val_loss: 125.1129\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.4541 - val_loss: 125.3547\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0693 - val_loss: 125.9867\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.1995 - val_loss: 125.9881\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.2363 - val_loss: 125.4727\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.0975 - val_loss: 126.2717\n",
      "Epoch 49/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 121.1819Restoring model weights from the end of the best epoch: 39.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.1161 - val_loss: 126.1950\n",
      "Epoch 49: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 138.2289 - val_loss: 138.5850\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 131.5038 - val_loss: 133.7070\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 127.9751 - val_loss: 133.9031\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 128.8520 - val_loss: 134.4240\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 127.2059 - val_loss: 133.5059\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 126.1619 - val_loss: 131.6693\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 126.2816 - val_loss: 132.6702\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.0571 - val_loss: 132.7759\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 125.0213 - val_loss: 130.4262\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 124.4295 - val_loss: 132.9771\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 127.2925 - val_loss: 129.1090\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.2427 - val_loss: 130.0860\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125.0976 - val_loss: 129.0788\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.6576 - val_loss: 129.1922\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.9578 - val_loss: 128.4293\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.1788 - val_loss: 129.8185\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.6350 - val_loss: 127.9278\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.1079 - val_loss: 129.3928\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.5808 - val_loss: 127.2363\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 122.0294 - val_loss: 126.4450\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.0160 - val_loss: 126.2755\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.9341 - val_loss: 130.4779\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.3077 - val_loss: 128.6399\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 121.7973 - val_loss: 127.5836\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.2532 - val_loss: 126.2860\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.3575 - val_loss: 127.8719\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.4918 - val_loss: 125.8344\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.6895 - val_loss: 126.4180\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.9634 - val_loss: 125.7138\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.5197 - val_loss: 126.3060\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.0868 - val_loss: 125.2975\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.3621 - val_loss: 126.8999\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.0875 - val_loss: 127.3051\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.4592 - val_loss: 126.3712\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.7349 - val_loss: 126.6534\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 122.1194 - val_loss: 127.8776\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1703 - val_loss: 126.1623\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.3918 - val_loss: 128.0972\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.0691 - val_loss: 128.6347\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.9420 - val_loss: 125.2676\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.5342 - val_loss: 127.6288\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.7535 - val_loss: 127.5094\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 121.3553 - val_loss: 128.3307\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.6313 - val_loss: 127.3170\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.7019 - val_loss: 125.3160\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2739 - val_loss: 125.1108\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8213 - val_loss: 125.4170\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8078 - val_loss: 127.7066\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.2436 - val_loss: 127.2305\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.5608 - val_loss: 126.2441\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.5164 - val_loss: 124.7342\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.3497 - val_loss: 125.8273\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.3611 - val_loss: 124.7251\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.2403 - val_loss: 124.6961\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.3328 - val_loss: 126.1415\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.5192 - val_loss: 126.7538\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.9655 - val_loss: 125.3599\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.6820 - val_loss: 126.0499\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.4470 - val_loss: 126.0554\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.4890 - val_loss: 125.5457\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.7029 - val_loss: 125.2469\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.4779 - val_loss: 125.0341\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.2477 - val_loss: 125.1730\n",
      "Epoch 64/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.6161Restoring model weights from the end of the best epoch: 54.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.5853 - val_loss: 125.0177\n",
      "Epoch 64: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 138.6140 - val_loss: 136.3368\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 131.7629 - val_loss: 134.6468\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 129.2253 - val_loss: 132.9792\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 128.5401 - val_loss: 135.4465\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 128.3099 - val_loss: 131.7920\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 127.1303 - val_loss: 134.2586\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 126.9294 - val_loss: 130.9496\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.4735 - val_loss: 134.3321\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 125.7891 - val_loss: 129.8970\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.2314 - val_loss: 129.8355\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.7440 - val_loss: 130.2199\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.7136 - val_loss: 131.8976\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.8223 - val_loss: 128.5014\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.4421 - val_loss: 130.2980\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.1545 - val_loss: 131.0166\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.5056 - val_loss: 128.9778\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.7392 - val_loss: 129.4867\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.6510 - val_loss: 129.8665\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.9764 - val_loss: 129.4019\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.2479 - val_loss: 127.1501\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.7456 - val_loss: 127.9400\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.1479 - val_loss: 128.1015\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.3621 - val_loss: 129.1321\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.8301 - val_loss: 125.9268\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.8572 - val_loss: 126.7403\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.1400 - val_loss: 129.4736\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.3381 - val_loss: 128.0269\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.0850 - val_loss: 127.8167\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.9739 - val_loss: 127.6355\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.2490 - val_loss: 126.9481\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.2239 - val_loss: 125.9320\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.7693 - val_loss: 125.8173\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.4323 - val_loss: 125.8970\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9240 - val_loss: 127.1062\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.4924 - val_loss: 125.6710\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.6476 - val_loss: 126.5922\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.7552 - val_loss: 128.5105\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 122.3550 - val_loss: 126.5977\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.6178 - val_loss: 125.3140\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.9094 - val_loss: 125.9762\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9650 - val_loss: 126.4148\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9378 - val_loss: 125.9647\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8311 - val_loss: 125.0119\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.4424 - val_loss: 126.2292\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9792 - val_loss: 127.4902\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.9706 - val_loss: 126.2504\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8194 - val_loss: 124.8948\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.7854 - val_loss: 129.8584\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.8418 - val_loss: 125.4627\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.7485 - val_loss: 125.0010\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.6125 - val_loss: 124.7892\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.1941 - val_loss: 126.3206\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.4778 - val_loss: 125.4367\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9554 - val_loss: 125.7268\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.6594 - val_loss: 125.5781\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.4189 - val_loss: 126.4270\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.1254 - val_loss: 125.6882\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.0309 - val_loss: 125.1761\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.1199 - val_loss: 125.4280\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.4012 - val_loss: 129.0334\n",
      "Epoch 61/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 122.4187Restoring model weights from the end of the best epoch: 51.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.3614 - val_loss: 124.8227\n",
      "Epoch 61: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 136.1216 - val_loss: 136.2337\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 131.6021 - val_loss: 135.4176\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 129.8893 - val_loss: 134.4518\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 127.7366 - val_loss: 131.5894\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 127.1642 - val_loss: 131.9712\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 126.1597 - val_loss: 130.9188\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.7859 - val_loss: 130.2995\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.9857 - val_loss: 129.3019\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 125.5938 - val_loss: 130.1990\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 124.4592 - val_loss: 129.8952\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.9412 - val_loss: 130.0320\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 124.7957 - val_loss: 131.3836\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 124.3393 - val_loss: 129.7486\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 123.1807 - val_loss: 129.4745\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 123.4208 - val_loss: 128.6377\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.9924 - val_loss: 128.0989\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.7213 - val_loss: 127.4471\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 123.7774 - val_loss: 129.3484\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.7267 - val_loss: 127.4502\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 122.2576 - val_loss: 127.7399\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.6949 - val_loss: 125.8749\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.7174 - val_loss: 127.5005\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.6842 - val_loss: 129.0262\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 122.8683 - val_loss: 127.3888\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.2445 - val_loss: 125.8180\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 121.5924 - val_loss: 128.1940\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121.3132 - val_loss: 128.4454\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.5185 - val_loss: 127.0449\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.1282 - val_loss: 126.2505\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8551 - val_loss: 126.1103\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.1349 - val_loss: 125.9161\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.4267 - val_loss: 126.2563\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8800 - val_loss: 125.4772\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.8029 - val_loss: 126.1448\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.8294 - val_loss: 126.3892\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.2469 - val_loss: 126.2169\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.6718 - val_loss: 126.8343\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 121.8275 - val_loss: 128.0030\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.9849 - val_loss: 126.0712\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.5105 - val_loss: 125.7244\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.9050 - val_loss: 126.3363\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.6008 - val_loss: 126.2943\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.4674 - val_loss: 125.3471\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.5503 - val_loss: 126.3986\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.3644 - val_loss: 125.6467\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.6960 - val_loss: 127.4846\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 121.2973 - val_loss: 124.7490\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 120.9700 - val_loss: 125.3924\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.1649 - val_loss: 127.0341\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 119.9280 - val_loss: 124.8782\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.3321 - val_loss: 126.1105\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.7565 - val_loss: 125.1449\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.1856 - val_loss: 125.4176\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 119.8275 - val_loss: 124.3527\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 119.9816 - val_loss: 125.0551\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.0469 - val_loss: 125.2305\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.5656 - val_loss: 127.0903\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.4024 - val_loss: 126.3856\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.2930 - val_loss: 125.9364\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.2436 - val_loss: 124.6234\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 119.8676 - val_loss: 124.6477\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.0386 - val_loss: 126.1871\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 120.0674 - val_loss: 127.1706\n",
      "Epoch 64/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.2451Restoring model weights from the end of the best epoch: 54.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 120.2867 - val_loss: 126.1420\n",
      "Epoch 64: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 94ms/step - loss: 140.1486 - val_loss: 138.3159\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 130.6602 - val_loss: 133.8404\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 129.9503 - val_loss: 131.9664\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 128.7318 - val_loss: 132.8613\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 127.1228 - val_loss: 132.9224\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 126.2444 - val_loss: 131.2346\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 125.7892 - val_loss: 130.0925\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 125.4031 - val_loss: 131.2196\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 124.5406 - val_loss: 129.8145\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 124.4439 - val_loss: 129.7648\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 123.6006 - val_loss: 128.8683\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 123.1099 - val_loss: 127.9320\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 123.8276 - val_loss: 127.5304\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.3603 - val_loss: 126.9167\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.9818 - val_loss: 129.5269\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 123.8090 - val_loss: 127.5666\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.7077 - val_loss: 126.7416\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.9245 - val_loss: 127.4228\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.8867 - val_loss: 128.9835\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.2202 - val_loss: 126.7161\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.7947 - val_loss: 128.6255\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.9761 - val_loss: 126.1413\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.5225 - val_loss: 127.0154\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.6679 - val_loss: 126.4788\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.2027 - val_loss: 126.4567\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8478 - val_loss: 125.8417\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 121.1311 - val_loss: 126.6221\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.0514 - val_loss: 125.9747\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 121.0092 - val_loss: 126.2418\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.8459 - val_loss: 126.1958\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.5042 - val_loss: 126.5103\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0748 - val_loss: 126.6034\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.4905 - val_loss: 125.9134\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.8008 - val_loss: 125.5895\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.8604 - val_loss: 125.7092\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.9267 - val_loss: 125.5071\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.9442 - val_loss: 125.6112\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.5017 - val_loss: 125.7911\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.2934 - val_loss: 125.8811\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.2642 - val_loss: 125.3024\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.8959 - val_loss: 125.4332\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.7663 - val_loss: 125.8527\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.1691 - val_loss: 124.6979\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.4230 - val_loss: 126.9547\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.6094 - val_loss: 125.6879\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.4163 - val_loss: 125.0630\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.0730 - val_loss: 125.5214\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 120.6053 - val_loss: 125.9055\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 123.1457 - val_loss: 126.1851\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0090 - val_loss: 126.5197\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 120.5323 - val_loss: 126.0932\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 120.4684 - val_loss: 125.2957\n",
      "Epoch 53/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.4389Restoring model weights from the end of the best epoch: 43.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.4368 - val_loss: 126.3256\n",
      "Epoch 53: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 138.5707 - val_loss: 140.3309\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 131.2051 - val_loss: 136.2213\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 129.3675 - val_loss: 134.7310\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 127.4838 - val_loss: 134.0826\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 126.4449 - val_loss: 132.4315\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 125.8288 - val_loss: 130.1840\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 125.3282 - val_loss: 129.3549\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 125.0658 - val_loss: 131.2556\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.6340 - val_loss: 128.6377\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 124.2646 - val_loss: 128.6898\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 124.3160 - val_loss: 129.9507\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 123.0771 - val_loss: 128.6158\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.6218 - val_loss: 129.6964\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.9304 - val_loss: 127.8350\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.8779 - val_loss: 126.7647\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 123.1877 - val_loss: 129.5831\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.4409 - val_loss: 128.1121\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 122.2240 - val_loss: 128.0181\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 122.2116 - val_loss: 127.7371\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.0302 - val_loss: 126.7936\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 122.2000 - val_loss: 127.2441\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.3308 - val_loss: 127.0543\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.6638 - val_loss: 127.7767\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.1778 - val_loss: 127.5319\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.3756 - val_loss: 125.3596\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.1104 - val_loss: 125.6117\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.3566 - val_loss: 125.8095\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.4727 - val_loss: 126.8670\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0311 - val_loss: 127.1182\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.8553 - val_loss: 127.6358\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.8388 - val_loss: 127.8292\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.3826 - val_loss: 125.1575\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.9877 - val_loss: 125.6674\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.8550 - val_loss: 125.1081\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.9425 - val_loss: 125.3201\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.4785 - val_loss: 127.0229\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0620 - val_loss: 126.2766\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 120.7736 - val_loss: 127.3676\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 122.4813 - val_loss: 126.9328\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 121.2928 - val_loss: 128.0879\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 121.0981 - val_loss: 125.8462\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 120.4940 - val_loss: 126.6502\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.0927 - val_loss: 125.1863\n",
      "Epoch 44/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 120.8390Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 120.8296 - val_loss: 125.7504\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 0.3969 - val_loss: 0.2612\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2999 - val_loss: 0.2474\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2826 - val_loss: 0.2424\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2773 - val_loss: 0.2725\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2763 - val_loss: 0.2186\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2699 - val_loss: 0.2026\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2629 - val_loss: 0.2112\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2644 - val_loss: 0.2134\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2626 - val_loss: 0.2096\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2628 - val_loss: 0.2492\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2675 - val_loss: 0.2136\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2588 - val_loss: 0.2116\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2605 - val_loss: 0.1964\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2575 - val_loss: 0.2358\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2633 - val_loss: 0.2271\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2588 - val_loss: 0.1977\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2544 - val_loss: 0.2114\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2546 - val_loss: 0.1993\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2596 - val_loss: 0.2032\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2527 - val_loss: 0.2030\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2564 - val_loss: 0.2091\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2545 - val_loss: 0.2061\n",
      "Epoch 23/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2571Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2574 - val_loss: 0.2042\n",
      "Epoch 23: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 0.4215 - val_loss: 0.2672\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3016 - val_loss: 0.2302\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2808 - val_loss: 0.2272\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2778 - val_loss: 0.2118\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2691 - val_loss: 0.2065\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2659 - val_loss: 0.2094\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2647 - val_loss: 0.2200\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2638 - val_loss: 0.2029\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2654 - val_loss: 0.2056\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2612 - val_loss: 0.2013\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2612 - val_loss: 0.1988\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2626 - val_loss: 0.2062\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2573 - val_loss: 0.2041\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2602 - val_loss: 0.2034\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2598 - val_loss: 0.1981\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2591 - val_loss: 0.2021\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2560 - val_loss: 0.2053\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2568 - val_loss: 0.1959\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2613 - val_loss: 0.2120\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2567 - val_loss: 0.2027\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2548 - val_loss: 0.2014\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2622 - val_loss: 0.1983\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2562 - val_loss: 0.1997\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2556 - val_loss: 0.1992\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2529 - val_loss: 0.1962\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2533 - val_loss: 0.1948\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2552 - val_loss: 0.2020\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2532 - val_loss: 0.1953\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2547 - val_loss: 0.1987\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2570 - val_loss: 0.1972\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2523 - val_loss: 0.1914\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2523 - val_loss: 0.2005\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2542 - val_loss: 0.1970\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2532 - val_loss: 0.1943\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2534 - val_loss: 0.1948\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2515 - val_loss: 0.1920\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2525 - val_loss: 0.2063\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2509 - val_loss: 0.2073\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2547 - val_loss: 0.2062\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2522 - val_loss: 0.1929\n",
      "Epoch 41/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2531Restoring model weights from the end of the best epoch: 31.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2530 - val_loss: 0.1951\n",
      "Epoch 41: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 0.4115 - val_loss: 0.2423\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2877 - val_loss: 0.2261\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2855 - val_loss: 0.2151\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2746 - val_loss: 0.2108\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2721 - val_loss: 0.2139\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2709 - val_loss: 0.2134\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2655 - val_loss: 0.2350\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2670 - val_loss: 0.2025\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2629 - val_loss: 0.2023\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2595 - val_loss: 0.1980\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2576 - val_loss: 0.2449\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2681 - val_loss: 0.2042\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2588 - val_loss: 0.1998\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2595 - val_loss: 0.2168\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2559 - val_loss: 0.2072\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2557 - val_loss: 0.2052\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2553 - val_loss: 0.2039\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2574 - val_loss: 0.2004\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2548 - val_loss: 0.2002\n",
      "Epoch 20/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2563Restoring model weights from the end of the best epoch: 10.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2562 - val_loss: 0.2009\n",
      "Epoch 20: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 0.4163 - val_loss: 0.2473\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2931 - val_loss: 0.2384\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2823 - val_loss: 0.2207\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2787 - val_loss: 0.2412\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2710 - val_loss: 0.2112\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2701 - val_loss: 0.2012\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2672 - val_loss: 0.2086\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2639 - val_loss: 0.2129\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2626 - val_loss: 0.1993\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2639 - val_loss: 0.2110\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2606 - val_loss: 0.2079\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2638 - val_loss: 0.2117\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2592 - val_loss: 0.2073\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2669 - val_loss: 0.2001\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2602 - val_loss: 0.2021\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2571 - val_loss: 0.1952\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2580 - val_loss: 0.2008\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2573 - val_loss: 0.1982\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2552 - val_loss: 0.1943\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2543 - val_loss: 0.2156\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2584 - val_loss: 0.1973\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2558 - val_loss: 0.2008\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2571 - val_loss: 0.2078\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2549 - val_loss: 0.2064\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2563 - val_loss: 0.2037\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2538 - val_loss: 0.1986\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2561 - val_loss: 0.2023\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2538 - val_loss: 0.2024\n",
      "Epoch 29/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2550Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2550 - val_loss: 0.1977\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 0.4098 - val_loss: 0.2787\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2971 - val_loss: 0.2352\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2760 - val_loss: 0.2857\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2777 - val_loss: 0.2373\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2719 - val_loss: 0.2256\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2716 - val_loss: 0.2140\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2771 - val_loss: 0.2065\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2646 - val_loss: 0.2098\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2645 - val_loss: 0.2072\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2658 - val_loss: 0.2100\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2642 - val_loss: 0.2090\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2578 - val_loss: 0.2179\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2589 - val_loss: 0.2208\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2624 - val_loss: 0.2045\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2589 - val_loss: 0.1990\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2579 - val_loss: 0.2038\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2571 - val_loss: 0.2094\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2578 - val_loss: 0.2031\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2578 - val_loss: 0.2014\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2559 - val_loss: 0.2011\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2570 - val_loss: 0.2138\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2542 - val_loss: 0.2082\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2565 - val_loss: 0.1990\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2575 - val_loss: 0.2109\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2546 - val_loss: 0.1946\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2534 - val_loss: 0.2009\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2551 - val_loss: 0.2162\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2552 - val_loss: 0.2036\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2544 - val_loss: 0.2031\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2549 - val_loss: 0.1953\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2507 - val_loss: 0.1952\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2496 - val_loss: 0.2036\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2537 - val_loss: 0.1982\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2519 - val_loss: 0.2159\n",
      "Epoch 35/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2531Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2535 - val_loss: 0.2009\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 0.3976 - val_loss: 0.2747\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2947 - val_loss: 0.2615\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2885 - val_loss: 0.2149\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2766 - val_loss: 0.2245\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2717 - val_loss: 0.2164\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2754 - val_loss: 0.2132\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2641 - val_loss: 0.2089\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2648 - val_loss: 0.2367\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2654 - val_loss: 0.2035\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2648 - val_loss: 0.2022\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2638 - val_loss: 0.2115\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2586 - val_loss: 0.2017\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2555 - val_loss: 0.2261\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2608 - val_loss: 0.2066\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2627 - val_loss: 0.1997\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2585 - val_loss: 0.2103\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2594 - val_loss: 0.2062\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2570 - val_loss: 0.1972\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2584 - val_loss: 0.2038\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2581 - val_loss: 0.2080\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2571 - val_loss: 0.2074\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2553 - val_loss: 0.1966\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2534 - val_loss: 0.1977\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2526 - val_loss: 0.2226\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2562 - val_loss: 0.2049\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2526 - val_loss: 0.1972\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2542 - val_loss: 0.1972\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2533 - val_loss: 0.2065\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2546 - val_loss: 0.2007\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2505 - val_loss: 0.2158\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2528 - val_loss: 0.1957\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2539 - val_loss: 0.1938\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2529 - val_loss: 0.1993\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2519 - val_loss: 0.2038\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2533 - val_loss: 0.1960\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2529 - val_loss: 0.2007\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2511 - val_loss: 0.1933\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2525 - val_loss: 0.1970\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2505 - val_loss: 0.2139\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2555 - val_loss: 0.1975\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2511 - val_loss: 0.1947\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2512 - val_loss: 0.1952\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2536 - val_loss: 0.1986\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2526 - val_loss: 0.2016\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2571 - val_loss: 0.1960\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2513 - val_loss: 0.1937\n",
      "Epoch 47/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2525Restoring model weights from the end of the best epoch: 37.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2528 - val_loss: 0.1980\n",
      "Epoch 47: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 0.4086 - val_loss: 0.2512\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3011 - val_loss: 0.2210\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2833 - val_loss: 0.2343\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2817 - val_loss: 0.2172\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2702 - val_loss: 0.2165\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2671 - val_loss: 0.2230\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2720 - val_loss: 0.2122\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2662 - val_loss: 0.2047\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2646 - val_loss: 0.2141\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2627 - val_loss: 0.2085\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2620 - val_loss: 0.2202\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2620 - val_loss: 0.2121\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2626 - val_loss: 0.2059\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2597 - val_loss: 0.2034\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2615 - val_loss: 0.2155\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2561 - val_loss: 0.2061\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2601 - val_loss: 0.2027\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2565 - val_loss: 0.2327\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2595 - val_loss: 0.2024\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2558 - val_loss: 0.2027\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2578 - val_loss: 0.2018\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2538 - val_loss: 0.2081\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2603 - val_loss: 0.2082\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2559 - val_loss: 0.1973\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2557 - val_loss: 0.2056\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2570 - val_loss: 0.1966\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2518 - val_loss: 0.1969\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2547 - val_loss: 0.1958\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2543 - val_loss: 0.1928\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2568 - val_loss: 0.2200\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2545 - val_loss: 0.2008\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2540 - val_loss: 0.1999\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2518 - val_loss: 0.1959\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2517 - val_loss: 0.1962\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2519 - val_loss: 0.1945\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2486 - val_loss: 0.1964\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2539 - val_loss: 0.1968\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2543 - val_loss: 0.1934\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2531 - val_loss: 0.1922\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2512 - val_loss: 0.2109\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2561 - val_loss: 0.1999\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2516 - val_loss: 0.1970\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2528 - val_loss: 0.1994\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2521 - val_loss: 0.1945\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2510 - val_loss: 0.1932\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2515 - val_loss: 0.1950\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2529 - val_loss: 0.2117\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2521 - val_loss: 0.2188\n",
      "Epoch 49/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2581Restoring model weights from the end of the best epoch: 39.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2578 - val_loss: 0.2031\n",
      "Epoch 49: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 0.4171 - val_loss: 0.2599\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2967 - val_loss: 0.2413\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2816 - val_loss: 0.2349\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2808 - val_loss: 0.2180\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2723 - val_loss: 0.2508\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2740 - val_loss: 0.2109\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2700 - val_loss: 0.2157\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2651 - val_loss: 0.2100\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2664 - val_loss: 0.2177\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2702 - val_loss: 0.2249\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2625 - val_loss: 0.1991\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2578 - val_loss: 0.2029\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2590 - val_loss: 0.2007\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2574 - val_loss: 0.2030\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2593 - val_loss: 0.1999\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2577 - val_loss: 0.2187\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2594 - val_loss: 0.2148\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2592 - val_loss: 0.1954\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2588 - val_loss: 0.2023\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2543 - val_loss: 0.2126\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2578 - val_loss: 0.2054\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2538 - val_loss: 0.1963\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2555 - val_loss: 0.1998\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2553 - val_loss: 0.2013\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2539 - val_loss: 0.2014\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2541 - val_loss: 0.2032\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2558 - val_loss: 0.1932\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2510 - val_loss: 0.1948\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2537 - val_loss: 0.1987\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2560 - val_loss: 0.2054\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2542 - val_loss: 0.2039\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2528 - val_loss: 0.1930\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2546 - val_loss: 0.2030\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2517 - val_loss: 0.2033\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2586 - val_loss: 0.2014\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2529 - val_loss: 0.1978\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2522 - val_loss: 0.1957\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2527 - val_loss: 0.1987\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2518 - val_loss: 0.2005\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2545 - val_loss: 0.2176\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2551 - val_loss: 0.2081\n",
      "Epoch 42/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2508Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2507 - val_loss: 0.2046\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 0.4000 - val_loss: 0.2423\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2943 - val_loss: 0.2429\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2825 - val_loss: 0.2202\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2785 - val_loss: 0.2282\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2768 - val_loss: 0.2280\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2725 - val_loss: 0.2186\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2691 - val_loss: 0.2199\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2691 - val_loss: 0.2067\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2668 - val_loss: 0.2109\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2615 - val_loss: 0.2174\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2603 - val_loss: 0.2038\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2636 - val_loss: 0.2041\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2613 - val_loss: 0.2061\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2573 - val_loss: 0.2020\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2643 - val_loss: 0.2058\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2595 - val_loss: 0.2060\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2613 - val_loss: 0.1991\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2558 - val_loss: 0.1986\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2574 - val_loss: 0.1976\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2592 - val_loss: 0.2048\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2575 - val_loss: 0.1997\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2608 - val_loss: 0.2005\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2554 - val_loss: 0.2058\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2568 - val_loss: 0.2087\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2552 - val_loss: 0.2018\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2516 - val_loss: 0.2089\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2607 - val_loss: 0.2027\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2577 - val_loss: 0.2057\n",
      "Epoch 29/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2548Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2545 - val_loss: 0.2005\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 0.4224 - val_loss: 0.2741\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2922 - val_loss: 0.2272\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2821 - val_loss: 0.2192\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2814 - val_loss: 0.2190\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2685 - val_loss: 0.2179\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2730 - val_loss: 0.2103\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2678 - val_loss: 0.2101\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2661 - val_loss: 0.2110\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2668 - val_loss: 0.2086\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2632 - val_loss: 0.2095\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2648 - val_loss: 0.2362\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2722 - val_loss: 0.2070\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2621 - val_loss: 0.2026\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2560 - val_loss: 0.2117\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2636 - val_loss: 0.2063\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2604 - val_loss: 0.2049\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2567 - val_loss: 0.2038\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2581 - val_loss: 0.2041\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2541 - val_loss: 0.2071\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2598 - val_loss: 0.1990\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2582 - val_loss: 0.1973\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2575 - val_loss: 0.2014\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2555 - val_loss: 0.1979\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2513 - val_loss: 0.2072\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2542 - val_loss: 0.1962\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2526 - val_loss: 0.1974\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2526 - val_loss: 0.2059\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2517 - val_loss: 0.2003\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2518 - val_loss: 0.1996\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2534 - val_loss: 0.1991\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2553 - val_loss: 0.2035\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2532 - val_loss: 0.1977\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2528 - val_loss: 0.2014\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2551 - val_loss: 0.1978\n",
      "Epoch 35/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2520Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2519 - val_loss: 0.2047\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 0.4652 - val_loss: 0.2012\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2775 - val_loss: 0.2137\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2660 - val_loss: 0.1786\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2598 - val_loss: 0.1781\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2577 - val_loss: 0.1764\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2575 - val_loss: 0.2130\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2535 - val_loss: 0.2055\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2610 - val_loss: 0.1812\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2535 - val_loss: 0.1741\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2480 - val_loss: 0.1704\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2469 - val_loss: 0.1795\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2476 - val_loss: 0.1872\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2441 - val_loss: 0.1806\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2525 - val_loss: 0.1817\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2465 - val_loss: 0.1705\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2400 - val_loss: 0.1784\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2448 - val_loss: 0.1726\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2408 - val_loss: 0.1680\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2409 - val_loss: 0.1862\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2397 - val_loss: 0.1769\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2466 - val_loss: 0.1730\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2360 - val_loss: 0.1858\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2429 - val_loss: 0.1773\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2382 - val_loss: 0.1765\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2402 - val_loss: 0.1759\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2410 - val_loss: 0.1686\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2401 - val_loss: 0.1657\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2397 - val_loss: 0.1772\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2390 - val_loss: 0.1672\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2351 - val_loss: 0.1684\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2414 - val_loss: 0.1736\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2386 - val_loss: 0.1789\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2420 - val_loss: 0.1674\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2319 - val_loss: 0.1692\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2387 - val_loss: 0.1765\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2335 - val_loss: 0.1702\n",
      "Epoch 37/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2334Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2331 - val_loss: 0.1719\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 0.4282 - val_loss: 0.2250\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2875 - val_loss: 0.1963\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2697 - val_loss: 0.2068\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2631 - val_loss: 0.2090\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2595 - val_loss: 0.1900\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2550 - val_loss: 0.1777\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2491 - val_loss: 0.1736\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2476 - val_loss: 0.1772\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2500 - val_loss: 0.2006\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2525 - val_loss: 0.1886\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2455 - val_loss: 0.1691\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2504 - val_loss: 0.1819\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2433 - val_loss: 0.1684\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2485 - val_loss: 0.1966\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2457 - val_loss: 0.2146\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2482 - val_loss: 0.1738\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2409 - val_loss: 0.1882\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2447 - val_loss: 0.1691\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2441 - val_loss: 0.1770\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2409 - val_loss: 0.1708\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2428 - val_loss: 0.1766\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2381 - val_loss: 0.1795\n",
      "Epoch 23/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2396Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2398 - val_loss: 0.1752\n",
      "Epoch 23: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 0.4497 - val_loss: 0.2746\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2850 - val_loss: 0.2097\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2748 - val_loss: 0.1962\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2598 - val_loss: 0.1755\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2598 - val_loss: 0.1833\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2559 - val_loss: 0.1818\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2545 - val_loss: 0.1905\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2559 - val_loss: 0.1810\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2478 - val_loss: 0.2027\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2514 - val_loss: 0.1711\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2534 - val_loss: 0.1737\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2462 - val_loss: 0.1740\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2447 - val_loss: 0.1922\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2471 - val_loss: 0.1693\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2427 - val_loss: 0.1702\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2392 - val_loss: 0.1771\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2414 - val_loss: 0.1759\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2420 - val_loss: 0.1721\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2475 - val_loss: 0.1730\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2377 - val_loss: 0.1929\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2403 - val_loss: 0.1756\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2502 - val_loss: 0.1727\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2416 - val_loss: 0.1960\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2474 - val_loss: 0.1665\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2357 - val_loss: 0.1838\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2382 - val_loss: 0.1735\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2418 - val_loss: 0.1701\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2369 - val_loss: 0.1684\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2326 - val_loss: 0.1689\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2376 - val_loss: 0.1642\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2365 - val_loss: 0.1679\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2354 - val_loss: 0.1702\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2337 - val_loss: 0.1684\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2367 - val_loss: 0.1684\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2329 - val_loss: 0.1645\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2379 - val_loss: 0.1878\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2357 - val_loss: 0.1665\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2372 - val_loss: 0.2199\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2331 - val_loss: 0.1705\n",
      "Epoch 40/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2335Restoring model weights from the end of the best epoch: 30.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2334 - val_loss: 0.1720\n",
      "Epoch 40: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 0.4656 - val_loss: 0.2375\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2801 - val_loss: 0.1943\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2684 - val_loss: 0.1891\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2708 - val_loss: 0.2372\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2652 - val_loss: 0.1940\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2593 - val_loss: 0.1841\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2542 - val_loss: 0.1735\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2494 - val_loss: 0.1723\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2558 - val_loss: 0.2115\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2571 - val_loss: 0.1811\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2476 - val_loss: 0.1712\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2473 - val_loss: 0.1710\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2477 - val_loss: 0.1710\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2418 - val_loss: 0.1728\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2410 - val_loss: 0.1711\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2412 - val_loss: 0.1709\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2457 - val_loss: 0.1739\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2407 - val_loss: 0.1933\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2455 - val_loss: 0.1986\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2472 - val_loss: 0.1779\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2400 - val_loss: 0.1768\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2420 - val_loss: 0.1732\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2381 - val_loss: 0.1813\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2419 - val_loss: 0.1756\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2433 - val_loss: 0.1746\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2371 - val_loss: 0.1701\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2375 - val_loss: 0.1691\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2366 - val_loss: 0.1655\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2343 - val_loss: 0.1693\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2364 - val_loss: 0.1718\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2391 - val_loss: 0.1904\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2444 - val_loss: 0.1715\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2336 - val_loss: 0.1673\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2323 - val_loss: 0.1728\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2368 - val_loss: 0.1728\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2311 - val_loss: 0.1649\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2328 - val_loss: 0.1646\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2306 - val_loss: 0.1653\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2319 - val_loss: 0.1850\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2328 - val_loss: 0.1673\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2353 - val_loss: 0.1647\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2311 - val_loss: 0.1635\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2307 - val_loss: 0.1656\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2265 - val_loss: 0.1654\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2317 - val_loss: 0.1773\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2369 - val_loss: 0.1759\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2358 - val_loss: 0.1652\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2317 - val_loss: 0.1636\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2278 - val_loss: 0.1616\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2288 - val_loss: 0.1859\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2280 - val_loss: 0.1683\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2255 - val_loss: 0.1760\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2309 - val_loss: 0.1740\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2254 - val_loss: 0.1667\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2272 - val_loss: 0.1677\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2247 - val_loss: 0.1623\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2240 - val_loss: 0.1688\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2295 - val_loss: 0.1718\n",
      "Epoch 59/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2351Restoring model weights from the end of the best epoch: 49.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2355 - val_loss: 0.1642\n",
      "Epoch 59: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 0.4751 - val_loss: 0.2595\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2770 - val_loss: 0.2360\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2706 - val_loss: 0.1848\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2706 - val_loss: 0.2148\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2551 - val_loss: 0.1770\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2546 - val_loss: 0.1806\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2524 - val_loss: 0.1875\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2548 - val_loss: 0.1753\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2555 - val_loss: 0.2025\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2501 - val_loss: 0.1974\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2475 - val_loss: 0.1958\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2542 - val_loss: 0.1757\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2447 - val_loss: 0.1831\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2455 - val_loss: 0.1891\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2448 - val_loss: 0.1733\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2480 - val_loss: 0.1736\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2403 - val_loss: 0.1932\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2428 - val_loss: 0.1775\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2472 - val_loss: 0.1798\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2459 - val_loss: 0.1877\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2467 - val_loss: 0.1749\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2401 - val_loss: 0.1764\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2396 - val_loss: 0.1758\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2366 - val_loss: 0.1706\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2375 - val_loss: 0.1762\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2351 - val_loss: 0.1759\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2387 - val_loss: 0.1713\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2415 - val_loss: 0.1923\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2396 - val_loss: 0.1857\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2377 - val_loss: 0.1690\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2344 - val_loss: 0.1728\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2346 - val_loss: 0.1692\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2367 - val_loss: 0.1685\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2333 - val_loss: 0.1720\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2341 - val_loss: 0.1707\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2327 - val_loss: 0.1651\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2358 - val_loss: 0.1697\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2319 - val_loss: 0.1670\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2294 - val_loss: 0.1625\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2352 - val_loss: 0.1650\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2338 - val_loss: 0.1626\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2320 - val_loss: 0.1744\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2288 - val_loss: 0.1752\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2349 - val_loss: 0.1644\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2294 - val_loss: 0.1631\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2309 - val_loss: 0.1652\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2285 - val_loss: 0.1723\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2253 - val_loss: 0.1619\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2291 - val_loss: 0.1842\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2291 - val_loss: 0.1645\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2295 - val_loss: 0.1656\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2290 - val_loss: 0.1747\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2297 - val_loss: 0.1634\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2228 - val_loss: 0.1715\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2248 - val_loss: 0.1740\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2235 - val_loss: 0.1923\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2273 - val_loss: 0.1608\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2236 - val_loss: 0.1700\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2254 - val_loss: 0.1701\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2284 - val_loss: 0.1644\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2258 - val_loss: 0.1637\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2236 - val_loss: 0.1732\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2277 - val_loss: 0.1620\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2186 - val_loss: 0.1636\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2237 - val_loss: 0.1677\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2323 - val_loss: 0.1682\n",
      "Epoch 67/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2185Restoring model weights from the end of the best epoch: 57.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2187 - val_loss: 0.1682\n",
      "Epoch 67: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 95ms/step - loss: 0.4970 - val_loss: 0.2341\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2840 - val_loss: 0.1890\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2604 - val_loss: 0.2534\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2727 - val_loss: 0.2076\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2628 - val_loss: 0.2146\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2530 - val_loss: 0.1783\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2519 - val_loss: 0.1968\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2612 - val_loss: 0.1799\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2493 - val_loss: 0.1944\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2441 - val_loss: 0.1999\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2465 - val_loss: 0.1734\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2408 - val_loss: 0.1707\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2406 - val_loss: 0.1694\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2390 - val_loss: 0.1701\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2387 - val_loss: 0.1859\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2408 - val_loss: 0.1739\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2434 - val_loss: 0.1793\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2379 - val_loss: 0.1701\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2411 - val_loss: 0.1752\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2380 - val_loss: 0.1691\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2388 - val_loss: 0.1730\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2392 - val_loss: 0.1689\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2423 - val_loss: 0.1714\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2365 - val_loss: 0.1837\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2372 - val_loss: 0.1722\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2407 - val_loss: 0.1715\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2349 - val_loss: 0.1898\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2364 - val_loss: 0.1678\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2352 - val_loss: 0.1719\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2353 - val_loss: 0.1690\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2397 - val_loss: 0.1703\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2329 - val_loss: 0.1689\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2343 - val_loss: 0.1707\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2365 - val_loss: 0.1668\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2323 - val_loss: 0.1620\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2439 - val_loss: 0.1635\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2360 - val_loss: 0.1632\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2329 - val_loss: 0.1642\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2308 - val_loss: 0.1622\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2291 - val_loss: 0.1631\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2303 - val_loss: 0.1801\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2298 - val_loss: 0.1638\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2289 - val_loss: 0.1730\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2291 - val_loss: 0.2279\n",
      "Epoch 45/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2402Restoring model weights from the end of the best epoch: 35.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2403 - val_loss: 0.1646\n",
      "Epoch 45: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 0.4750 - val_loss: 0.2163\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2809 - val_loss: 0.2010\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2639 - val_loss: 0.2201\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2697 - val_loss: 0.1866\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2624 - val_loss: 0.1907\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2610 - val_loss: 0.1844\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2514 - val_loss: 0.1774\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2498 - val_loss: 0.1785\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2498 - val_loss: 0.2013\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2522 - val_loss: 0.1830\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2442 - val_loss: 0.2287\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2521 - val_loss: 0.2045\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2500 - val_loss: 0.1780\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2470 - val_loss: 0.1807\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2498 - val_loss: 0.1696\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2494 - val_loss: 0.1708\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2404 - val_loss: 0.1712\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2514 - val_loss: 0.1712\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2380 - val_loss: 0.1773\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2405 - val_loss: 0.1657\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2419 - val_loss: 0.1749\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2420 - val_loss: 0.1779\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2381 - val_loss: 0.1706\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2374 - val_loss: 0.1759\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2379 - val_loss: 0.1756\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2360 - val_loss: 0.1671\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2382 - val_loss: 0.1990\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2388 - val_loss: 0.1690\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2351 - val_loss: 0.1693\n",
      "Epoch 30/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2355Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2353 - val_loss: 0.1677\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 0.4108 - val_loss: 0.2350\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2752 - val_loss: 0.2408\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2746 - val_loss: 0.2005\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2587 - val_loss: 0.1815\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2626 - val_loss: 0.1809\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2531 - val_loss: 0.1903\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2512 - val_loss: 0.1804\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2549 - val_loss: 0.1814\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2543 - val_loss: 0.1774\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2579 - val_loss: 0.2035\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2553 - val_loss: 0.1769\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2472 - val_loss: 0.1775\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2518 - val_loss: 0.1866\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2445 - val_loss: 0.1846\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2417 - val_loss: 0.2063\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2471 - val_loss: 0.1706\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2390 - val_loss: 0.1754\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2416 - val_loss: 0.1747\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2425 - val_loss: 0.1684\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2398 - val_loss: 0.1696\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2423 - val_loss: 0.1730\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2433 - val_loss: 0.1756\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2345 - val_loss: 0.1680\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2417 - val_loss: 0.1852\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2371 - val_loss: 0.1758\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2376 - val_loss: 0.1797\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2379 - val_loss: 0.1802\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2433 - val_loss: 0.1745\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2367 - val_loss: 0.1671\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2369 - val_loss: 0.1680\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2366 - val_loss: 0.1806\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2397 - val_loss: 0.1750\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2304 - val_loss: 0.1774\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2332 - val_loss: 0.1684\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2367 - val_loss: 0.1713\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2348 - val_loss: 0.1795\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2327 - val_loss: 0.1698\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2365 - val_loss: 0.1753\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2307 - val_loss: 0.1663\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2298 - val_loss: 0.1928\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2350 - val_loss: 0.1671\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2321 - val_loss: 0.2111\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2309 - val_loss: 0.1647\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2303 - val_loss: 0.1696\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2295 - val_loss: 0.1699\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2341 - val_loss: 0.1693\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2328 - val_loss: 0.1797\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2242 - val_loss: 0.1800\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2290 - val_loss: 0.1662\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2330 - val_loss: 0.1677\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2286 - val_loss: 0.1658\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2299 - val_loss: 0.1666\n",
      "Epoch 53/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2296Restoring model weights from the end of the best epoch: 43.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2296 - val_loss: 0.1679\n",
      "Epoch 53: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 0.4509 - val_loss: 0.2565\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2845 - val_loss: 0.2484\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2815 - val_loss: 0.1959\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2663 - val_loss: 0.1832\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2542 - val_loss: 0.2064\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2617 - val_loss: 0.1762\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2549 - val_loss: 0.1808\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2518 - val_loss: 0.1833\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2479 - val_loss: 0.1771\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2426 - val_loss: 0.1711\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2452 - val_loss: 0.1698\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2496 - val_loss: 0.2136\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2475 - val_loss: 0.1889\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2453 - val_loss: 0.1759\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2495 - val_loss: 0.1708\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2452 - val_loss: 0.1699\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2426 - val_loss: 0.1758\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2430 - val_loss: 0.2022\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2498 - val_loss: 0.1716\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2388 - val_loss: 0.1766\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2374 - val_loss: 0.1682\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2386 - val_loss: 0.1793\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2344 - val_loss: 0.1651\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2346 - val_loss: 0.1698\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2355 - val_loss: 0.1677\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2391 - val_loss: 0.1662\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2355 - val_loss: 0.1902\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2377 - val_loss: 0.1679\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2356 - val_loss: 0.1662\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2350 - val_loss: 0.1672\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2409 - val_loss: 0.1829\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2367 - val_loss: 0.1713\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2349 - val_loss: 0.1642\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2380 - val_loss: 0.1655\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2328 - val_loss: 0.1757\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2296 - val_loss: 0.1637\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2304 - val_loss: 0.1902\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2307 - val_loss: 0.1736\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2348 - val_loss: 0.1736\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2384 - val_loss: 0.1608\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2309 - val_loss: 0.1638\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2411 - val_loss: 0.1710\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2296 - val_loss: 0.1741\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2288 - val_loss: 0.1653\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2368 - val_loss: 0.1610\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2332 - val_loss: 0.1625\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2291 - val_loss: 0.1637\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2278 - val_loss: 0.1703\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2298 - val_loss: 0.1689\n",
      "Epoch 50/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2255Restoring model weights from the end of the best epoch: 40.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2265 - val_loss: 0.1645\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 0.4429 - val_loss: 0.2098\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2801 - val_loss: 0.1857\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2747 - val_loss: 0.2183\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2561 - val_loss: 0.2326\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2582 - val_loss: 0.1985\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2531 - val_loss: 0.1746\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2494 - val_loss: 0.1987\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2514 - val_loss: 0.1815\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2487 - val_loss: 0.1845\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2534 - val_loss: 0.1808\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2525 - val_loss: 0.1940\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2465 - val_loss: 0.1905\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2439 - val_loss: 0.1741\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2448 - val_loss: 0.1838\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2430 - val_loss: 0.1735\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2482 - val_loss: 0.1718\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2455 - val_loss: 0.1772\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2409 - val_loss: 0.1929\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2426 - val_loss: 0.1784\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2440 - val_loss: 0.1763\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2369 - val_loss: 0.1717\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2426 - val_loss: 0.1676\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2410 - val_loss: 0.1701\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2400 - val_loss: 0.1802\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2373 - val_loss: 0.1679\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2368 - val_loss: 0.1666\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2403 - val_loss: 0.1862\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2403 - val_loss: 0.1669\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2443 - val_loss: 0.1830\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2389 - val_loss: 0.1761\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2386 - val_loss: 0.1693\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2374 - val_loss: 0.1639\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2335 - val_loss: 0.1706\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2392 - val_loss: 0.1773\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2407 - val_loss: 0.1698\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2354 - val_loss: 0.1666\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2347 - val_loss: 0.1781\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2414 - val_loss: 0.1725\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2328 - val_loss: 0.1723\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2388 - val_loss: 0.1709\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2345 - val_loss: 0.1670\n",
      "Epoch 42/2000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.2310Restoring model weights from the end of the best epoch: 32.\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2307 - val_loss: 0.1704\n",
      "Epoch 42: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 33ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 27ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 27ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 2s 29ms/step\n",
      "5/5 [==============================] - 0s 28ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 32ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n",
      "5/5 [==============================] - 0s 31ms/step\n",
      "5/5 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.40127, 0.4218, 0.42332, 0.96668, 0.42786)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n",
    "mae_predictions = bagging_predict2(pred4, X_train_val)\n",
    "mse_predictions =  bagging_predict2(pred5, X_train_val)\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "sMAPE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAPE= np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MASE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MSE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "\n",
    "MSE, MASE, MAE, MAPE, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed0c7ce-ad6f-47e0-b18a-7e5a1e0a7f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "exp 0.47407\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions2 = bagging_predict2(pred1, test_X)\n",
    "smape_predictions2 = np.median(np.concatenate([smape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mase_predictions2 =bagging_predict2(pred2, test_X)\n",
    "mase_predictions2 = np.median(np.concatenate([mase_predictions2],axis=0),axis=0)\n",
    "\n",
    "mape_predictions2 =bagging_predict2(pred3, test_X)\n",
    "mape_predictions2 = np.median(np.concatenate([mape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mae_predictions2 = bagging_predict2(pred4,test_X)\n",
    "mae_predictions2 = np.median(np.concatenate([mae_predictions2],axis=0),axis=0)\n",
    "\n",
    "mse_predictions2 =bagging_predict2(pred5,test_X)\n",
    "mse_predictions2 = np.median(np.concatenate([mse_predictions2],axis=0),axis=0)\n",
    "\n",
    "\n",
    "#concat_mase = np.concatenate([np.nan_to_num(np.array(mase_predictions2), nan=0)])\n",
    "#fin_pred_mase = np.median(concat_mase,axis=1)\n",
    "\n",
    "#concat_mape = np.concatenate([np.nan_to_num(np.array(mape_predictions2), nan=0)])\n",
    "#fin_pred_mape = np.median(concat_mape,axis=1)\n",
    "\n",
    "#concat_smape = np.concatenate([np.nan_to_num(np.array(smape_predictions2), nan=0)])\n",
    "#fin_pred_smape = np.median(concat_smape,axis=1)\n",
    "\n",
    "#concat_mae = np.concatenate([np.nan_to_num(np.array(mae_predictions2), nan=0)])\n",
    "#fin_pred_mae = np.median(concat_mae,axis=1)\n",
    "\n",
    "#concat_mse = np.concatenate([np.nan_to_num(np.array(mse_predictions2), nan=0)])\n",
    "#fin_pred_mse = np.median(concat_mse,axis=1)\n",
    "\n",
    "performance = np.array([MAE, MAPE,sMAPE,MSE,MASE])\n",
    "beta = 3 # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([mae_predictions2,\n",
    "                    mape_predictions2,\n",
    "                   smape_predictions2,\n",
    "                   mse_predictions2,\n",
    "                   mase_predictions2],axis=0)\n",
    "#gd=np.median(gd,axis=2)\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "pd.DataFrame(ensemble_prediction.flatten()).to_csv('exp7/LSTM.csv')\n",
    "\n",
    "\n",
    "print('exp',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93fd3e88-6523-4042-a996-ad1f65d9fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 30ms/step\n",
      "12/12 [==============================] - 0s 29ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "all 0.47959\n",
      "original 0.48744\n",
      "best 0.47817\n",
      "mse 0.46746\n",
      "mase 0.48523\n",
      "mae 0.48684\n",
      "mape 1.08731\n",
      "smape 0.49103\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "mae_predictions_G = bagging_predict2(pred4, test_X)\n",
    "mse_predictions_G = bagging_predict2(pred5,test_X)\n",
    "\n",
    "\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G,mae_predictions_G,mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('all',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5)) \n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G, mase_predictions_G,mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('best',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mse',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mae',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([smape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce6054d-bd3d-4981-90dc-91c0ab363196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.4793389),\n",
       " (2, 0.47443143),\n",
       " (3, 0.47406745),\n",
       " (4, 0.47484627),\n",
       " (5, 0.47562924),\n",
       " (6, 0.47616154),\n",
       " (7, 0.4764584),\n",
       " (8, 0.4765883),\n",
       " (9, 0.47661152),\n",
       " (10, 0.47656977),\n",
       " (11, 0.4764896),\n",
       " (12, 0.476387),\n",
       " (13, 0.47627136),\n",
       " (14, 0.47614828),\n",
       " (15, 0.47602093),\n",
       " (16, 0.47589105),\n",
       " (17, 0.47575995),\n",
       " (18, 0.47562814),\n",
       " (19, 0.47549605)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eee = []\n",
    "for i in range(1,20):\n",
    "    weights = np.exp(-i* performance)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "    eee.append((i,np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten().round(5)))))\n",
    "    #print(f'exp_beta{i}',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))\n",
    "\n",
    "eee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
