{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1edfab-1b16-4c7c-b29b-33e21e01ac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:26:27.883794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 22:26:27.963021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-17 22:26:27.963039: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-17 22:26:28.338235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-17 22:26:28.338285: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-17 22:26:28.338291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "\n",
    "\n",
    "data = 'solor'\n",
    "target_X= pd.read_csv(f\"../data/{data}_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(f\"../data/{data}_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train = target_X[:-round(target_X.shape[0]*0.2),:].astype(np.float32)\n",
    "y_train = target_y[:-round(target_y.shape[0]*0.2)].astype(np.float32)\n",
    "\n",
    "X_train_val= target_X[-round(target_X.shape[0]*0.2):,:].astype(np.float32)\n",
    "y_train_val =target_y[-round(target_y.shape[0]*0.2):].astype(np.float32)\n",
    "\n",
    "\n",
    "test_X= pd.read_csv(f\"../data/{data}_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(f\"../data/{data}_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f6adfa-e2fe-4577-957c-5c76c5ee9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 생성 함수\n",
    "def build_FC_model(input_length, output_length,dropout_rate=0.2,lossfn='mse' ):\n",
    "    # 입력 레이어 정의\n",
    "    input_seq = layers.Input(shape=(input_length, 1))\n",
    "\n",
    "    # LSTM 레이어 추가\n",
    "    lstm_output = layers.Dense(128,activation='linear')(input_seq)\n",
    "    lstm_output = layers.Dropout(dropout_rate)(lstm_output)\n",
    "\n",
    "    # 밀집층 추가\n",
    "    dense_output = layers.Dense(64, activation='linear')(lstm_output)\n",
    "    dense_output = layers.Dropout(dropout_rate)(dense_output)\n",
    "    \n",
    "    # 출력 크기에 맞춰서 리쉐이프\n",
    "    dense_output = layers.Reshape((output_length, -1))(dense_output)\n",
    "\n",
    "    # 출력 레이어\n",
    "    output = layers.Dense(1, activation='linear')(dense_output)\n",
    "\n",
    "    # 모델 정의\n",
    "    model = models.Model(inputs=input_seq, outputs=output)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer='adam', loss=lossfn)\n",
    "\n",
    "    \n",
    "    return model\n",
    "###############################################################################\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_FC_model(backcast,forecast )\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_data = [X_train_val,y_train_val])\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e469d41a-1ab3-4fcc-8fd1-4aacf5b2a3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 22:26:29.230543: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-17 22:26:29.230577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-17 22:26:29.231103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num, 'mape',2000,10,8,0.001)\n",
    "mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f5fba-4d7e-4bd7-bcff-9ca82d43e580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.81399, 0.94858, 0.9483, 0.96659, 0.96659)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, X_train_val)\n",
    "mase_predictions =  bagging_predict2(pred2, X_train_val)\n",
    "mape_predictions =  bagging_predict2(pred3, X_train_val)\n",
    "mae_predictions = bagging_predict2(pred4, X_train_val)\n",
    "mse_predictions =  bagging_predict2(pred5, X_train_val)\n",
    "\n",
    "\n",
    "concat_G = np.concatenate(np.nan_to_num([smape_predictions]),axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "sMAPE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAPE= np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MASE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MAE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "MSE = np.sqrt(mean_squared_error(y_train_val.flatten(),fin_pred_G.flatten())).round(5)\n",
    "\n",
    "\n",
    "MSE, MASE, MAE, MAPE, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ea409a-8b0e-4722-a8be-4e5a79226fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m############################################################################################\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     54\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(ensemble_prediction\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp7/vanila_FC.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mensemble_prediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions2 = bagging_predict2(pred1, test_X)\n",
    "smape_predictions2 = np.median(np.concatenate([smape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mase_predictions2 =bagging_predict2(pred2, test_X)\n",
    "mase_predictions2 = np.median(np.concatenate([mase_predictions2],axis=0),axis=0)\n",
    "\n",
    "mape_predictions2 =bagging_predict2(pred3, test_X)\n",
    "mape_predictions2 = np.median(np.concatenate([mape_predictions2],axis=0),axis=0)\n",
    "\n",
    "mae_predictions2 = bagging_predict2(pred4,test_X)\n",
    "mae_predictions2 = np.median(np.concatenate([mae_predictions2],axis=0),axis=0)\n",
    "\n",
    "mse_predictions2 =bagging_predict2(pred5,test_X)\n",
    "mse_predictions2 = np.median(np.concatenate([mse_predictions2],axis=0),axis=0)\n",
    "\n",
    "\n",
    "#concat_mase = np.concatenate([np.nan_to_num(np.array(mase_predictions2), nan=0)])\n",
    "#fin_pred_mase = np.median(concat_mase,axis=1)\n",
    "\n",
    "#concat_mape = np.concatenate([np.nan_to_num(np.array(mape_predictions2), nan=0)])\n",
    "#fin_pred_mape = np.median(concat_mape,axis=1)\n",
    "\n",
    "#concat_smape = np.concatenate([np.nan_to_num(np.array(smape_predictions2), nan=0)])\n",
    "#fin_pred_smape = np.median(concat_smape,axis=1)\n",
    "\n",
    "#concat_mae = np.concatenate([np.nan_to_num(np.array(mae_predictions2), nan=0)])\n",
    "#fin_pred_mae = np.median(concat_mae,axis=1)\n",
    "\n",
    "#concat_mse = np.concatenate([np.nan_to_num(np.array(mse_predictions2), nan=0)])\n",
    "#fin_pred_mse = np.median(concat_mse,axis=1)\n",
    "\n",
    "performance = np.array([MAE, MAPE,sMAPE,MSE,MASE])\n",
    "beta = 3 # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([mae_predictions2,\n",
    "                    mape_predictions2,\n",
    "                   smape_predictions2,\n",
    "                   mse_predictions2,\n",
    "                   mase_predictions2],axis=0)\n",
    "#gd=np.median(gd,axis=2)\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "pd.DataFrame(ensemble_prediction.flatten()).to_csv('exp7/vanila_FC.csv')\n",
    "\n",
    "\n",
    "print('exp',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c62b1b8-9e4b-496e-81ca-d7c8c9d0973d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "all 1.06891\n",
      "original 1.08722\n",
      "best 1.0637\n",
      "mse 0.90987\n",
      "mase 1.0663\n",
      "mae 1.06581\n",
      "mape 1.08722\n",
      "smape 1.08722\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2,test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3,test_X)\n",
    "mae_predictions_G = bagging_predict2(pred4, test_X)\n",
    "mse_predictions_G = bagging_predict2(pred5,test_X)\n",
    "\n",
    "\n",
    "print('############################################################################################') \n",
    "print('############################################################################################') \n",
    "\n",
    "concat_G = np.concatenate([np.nan_to_num(smape_predictions_G), mase_predictions_G,mape_predictions_G,mae_predictions_G,mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('all',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5)) \n",
    "\n",
    "concat_G = np.concatenate([np.nan_to_num(smape_predictions_G), mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('original',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G, mase_predictions_G,mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('best',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mse_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mse',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "\n",
    "concat_G = np.concatenate([mase_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mase',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mae_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mae',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('mape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))\n",
    "\n",
    "concat_G = np.concatenate([np.nan_to_num(smape_predictions_G)],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "print('smape',np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578132bc-ba3b-48ae-94ca-2f30d48ede43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0872184),\n",
       " (2, 1.0872184),\n",
       " (3, 1.0872184),\n",
       " (4, 1.0872184),\n",
       " (5, 1.0872184),\n",
       " (6, 1.0872184),\n",
       " (7, 1.0872184),\n",
       " (8, 1.0872184),\n",
       " (9, 1.0872184),\n",
       " (10, 1.0872184),\n",
       " (11, 1.0872184),\n",
       " (12, 1.0872184),\n",
       " (13, 1.0872184),\n",
       " (14, 1.0872184),\n",
       " (15, 1.0872184),\n",
       " (16, 1.0872184),\n",
       " (17, 1.0872184),\n",
       " (18, 1.0872184),\n",
       " (19, 1.0872184)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eee = []\n",
    "for i in range(1,20):\n",
    "    weights = np.exp(-i* performance)\n",
    "    normalized_weights = np.nan_to_num(weights,nan=0) / np.sum(weights)\n",
    "    ensemble_prediction = np.dot(normalized_weights, gd.reshape(5,-1))\n",
    "    eee.append((i,np.sqrt(mean_squared_error(test_y.flatten(),np.nan_to_num(ensemble_prediction,nan=0).flatten().round(5)))))\n",
    "    #print(f'exp_beta{i}',np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction.flatten())).round(5))\n",
    "\n",
    "eee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
