{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0036d6-fe11-4032-9ebd-2bf63597776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 10:40:06.869801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-22 10:40:06.969569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-22 10:40:06.969594: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-22 10:40:07.423432: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-22 10:40:07.423505: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-22 10:40:07.423513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c11e4f2-f795-4d8d-8c76-7cd76c0187d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((721, 168), (356, 168))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_X= pd.read_csv(\"../data/solor_train_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"../data/solor_train_output_7.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"../data/solor_val_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"../data/solor_val_output_7.csv\").iloc[:,1:].values\n",
    "#backcast_length = X_train.shape[1]\n",
    "#forecast_length = y_train.shape[1]\n",
    "X_train= target_X\n",
    "y_train=target_y\n",
    "target_X.shape,test_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff7750c-7635-4132-984a-0f92f50db72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 unit\n",
    "    # 4 nhead\n",
    "    # 5 nlayers\n",
    "    # dropout\n",
    "    return X_train.shape[1],y_train.shape[1],64,2,2,0.1\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train,y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models_G(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model_G(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "# SMAPE 용\n",
    "def train_bagging_models_smape(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[np.newaxis, ...]\n",
    "\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "def create_model(fn,d_model, nlayers, nhead, dropout, iw, ow,lr):\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(pretrained_output_reshaped)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "    \n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    \n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    target_model = Model(inputs=inputs, outputs=outputs)\n",
    "    target_model.compile(optimizer=optimizer, loss=fn)\n",
    "    \n",
    "    return target_model\n",
    "\n",
    "#################################################################################\n",
    "# 트랜스포머 모델 생성 함수\n",
    "def bulid_model(iw, ow, d_model, nhead, nlayers, dropout=0.5):\n",
    "    inputs = tf.keras.Input(shape=(iw, 1))\n",
    "    x = layers.Dense(d_model // 2, activation='relu')(inputs)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "\n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "\n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "\n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr=0.001):\n",
    "    models = {}\n",
    "    iw, ow, d_model, nhead, nlayers, dropout = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(iw, ow, d_model, nhead, nlayers, dropout=0.5)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# 예측\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93870f9b-5e3c-42bf-ae32-acfc580104ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 24ms/step - loss: 1.2831 - val_loss: 0.7126\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9449 - val_loss: 0.7648\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.9260 - val_loss: 0.7663\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9281 - val_loss: 0.7981\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9288 - val_loss: 0.6930\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9153 - val_loss: 0.8920\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8990 - val_loss: 0.6919\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8968 - val_loss: 0.6991\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8359 - val_loss: 0.6463\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.8549 - val_loss: 0.6790\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8267 - val_loss: 0.6444\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.8355 - val_loss: 0.6494\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8211 - val_loss: 0.6349\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8348 - val_loss: 0.7073\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7967 - val_loss: 0.6696\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.8120 - val_loss: 0.6435\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8012 - val_loss: 0.6659\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8238 - val_loss: 0.6362\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8015 - val_loss: 0.6200\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.7871 - val_loss: 0.6286\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8091 - val_loss: 0.6630\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7935 - val_loss: 0.6486\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8132 - val_loss: 0.6181\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8117 - val_loss: 0.6246\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7826 - val_loss: 0.6815\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8280 - val_loss: 0.7649\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8071 - val_loss: 0.6015\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7812 - val_loss: 0.5885\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7866 - val_loss: 0.6274\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7829 - val_loss: 0.6128\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8185 - val_loss: 0.7792\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.7852 - val_loss: 0.6173\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.7649 - val_loss: 0.7078\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7716 - val_loss: 0.6568\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7806 - val_loss: 0.6249\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7783 - val_loss: 0.6324\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7737 - val_loss: 0.6661\n",
      "Epoch 38/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7717Restoring model weights from the end of the best epoch: 28.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7732 - val_loss: 0.6865\n",
      "Epoch 38: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 24ms/step - loss: 1.2739 - val_loss: 0.7670\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9423 - val_loss: 0.6903\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9250 - val_loss: 0.8565\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9190 - val_loss: 0.6764\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9273 - val_loss: 0.6891\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8473 - val_loss: 0.7190\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.8603 - val_loss: 0.6574\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8415 - val_loss: 0.6364\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.8363 - val_loss: 0.7029\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8435 - val_loss: 0.6488\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8296 - val_loss: 0.8061\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8258 - val_loss: 0.6633\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8105 - val_loss: 0.6497\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8076 - val_loss: 0.6343\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7943 - val_loss: 0.7020\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7865 - val_loss: 0.8289\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7859 - val_loss: 0.6390\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7890 - val_loss: 0.6747\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.8358 - val_loss: 0.6402\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.7958 - val_loss: 0.6455\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.7842 - val_loss: 0.6128\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7818 - val_loss: 0.6675\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7993 - val_loss: 0.6726\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7915 - val_loss: 0.7888\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7988 - val_loss: 0.6184\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7667 - val_loss: 0.6247\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7750 - val_loss: 0.6288\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7592 - val_loss: 0.6875\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7716 - val_loss: 0.6141\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7829 - val_loss: 0.6633\n",
      "Epoch 31/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7927Restoring model weights from the end of the best epoch: 21.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7932 - val_loss: 0.6209\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 1.4181 - val_loss: 0.7498\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9374 - val_loss: 0.7852\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9301 - val_loss: 0.7796\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9358 - val_loss: 0.7046\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9340 - val_loss: 0.7032\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9017 - val_loss: 0.7867\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9004 - val_loss: 0.6716\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8456 - val_loss: 0.7288\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8674 - val_loss: 0.6483\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8281 - val_loss: 0.7181\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8268 - val_loss: 0.6401\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8051 - val_loss: 0.7100\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8500 - val_loss: 0.7451\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8090 - val_loss: 0.6714\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7916 - val_loss: 0.6715\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8045 - val_loss: 0.6332\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8020 - val_loss: 0.6514\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7806 - val_loss: 0.6524\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7896 - val_loss: 0.6023\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7843 - val_loss: 0.6107\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7862 - val_loss: 0.5987\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7841 - val_loss: 0.6426\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7749 - val_loss: 0.7377\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7931 - val_loss: 0.6497\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7691 - val_loss: 0.7351\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8070 - val_loss: 0.6412\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7746 - val_loss: 0.6316\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7567 - val_loss: 0.6298\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7643 - val_loss: 0.6099\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7839 - val_loss: 0.6098\n",
      "Epoch 31/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7834Restoring model weights from the end of the best epoch: 21.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7826 - val_loss: 0.6972\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.3601 - val_loss: 1.1058\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9896 - val_loss: 0.8682\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9393 - val_loss: 0.6951\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9218 - val_loss: 0.7012\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9197 - val_loss: 0.7933\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8910 - val_loss: 0.7044\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8519 - val_loss: 0.7945\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8868 - val_loss: 0.8111\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8509 - val_loss: 0.6537\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.8526 - val_loss: 0.6730\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8247 - val_loss: 0.8293\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8376 - val_loss: 0.6619\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8020 - val_loss: 0.6391\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8250 - val_loss: 0.6420\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.7997 - val_loss: 0.6576\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8186 - val_loss: 0.6490\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8192 - val_loss: 0.6982\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.8067 - val_loss: 0.6509\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.8040 - val_loss: 0.6257\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8181 - val_loss: 0.6653\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7898 - val_loss: 0.6094\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7838 - val_loss: 0.7206\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7893 - val_loss: 0.6120\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7780 - val_loss: 0.6418\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7946 - val_loss: 0.6850\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7914 - val_loss: 0.6161\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7747 - val_loss: 0.6150\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7829 - val_loss: 0.6302\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7939 - val_loss: 0.6115\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7730 - val_loss: 0.6175\n",
      "Epoch 31/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7699Restoring model weights from the end of the best epoch: 21.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7737 - val_loss: 0.6484\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.1885 - val_loss: 0.7223\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9566 - val_loss: 0.7401\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9325 - val_loss: 0.7434\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9244 - val_loss: 0.9003\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9110 - val_loss: 0.8167\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9047 - val_loss: 0.7476\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.8874 - val_loss: 0.6994\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8542 - val_loss: 0.6781\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8165 - val_loss: 0.6910\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8325 - val_loss: 0.6624\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7990 - val_loss: 0.6276\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8084 - val_loss: 0.6831\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8127 - val_loss: 0.8430\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8156 - val_loss: 0.6643\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7907 - val_loss: 0.6505\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8214 - val_loss: 0.6738\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8484 - val_loss: 0.7669\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8240 - val_loss: 0.6778\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7851 - val_loss: 0.6438\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7904 - val_loss: 0.6884\n",
      "Epoch 21/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.8431Restoring model weights from the end of the best epoch: 11.\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.8388 - val_loss: 0.7960\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.3453 - val_loss: 0.7206\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9439 - val_loss: 0.8555\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9187 - val_loss: 0.7267\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9158 - val_loss: 0.6957\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8781 - val_loss: 0.7211\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8347 - val_loss: 0.7419\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8558 - val_loss: 0.6957\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8390 - val_loss: 0.6825\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8468 - val_loss: 0.6635\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8252 - val_loss: 0.6530\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8538 - val_loss: 0.6912\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8232 - val_loss: 0.6386\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8091 - val_loss: 0.6988\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8227 - val_loss: 0.6226\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7886 - val_loss: 0.6339\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7974 - val_loss: 0.6471\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8139 - val_loss: 0.6578\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7933 - val_loss: 0.6445\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7895 - val_loss: 0.6240\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7797 - val_loss: 0.5997\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7769 - val_loss: 0.6178\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7903 - val_loss: 0.6336\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7900 - val_loss: 0.6207\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7662 - val_loss: 0.6419\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7919 - val_loss: 0.6410\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7867 - val_loss: 0.6208\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7965 - val_loss: 0.6517\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7795 - val_loss: 0.6657\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7982 - val_loss: 0.6685\n",
      "Epoch 30/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7967Restoring model weights from the end of the best epoch: 20.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8008 - val_loss: 0.6277\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.3124 - val_loss: 0.7450\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9277 - val_loss: 0.7224\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9093 - val_loss: 0.8753\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8941 - val_loss: 0.7364\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8969 - val_loss: 0.6552\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.8607 - val_loss: 0.7746\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8749 - val_loss: 0.6696\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8297 - val_loss: 0.6637\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8426 - val_loss: 0.6565\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8282 - val_loss: 0.7755\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8391 - val_loss: 0.6337\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8199 - val_loss: 0.7241\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8059 - val_loss: 0.6827\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8200 - val_loss: 0.6634\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8071 - val_loss: 0.6671\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8000 - val_loss: 0.7357\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8012 - val_loss: 0.6556\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7909 - val_loss: 0.6421\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.7700 - val_loss: 0.6458\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7997 - val_loss: 0.7174\n",
      "Epoch 21/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7805Restoring model weights from the end of the best epoch: 11.\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.7812 - val_loss: 0.6358\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.3469 - val_loss: 0.7753\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9670 - val_loss: 0.7838\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9266 - val_loss: 0.8448\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.9000 - val_loss: 0.6740\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8653 - val_loss: 0.6814\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8522 - val_loss: 0.6663\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8151 - val_loss: 0.8089\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8239 - val_loss: 0.6824\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8039 - val_loss: 0.7145\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8490 - val_loss: 0.6674\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8177 - val_loss: 0.7017\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8057 - val_loss: 0.6103\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7989 - val_loss: 0.7458\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8169 - val_loss: 0.6955\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7898 - val_loss: 0.6300\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.7815 - val_loss: 0.6247\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7764 - val_loss: 0.6272\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7631 - val_loss: 0.7107\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7969 - val_loss: 0.6558\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7807 - val_loss: 0.6283\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7816 - val_loss: 0.6248\n",
      "Epoch 22/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7778Restoring model weights from the end of the best epoch: 12.\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.7822 - val_loss: 0.6391\n",
      "Epoch 22: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 1.2959 - val_loss: 0.7353\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9567 - val_loss: 0.7031\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9204 - val_loss: 0.7118\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.9104 - val_loss: 0.7611\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8757 - val_loss: 0.6798\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8469 - val_loss: 0.6676\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8483 - val_loss: 0.6402\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8033 - val_loss: 0.7235\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8244 - val_loss: 0.6447\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8196 - val_loss: 0.6260\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8035 - val_loss: 0.6220\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.8005 - val_loss: 0.6246\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8009 - val_loss: 0.6703\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 0.8021 - val_loss: 0.7005\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7919 - val_loss: 0.6317\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7991 - val_loss: 0.6386\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7973 - val_loss: 0.6507\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.7834 - val_loss: 0.6375\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.7813 - val_loss: 0.6490\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7864 - val_loss: 0.6095\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 0.7976 - val_loss: 0.6051\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7856 - val_loss: 0.6384\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7854 - val_loss: 0.6291\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7829 - val_loss: 0.6390\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7883 - val_loss: 0.6206\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7867 - val_loss: 0.6943\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7897 - val_loss: 0.6226\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7845 - val_loss: 0.6280\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7708 - val_loss: 0.7326\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7823 - val_loss: 0.6001\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7646 - val_loss: 0.6511\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7871 - val_loss: 0.6279\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7665 - val_loss: 0.6379\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7720 - val_loss: 0.6025\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7731 - val_loss: 0.6085\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7837 - val_loss: 0.6325\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7699 - val_loss: 0.6159\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7697 - val_loss: 0.6887\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7629 - val_loss: 0.6108\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7568 - val_loss: 0.5841\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7535 - val_loss: 0.6720\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7663 - val_loss: 0.5844\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.7626 - val_loss: 0.6001\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7652 - val_loss: 0.6069\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7731 - val_loss: 0.6091\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7637 - val_loss: 0.6045\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7697 - val_loss: 0.5980\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7640 - val_loss: 0.5929\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7558 - val_loss: 0.6131\n",
      "Epoch 50/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7536Restoring model weights from the end of the best epoch: 40.\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7512 - val_loss: 0.5855\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 1.4140 - val_loss: 1.0141\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.9251 - val_loss: 0.7042\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.9190 - val_loss: 0.7542\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8794 - val_loss: 0.6852\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8672 - val_loss: 0.7000\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8266 - val_loss: 0.6599\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8220 - val_loss: 0.8980\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8126 - val_loss: 0.6619\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.8203 - val_loss: 0.6390\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.7886 - val_loss: 0.6697\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.8322 - val_loss: 0.6165\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.8226 - val_loss: 0.6188\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7961 - val_loss: 0.6324\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.8091 - val_loss: 1.0338\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.8307 - val_loss: 0.6592\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.8106 - val_loss: 0.6183\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7867 - val_loss: 0.6699\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.8082 - val_loss: 0.6415\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7842 - val_loss: 0.6249\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7902 - val_loss: 0.6485\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7748 - val_loss: 0.6064\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7867 - val_loss: 0.6474\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7733 - val_loss: 0.6474\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7810 - val_loss: 0.6243\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 0.7991 - val_loss: 0.5996\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7770 - val_loss: 0.6185\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 0.7723 - val_loss: 0.6292\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.7776 - val_loss: 0.6259\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 0.7928 - val_loss: 0.6170\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.7740 - val_loss: 0.6285\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 0.7853 - val_loss: 0.6062\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 0.7580 - val_loss: 0.6057\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7571 - val_loss: 0.6126\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7713 - val_loss: 0.6085\n",
      "Epoch 35/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.7593Restoring model weights from the end of the best epoch: 25.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 0.7587 - val_loss: 0.7220\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 57628636.0000 - val_loss: 3529932.2500\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 2215928.5000 - val_loss: 371477.5312\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 745685.8125 - val_loss: 748869.1875\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 197791.1094 - val_loss: 205906.1719\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 71569.7656 - val_loss: 235994.0625\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 69751.7500 - val_loss: 262454.9375\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 77178.9219 - val_loss: 251515.2812\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 62836.0000 - val_loss: 279936.9062\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 67209.0625 - val_loss: 315769.5312\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 70237.3750 - val_loss: 332878.2500\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 71985.8828 - val_loss: 345765.9062\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 65771.4297 - val_loss: 350523.2500\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 61555.3320 - val_loss: 337442.3750\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 57019.9844Restoring model weights from the end of the best epoch: 4.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 57019.9844 - val_loss: 338518.4375\n",
      "Epoch 14: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 31733656.0000 - val_loss: 732627.0625\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 848549.7500 - val_loss: 175424.5000\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 94994.4062 - val_loss: 196121.5000\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 91641.5156 - val_loss: 225347.5469\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 81896.3125 - val_loss: 256914.3750\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 90347.8594 - val_loss: 304542.5625\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 87766.9297 - val_loss: 324954.2500\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 75633.2266 - val_loss: 305374.2500\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 74815.0703 - val_loss: 300146.1562\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 60870.5156 - val_loss: 314270.0625\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 62365.8203 - val_loss: 318792.1875\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 51670.4141Restoring model weights from the end of the best epoch: 2.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 51670.4141 - val_loss: 329804.4375\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 21658318.0000 - val_loss: 197264.7656\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 70687.6641 - val_loss: 162637.7344\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 68487.9453 - val_loss: 155638.6406\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 70143.7266 - val_loss: 235965.8750\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 63809.0430 - val_loss: 219400.6094\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 60765.7227 - val_loss: 252387.8906\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 64839.3750 - val_loss: 278364.3750\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 59848.9844 - val_loss: 288518.3438\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 56685.7305 - val_loss: 312446.8438\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 61506.8945 - val_loss: 308439.3438\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 58044.0039 - val_loss: 334173.1250\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 58514.5000 - val_loss: 337006.9375\n",
      "Epoch 13/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 56742.9805Restoring model weights from the end of the best epoch: 3.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 56790.2305 - val_loss: 333419.0625\n",
      "Epoch 13: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 40783488.0000 - val_loss: 758213.3750\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 465463.7188 - val_loss: 171238.6406\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 99254.3203 - val_loss: 198992.4219\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 85991.9062 - val_loss: 219812.8281\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 118981.1406 - val_loss: 253898.3750\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 67786.1484 - val_loss: 248023.0312\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 51969.0625 - val_loss: 248607.8281\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 52424.2773 - val_loss: 285983.8750\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 58563.3906 - val_loss: 297702.1250\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 52854.8398 - val_loss: 308207.7812\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 49655.3281 - val_loss: 305132.6875\n",
      "Epoch 12/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 48065.6016Restoring model weights from the end of the best epoch: 2.\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 47829.7773 - val_loss: 333083.1250\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 27ms/step - loss: 47616476.0000 - val_loss: 1463157.3750\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 875252.8750 - val_loss: 182632.8750\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 100102.1875 - val_loss: 258237.0469\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 1004032.4375 - val_loss: 353026.1875\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 80078.7188 - val_loss: 238909.3750\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 64947.3477 - val_loss: 258557.9531\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 65603.2891 - val_loss: 286748.2188\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 63331.5703 - val_loss: 288641.9062\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 59003.7656 - val_loss: 319162.2188\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 65925.8594 - val_loss: 334125.9375\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 67981.2188 - val_loss: 333770.6562\n",
      "Epoch 12/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 61691.3984Restoring model weights from the end of the best epoch: 2.\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 62354.0977 - val_loss: 355263.4375\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 44923676.0000 - val_loss: 1979114.1250\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 1846381.5000 - val_loss: 357963.9688\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 606129.5000 - val_loss: 204029.7344\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125047.6797 - val_loss: 215144.1406\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 89156.2969 - val_loss: 272259.8750\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 102053.2031 - val_loss: 289227.8125\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 88485.7109 - val_loss: 306818.8750\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 85091.4844 - val_loss: 313342.6875\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 76723.4453 - val_loss: 327340.7500\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 76183.1953 - val_loss: 321671.5000\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 72983.0703 - val_loss: 335443.2812\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 67376.5000 - val_loss: 353139.1562\n",
      "Epoch 13/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 63150.2852Restoring model weights from the end of the best epoch: 3.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 63459.2578 - val_loss: 377886.3125\n",
      "Epoch 13: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 36338896.0000 - val_loss: 2095840.8750\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 1020950.8750 - val_loss: 174559.9844\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 94510.6562 - val_loss: 151441.3906\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 75435.0000 - val_loss: 239300.9062\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 80364.8438 - val_loss: 257980.7500\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 501827.3750 - val_loss: 547456.3750\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 249162.2500 - val_loss: 266158.7500\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 67300.8281 - val_loss: 290005.1875\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 64291.5078 - val_loss: 307958.7188\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 60514.8281 - val_loss: 295008.1562\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 62847.6406 - val_loss: 347834.0000\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 62281.4570 - val_loss: 338736.1250\n",
      "Epoch 13/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 58787.5977Restoring model weights from the end of the best epoch: 3.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 59024.4297 - val_loss: 366662.0000\n",
      "Epoch 13: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 27127242.0000 - val_loss: 1226934.1250\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 410412.8438 - val_loss: 167566.7344\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 92369.4062 - val_loss: 195160.7656\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 74145.5469 - val_loss: 198407.1094\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 73335.6484 - val_loss: 226861.8281\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 71800.7031 - val_loss: 246719.2031\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 67689.0391 - val_loss: 271527.8750\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 59710.7930 - val_loss: 286443.7812\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 67146.8125 - val_loss: 308562.9062\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 61510.9531 - val_loss: 315538.9375\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 56933.7422 - val_loss: 327209.9375\n",
      "Epoch 12/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 62508.1367Restoring model weights from the end of the best epoch: 2.\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 62377.3477 - val_loss: 349739.7188\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 27512546.0000 - val_loss: 671235.0000\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 210101.0469 - val_loss: 161839.4062\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 82925.0547 - val_loss: 180020.5156\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 76847.3047 - val_loss: 210841.5156\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 65229.5469 - val_loss: 236969.9375\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 61028.5898 - val_loss: 259301.7344\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 49321.3477 - val_loss: 276997.5938\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 55803.3594 - val_loss: 309192.5312\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 58071.0586 - val_loss: 289875.5938\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 56046.5781 - val_loss: 347141.0625\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 56564.6641 - val_loss: 321815.5625\n",
      "Epoch 12/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 52968.1211Restoring model weights from the end of the best epoch: 2.\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 52685.3008 - val_loss: 314918.7188\n",
      "Epoch 12: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 63299300.0000 - val_loss: 1655565.7500\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 1194129.7500 - val_loss: 258292.3281\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 115653.0156 - val_loss: 195103.7031\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 85632.6875 - val_loss: 247085.7031\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 80118.7031 - val_loss: 226764.6875\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 84215.1641 - val_loss: 284044.3438\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 576011.3750 - val_loss: 756674.1250\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 445400.7500 - val_loss: 315302.0000\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 75781.8438 - val_loss: 311270.4062\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 78543.0703 - val_loss: 318357.8125\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 70939.8906 - val_loss: 342797.5000\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 73335.9219 - val_loss: 341180.8438\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 71666.2812Restoring model weights from the end of the best epoch: 3.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 71666.2812 - val_loss: 346188.9688\n",
      "Epoch 13: early stopping\n",
      "'########################################################Model9\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 142.2823 - val_loss: 141.3976\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 133.2896 - val_loss: 135.8799\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.1824 - val_loss: 136.2014\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 132.3345 - val_loss: 133.9366\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 131.7908 - val_loss: 134.5921\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.5700 - val_loss: 133.7099\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.2702 - val_loss: 132.2215\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 135.0163 - val_loss: 136.6292\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.5172 - val_loss: 134.3224\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 130.4410 - val_loss: 132.2344\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.5725 - val_loss: 136.8885\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.5589 - val_loss: 132.2384\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.0783 - val_loss: 135.8985\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.1281 - val_loss: 131.0937\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.0197 - val_loss: 132.4446\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.4629 - val_loss: 135.2613\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.8392 - val_loss: 131.9104\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 129.7089 - val_loss: 133.6557\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.9142 - val_loss: 131.7523\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.9438 - val_loss: 132.4192\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.3693 - val_loss: 131.5441\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.7755 - val_loss: 131.7387\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.0374 - val_loss: 131.7130\n",
      "Epoch 24/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 130.0763Restoring model weights from the end of the best epoch: 14.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.1061 - val_loss: 132.8703\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 152.9996 - val_loss: 147.8131\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 136.7933 - val_loss: 140.0076\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 136.5959 - val_loss: 139.4076\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 137.4889 - val_loss: 139.5732\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 138.4003 - val_loss: 137.5361\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 136.3556 - val_loss: 138.1800\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 133.4863 - val_loss: 136.4180\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 133.1187 - val_loss: 133.4482\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.8599 - val_loss: 137.4552\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.8163 - val_loss: 132.8239\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.3602 - val_loss: 131.9799\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.2580 - val_loss: 131.9826\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.6302 - val_loss: 131.6237\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.2605 - val_loss: 131.0322\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 132.4603 - val_loss: 143.1157\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 133.5603 - val_loss: 135.4448\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.8994 - val_loss: 136.0478\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.6087 - val_loss: 132.3621\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.1263 - val_loss: 133.4967\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.7302 - val_loss: 131.7551\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.0179 - val_loss: 132.8494\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.1717 - val_loss: 139.9811\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 132.8538 - val_loss: 133.2662\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 130.0108Restoring model weights from the end of the best epoch: 14.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.0108 - val_loss: 134.0006\n",
      "Epoch 24: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 4s 46ms/step - loss: 160.0071 - val_loss: 153.4223\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.8051 - val_loss: 150.4365\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 147.5431 - val_loss: 152.9653\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 146.1114 - val_loss: 148.3013\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 145.3030 - val_loss: 147.1796\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 146.5551 - val_loss: 146.7394\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 147.3171 - val_loss: 152.9870\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 147.0455 - val_loss: 141.2367\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.8058 - val_loss: 141.5449\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 141.7742 - val_loss: 141.3989\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.5171 - val_loss: 141.9370\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 141.3727 - val_loss: 143.0980\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.5058 - val_loss: 140.8539\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 136.3948 - val_loss: 138.7083\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.4736 - val_loss: 135.0511\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.5941 - val_loss: 135.9466\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.5638 - val_loss: 135.4168\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.2495 - val_loss: 132.9726\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 135.6370 - val_loss: 138.1909\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.0163 - val_loss: 133.9091\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 133.7636 - val_loss: 133.6968\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.0306 - val_loss: 134.8381\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.3721 - val_loss: 133.3585\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.5993 - val_loss: 132.8606\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.6943 - val_loss: 140.2583\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 133.1562 - val_loss: 134.2561\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.0110 - val_loss: 136.0865\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 134.7500 - val_loss: 133.2912\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 133.3468 - val_loss: 133.4452\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.9776 - val_loss: 132.5306\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.3142 - val_loss: 132.1330\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.4427 - val_loss: 133.3499\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.3716 - val_loss: 131.4556\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.4237 - val_loss: 131.9462\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.0652 - val_loss: 134.0197\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.9364 - val_loss: 134.5022\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.6333 - val_loss: 132.1670\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.5580 - val_loss: 133.6712\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.9011 - val_loss: 131.9290\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.5104 - val_loss: 134.2864\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 126.5784 - val_loss: 130.5999\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.9352 - val_loss: 129.7842\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 125.7741 - val_loss: 135.8043\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 126.1224 - val_loss: 131.1832\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 125.6016 - val_loss: 129.8219\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 125.2884 - val_loss: 130.1183\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 125.1639 - val_loss: 132.6424\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 125.5577 - val_loss: 131.2020\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.3817 - val_loss: 130.1057\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 124.7742 - val_loss: 131.3513\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 125.4329 - val_loss: 129.5359\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 125.0588 - val_loss: 127.5807\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.5120 - val_loss: 130.1566\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 124.9754 - val_loss: 129.0924\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 124.6414 - val_loss: 128.3851\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 124.4238 - val_loss: 128.5785\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 124.4652 - val_loss: 128.3278\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.0639 - val_loss: 127.8806\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 124.1977 - val_loss: 128.9034\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 124.3986 - val_loss: 128.1820\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 123.9928 - val_loss: 127.7133\n",
      "Epoch 62/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 124.2530Restoring model weights from the end of the best epoch: 52.\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 124.2595 - val_loss: 127.8863\n",
      "Epoch 62: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 30ms/step - loss: 152.7186 - val_loss: 148.4252\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 149.3483 - val_loss: 147.5045\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 148.0980 - val_loss: 146.1962\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 146.8166 - val_loss: 145.8316\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 146.1841 - val_loss: 144.6124\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 150.0087 - val_loss: 149.9027\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.7947 - val_loss: 148.5354\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 149.6034 - val_loss: 148.7700\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 149.8531 - val_loss: 150.1005\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.2278 - val_loss: 148.6101\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.1990 - val_loss: 148.3610\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.2768 - val_loss: 148.0763\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 148.0965 - val_loss: 142.0826\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.0783 - val_loss: 137.6047\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.3953 - val_loss: 141.1590\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 143.6398 - val_loss: 141.2274\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.1723 - val_loss: 138.1315\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.6678 - val_loss: 154.9260\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 139.8042 - val_loss: 141.2723\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 137.7745 - val_loss: 137.2186\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 136.1544 - val_loss: 136.5680\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 136.5818 - val_loss: 138.4069\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 133.4022 - val_loss: 133.6648\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 132.2579 - val_loss: 133.5023\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 131.1268 - val_loss: 132.6097\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.7750 - val_loss: 133.7604\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 132.1463 - val_loss: 133.5685\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 131.3542 - val_loss: 134.5170\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.1357 - val_loss: 134.3667\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.7214 - val_loss: 132.2038\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.4223 - val_loss: 133.4946\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.3042 - val_loss: 136.0063\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.0726 - val_loss: 132.2799\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.4911 - val_loss: 133.5284\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 130.7340 - val_loss: 133.4817\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.1594 - val_loss: 132.9005\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.0750 - val_loss: 132.1676\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 129.7626 - val_loss: 134.0749\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.7076 - val_loss: 131.9634\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.7253 - val_loss: 132.5900\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 129.6419 - val_loss: 131.7264\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 129.9525 - val_loss: 131.5661\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 129.7356 - val_loss: 132.7005\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 130.0038 - val_loss: 133.0719\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.9487 - val_loss: 131.5813\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 129.7843 - val_loss: 131.2458\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.4220 - val_loss: 131.6787\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.7598 - val_loss: 131.4417\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.5598 - val_loss: 131.8678\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.2050 - val_loss: 132.0016\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.2480 - val_loss: 132.5583\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.8261 - val_loss: 131.8547\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 129.6205 - val_loss: 132.3632\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 129.2968 - val_loss: 132.1683\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.6029 - val_loss: 131.6044\n",
      "Epoch 56/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 129.2446Restoring model weights from the end of the best epoch: 46.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 129.2312 - val_loss: 131.9932\n",
      "Epoch 56: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 148.6382 - val_loss: 149.9254\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.8117 - val_loss: 148.9925\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.8453 - val_loss: 150.2032\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 146.8437 - val_loss: 149.6919\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.7186 - val_loss: 145.7147\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.2241 - val_loss: 145.1601\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.7797 - val_loss: 147.3193\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 146.0987 - val_loss: 144.8428\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.9325 - val_loss: 147.7035\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.7104 - val_loss: 147.5916\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 144.9931 - val_loss: 146.1305\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 144.0816 - val_loss: 143.0896\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 140.4415 - val_loss: 141.2973\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 140.2276 - val_loss: 142.0973\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 140.2121 - val_loss: 140.1434\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 139.8929 - val_loss: 139.7359\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.6275 - val_loss: 143.2085\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.4120 - val_loss: 143.3490\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 140.7971 - val_loss: 140.1082\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.8986 - val_loss: 140.8918\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 139.3411 - val_loss: 140.6282\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.3146 - val_loss: 140.0544\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 138.9418 - val_loss: 140.6287\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 138.4158 - val_loss: 145.2270\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.0175 - val_loss: 139.3038\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 135.8829 - val_loss: 139.4427\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 135.9573 - val_loss: 138.0714\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.9744 - val_loss: 139.7343\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 136.8805 - val_loss: 139.8919\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 138.9181 - val_loss: 140.7902\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 138.7535 - val_loss: 140.7363\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.9975 - val_loss: 140.9853\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.2924 - val_loss: 139.5470\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.7629 - val_loss: 141.8085\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 139.1811 - val_loss: 139.1021\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 138.5404 - val_loss: 141.3401\n",
      "Epoch 37/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 138.7516Restoring model weights from the end of the best epoch: 27.\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 138.6784 - val_loss: 141.7572\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 153.6102 - val_loss: 150.5269\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.8900 - val_loss: 140.0725\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.5473 - val_loss: 138.9272\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.0307 - val_loss: 138.4816\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 135.5386 - val_loss: 134.9161\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 132.7576 - val_loss: 134.3598\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.1336 - val_loss: 133.1181\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.9588 - val_loss: 134.8973\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 131.1154 - val_loss: 132.6185\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 132.7842 - val_loss: 132.0807\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 127.8228 - val_loss: 132.7539\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.8948 - val_loss: 138.3146\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 143.5001 - val_loss: 140.1173\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 132.3714 - val_loss: 136.5501\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 133.4955 - val_loss: 146.2141\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 140.7729 - val_loss: 139.1832\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.6010 - val_loss: 133.0816\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 128.9223 - val_loss: 130.9592\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 127.7453 - val_loss: 132.2653\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 126.7210 - val_loss: 130.1262\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 126.2488 - val_loss: 132.9071\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.4492 - val_loss: 130.4707\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.9081 - val_loss: 130.0179\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 125.9162 - val_loss: 129.3479\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.0299 - val_loss: 131.3949\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 125.9382 - val_loss: 128.4364\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 125.8542 - val_loss: 130.6939\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 128.1879 - val_loss: 129.9751\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.6042 - val_loss: 129.6578\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.2611 - val_loss: 128.7649\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 125.6572 - val_loss: 127.9722\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.6643 - val_loss: 127.9638\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 125.7584 - val_loss: 128.6398\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.0246 - val_loss: 128.0056\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.5957 - val_loss: 127.7437\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.7602 - val_loss: 128.0929\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.0792 - val_loss: 127.8039\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 128.2536 - val_loss: 130.3384\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.6531 - val_loss: 129.7051\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.5435 - val_loss: 127.3696\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.4147 - val_loss: 129.7560\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.5911 - val_loss: 128.6425\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.6845 - val_loss: 131.2064\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.9754 - val_loss: 129.4220\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.6235 - val_loss: 129.8803\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.7301 - val_loss: 128.2715\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.1750 - val_loss: 129.4751\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 124.0668 - val_loss: 128.0242\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 123.8915 - val_loss: 128.5797\n",
      "Epoch 50/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 124.2382Restoring model weights from the end of the best epoch: 40.\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.2708 - val_loss: 128.2123\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model5\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 26ms/step - loss: 158.5689 - val_loss: 157.0956\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 156.5592 - val_loss: 156.8732\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 156.0968 - val_loss: 160.1563\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 156.4173 - val_loss: 159.4017\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 156.1946 - val_loss: 155.4689\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 154.2701 - val_loss: 156.8681\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 154.6322 - val_loss: 156.5406\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 153.0873 - val_loss: 158.3374\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 151.0714 - val_loss: 151.2375\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 148.7435 - val_loss: 151.5857\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 149.0833 - val_loss: 151.4507\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 149.1151 - val_loss: 151.3755\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 148.7336 - val_loss: 151.0769\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 148.3533 - val_loss: 150.7665\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 148.8258 - val_loss: 153.2032\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 146.4996 - val_loss: 149.5597\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 145.5708 - val_loss: 150.6183\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 145.2631 - val_loss: 149.3934\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 145.1633 - val_loss: 148.5204\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 145.4590 - val_loss: 148.7706\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 145.1236 - val_loss: 150.2359\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 145.4835 - val_loss: 148.4181\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 146.0563 - val_loss: 151.0282\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 146.4187 - val_loss: 150.3010\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 145.6771 - val_loss: 147.6170\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 145.4161 - val_loss: 148.4349\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 145.0301 - val_loss: 149.5185\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 144.7380 - val_loss: 148.6606\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 144.4758 - val_loss: 149.1321\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 144.5720 - val_loss: 149.2693\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 144.8587 - val_loss: 148.6940\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 144.7205 - val_loss: 148.2727\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 144.4269 - val_loss: 147.1047\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 144.7397 - val_loss: 149.1500\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 144.5617 - val_loss: 149.3584\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 145.1308 - val_loss: 148.1943\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 144.6859 - val_loss: 145.0193\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 138.1452 - val_loss: 141.5383\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 138.2155 - val_loss: 145.2898\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 138.1716 - val_loss: 142.1597\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 138.3250 - val_loss: 141.3624\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 138.0630 - val_loss: 142.1192\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 138.7170 - val_loss: 142.2858\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 137.7492 - val_loss: 141.1221\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 137.6579 - val_loss: 141.3746\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 138.1646 - val_loss: 140.8752\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 137.5669 - val_loss: 143.8549\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 138.0706 - val_loss: 141.5476\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 138.2629 - val_loss: 143.9420\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 138.2693 - val_loss: 141.5234\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 137.5698 - val_loss: 142.6485\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.3036 - val_loss: 145.6979\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 137.9689 - val_loss: 140.7573\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 138.3066 - val_loss: 141.6936\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.3315 - val_loss: 140.7895\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 137.9555 - val_loss: 141.2835\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.4660 - val_loss: 140.9675\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1669 - val_loss: 140.8835\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 137.7355 - val_loss: 140.7320\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 139.6490 - val_loss: 142.6333\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 132.6927 - val_loss: 135.5755\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.6028 - val_loss: 134.8210\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.5633 - val_loss: 136.3367\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 131.4281 - val_loss: 135.5744\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 2s 23ms/step - loss: 131.4373 - val_loss: 133.9529\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.9145 - val_loss: 135.5035\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.9589 - val_loss: 134.1959\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.9498 - val_loss: 136.2584\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.9635 - val_loss: 133.8090\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.0354 - val_loss: 135.6235\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.8062 - val_loss: 135.1907\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 131.0762 - val_loss: 135.2416\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.6002 - val_loss: 134.6809\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.6351 - val_loss: 134.4056\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.3289 - val_loss: 134.4977\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 130.4774 - val_loss: 133.3653\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 130.3427 - val_loss: 135.0934\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 130.5511 - val_loss: 134.2617\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.6315 - val_loss: 134.0876\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.7557 - val_loss: 134.5421\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.6203 - val_loss: 134.7010\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.5002 - val_loss: 134.6775\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.4875 - val_loss: 134.7181\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.7827 - val_loss: 134.4738\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 130.3560 - val_loss: 134.5336\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 130.1129Restoring model weights from the end of the best epoch: 76.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 130.1129 - val_loss: 135.0302\n",
      "Epoch 86: early stopping\n",
      "'########################################################Model6\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 29ms/step - loss: 144.0485 - val_loss: 143.0873\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 134.8059 - val_loss: 134.3029\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 132.0098 - val_loss: 132.6901\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 132.4294 - val_loss: 133.3403\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 132.0157 - val_loss: 136.1759\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 129.7593 - val_loss: 136.8882\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 128.3139 - val_loss: 131.4007\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 127.8223 - val_loss: 131.9550\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 127.8251 - val_loss: 130.9443\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 127.1091 - val_loss: 130.3990\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 128.5677 - val_loss: 132.4522\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 127.5359 - val_loss: 131.2616\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.7955 - val_loss: 130.3651\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.9937 - val_loss: 132.1441\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 127.0319 - val_loss: 130.5399\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 126.5164 - val_loss: 129.9245\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.8431 - val_loss: 129.7205\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 126.8285 - val_loss: 130.8275\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 126.1370 - val_loss: 129.7272\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 125.8218 - val_loss: 129.7060\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 126.1594 - val_loss: 128.6820\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 125.8096 - val_loss: 129.3651\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 126.6801 - val_loss: 133.2378\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 126.4090 - val_loss: 130.3010\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 21ms/step - loss: 127.4183 - val_loss: 131.5052\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 127.7599 - val_loss: 130.7390\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.5601 - val_loss: 131.1467\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.6021 - val_loss: 131.8927\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 126.0564 - val_loss: 130.3650\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 125.2932 - val_loss: 130.1020\n",
      "Epoch 31/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 124.6146Restoring model weights from the end of the best epoch: 21.\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 124.7077 - val_loss: 129.6244\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model7\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 25ms/step - loss: 146.2488 - val_loss: 143.9701\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 141.3682 - val_loss: 146.1944\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 138.2466 - val_loss: 142.1462\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 20ms/step - loss: 137.8003 - val_loss: 140.8194\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 138.1063 - val_loss: 144.2119\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 137.8489 - val_loss: 140.0646\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 137.6782 - val_loss: 141.5771\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 140.2749 - val_loss: 141.1040\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 22ms/step - loss: 140.2989 - val_loss: 142.0522\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 140.5304 - val_loss: 141.3716\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 143.8407 - val_loss: 143.9628\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 144.8884 - val_loss: 143.9337\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 145.2857 - val_loss: 145.4546\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.6899 - val_loss: 143.1461\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.4859 - val_loss: 145.3818\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 144.9682Restoring model weights from the end of the best epoch: 6.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.9682 - val_loss: 144.1091\n",
      "Epoch 16: early stopping\n",
      "'########################################################Model8\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 3s 31ms/step - loss: 150.0629 - val_loss: 147.3480\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.6542 - val_loss: 149.3356\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 147.4285 - val_loss: 148.4606\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 147.1545 - val_loss: 147.1371\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 147.0011 - val_loss: 146.9385\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 146.5004 - val_loss: 147.7595\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 147.5754 - val_loss: 148.2724\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.8029 - val_loss: 145.6566\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 145.3347 - val_loss: 149.9598\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 146.3054 - val_loss: 145.2807\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 145.3888 - val_loss: 147.2567\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 144.6723 - val_loss: 144.9840\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 140.3597 - val_loss: 139.2873\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.6286 - val_loss: 138.1882\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 138.5755 - val_loss: 140.4654\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 138.3017 - val_loss: 138.1389\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 138.7945 - val_loss: 138.1536\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 137.4196 - val_loss: 137.6525\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.1623 - val_loss: 137.9500\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 138.1157 - val_loss: 140.9947\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 2s 24ms/step - loss: 138.3737 - val_loss: 138.9279\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.7800 - val_loss: 138.9369\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 138.0849 - val_loss: 138.4315\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.6632 - val_loss: 138.0699\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.4900 - val_loss: 137.2530\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.6219 - val_loss: 138.4498\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 2s 28ms/step - loss: 137.6494 - val_loss: 138.5056\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.6555 - val_loss: 137.9268\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1695 - val_loss: 136.7240\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1477 - val_loss: 137.9975\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 137.7220 - val_loss: 140.5755\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 137.5679 - val_loss: 138.4523\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 137.0822 - val_loss: 139.6416\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 2s 25ms/step - loss: 138.1758 - val_loss: 137.5814\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1849 - val_loss: 137.6642\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 2s 27ms/step - loss: 137.2580 - val_loss: 137.8995\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1522 - val_loss: 139.9052\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.6360 - val_loss: 139.7995\n",
      "Epoch 39/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 137.1537Restoring model weights from the end of the best epoch: 29.\n",
      "72/72 [==============================] - 2s 26ms/step - loss: 137.1162 - val_loss: 137.8125\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),100,10,8)\n",
    "mape_models = train_bagging_models(model_num,'mape',100,10,8)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),100,10,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468ad253-6a73-4a53-9d07-ad17ec4c1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 20ms/step\n",
      "12/12 [==============================] - 0s 24ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 1s 22ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22511968088238352, 0.24830489333589362)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, test_X)\n",
    "mase_predictions = bagging_predict2(pred2, test_X)\n",
    "mape_predictions = bagging_predict2(pred3, test_X)\n",
    "concat = np.concatenate([smape_predictions, mase_predictions,mape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "mean_squared_error(test_y.flatten(),fin_pred.flatten()),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0204ee-8daf-431a-b391-0926f4570676",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin_pred).to_csv(\"../result7_new/transformer/pred_mid.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat[i]).to_csv(f\"../result7_new/transformer/pred{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
