{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e097566-30f2-4f57-b3a2-2ae3b7d02005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "#import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c74fd4-9d10-4b7d-9c83-4da3c7f1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target_X= pd.read_csv(\"../data/solor_train_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"../data/solor_train_output_7.csv\").iloc[:,1:].values\n",
    "test_X= pd.read_csv(\"../data/solor_val_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "test_y =pd.read_csv(\"../data/solor_val_output_7.csv\").iloc[:,1:].values\n",
    "\n",
    "X_train=target_X\n",
    "y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bd839-0f9e-49d3-96a8-f1fe23500d97",
   "metadata": {},
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117869af-8623-4129-aa35-bcc7ee3da674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],y_train.shape[1],1,1,128\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 생성 함수\n",
    "def bulid_model(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK)\n",
    "                   ,nb_blocks_per_stack=1, thetas_dim=(1,2,4,4),\n",
    "                   share_weights_in_stack=True, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + G모델 생성 함수    \n",
    "def bulid_model_G(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.GENERIC_BLOCK,NBeatsKeras.GENERIC_BLOCK)\n",
    "                   ,nb_blocks_per_stack=5, thetas_dim=(4,4),\n",
    "                   share_weights_in_stack=False, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models_G(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model_G(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "# SMAPE 용\n",
    "def train_bagging_models_smape(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[np.newaxis, ...]\n",
    "\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "def create_model(fn,d_model, nlayers, nhead, dropout, iw, ow,lr):\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(pretrained_output_reshaped)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "    \n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    \n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    target_model = Model(inputs=inputs, outputs=outputs)\n",
    "    target_model.compile(optimizer=optimizer, loss=fn)\n",
    "    \n",
    "    return target_model\n",
    "#################################################################################\n",
    "# 예측\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4c593-18a3-4fa1-be2e-8894dc7d453f",
   "metadata": {},
   "source": [
    "# 모형적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f13c2f-9812-460b-9269-22aa352bfc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 16:12:22.871278: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-09-06 16:12:22.871314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-09-06 16:12:22.872683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 1.2357 - val_loss: 0.7832\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8658 - val_loss: 0.7338\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8185 - val_loss: 0.7083\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7856 - val_loss: 0.6501\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7665 - val_loss: 0.6436\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7348 - val_loss: 0.6242\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7127 - val_loss: 0.6859\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7128 - val_loss: 0.6630\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6909 - val_loss: 0.6420\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6799 - val_loss: 0.6495\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 0.6383\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6455 - val_loss: 0.6353\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6314 - val_loss: 0.6538\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6226 - val_loss: 0.6659\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5924 - val_loss: 0.7111\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.6018 - val_loss: 0.6526\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 1.2592 - val_loss: 0.7836\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 0.7240\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8088 - val_loss: 0.6817\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7833 - val_loss: 0.6613\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7366 - val_loss: 0.6652\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7608 - val_loss: 0.6714\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 0.6267\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7194 - val_loss: 0.6678\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6913 - val_loss: 0.6131\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6700 - val_loss: 0.6604\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6825 - val_loss: 0.6258\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6564 - val_loss: 0.6082\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6361 - val_loss: 0.6287\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6232 - val_loss: 0.6150\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5904 - val_loss: 0.6397\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5874 - val_loss: 0.6015\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5713 - val_loss: 0.6334\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5636 - val_loss: 0.6270\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5399 - val_loss: 0.6511\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5257 - val_loss: 0.6177\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5226 - val_loss: 0.6386\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5126 - val_loss: 0.6493\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4953 - val_loss: 0.6308\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4769 - val_loss: 0.6314\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4601 - val_loss: 0.6312\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4807 - val_loss: 0.6588\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.3115 - val_loss: 0.7942\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8771 - val_loss: 0.7158\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8075 - val_loss: 0.6624\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7780 - val_loss: 0.6364\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7543 - val_loss: 0.6776\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7263 - val_loss: 0.6732\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7151 - val_loss: 0.6461\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7076 - val_loss: 0.6578\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7043 - val_loss: 0.6592\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6758 - val_loss: 0.6857\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6635 - val_loss: 0.6353\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6503 - val_loss: 0.6367\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6255 - val_loss: 0.6619\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6199 - val_loss: 0.6490\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5974 - val_loss: 0.6667\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6051 - val_loss: 0.6265\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6026 - val_loss: 0.6640\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5667 - val_loss: 0.6564\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5560 - val_loss: 0.6824\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5240 - val_loss: 0.6306\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5425 - val_loss: 0.6313\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5064 - val_loss: 0.6478\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4884 - val_loss: 0.6830\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5057 - val_loss: 0.6600\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4987 - val_loss: 0.6793\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4861 - val_loss: 0.6636\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2961 - val_loss: 0.7473\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8819 - val_loss: 0.6971\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8213 - val_loss: 0.7042\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7986 - val_loss: 0.6654\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7579 - val_loss: 0.6567\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7326 - val_loss: 0.6432\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7263 - val_loss: 0.6782\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7092 - val_loss: 0.6418\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7047 - val_loss: 0.6554\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6970 - val_loss: 0.6465\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6859 - val_loss: 0.6357\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6571 - val_loss: 0.6239\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6331 - val_loss: 0.6752\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6262 - val_loss: 0.6368\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6109 - val_loss: 0.6814\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6053 - val_loss: 0.6433\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5790 - val_loss: 0.6377\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5724 - val_loss: 0.6602\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5608 - val_loss: 0.6862\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5449 - val_loss: 0.6608\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5528 - val_loss: 0.6620\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5269 - val_loss: 0.6517\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2897 - val_loss: 0.7933\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8987 - val_loss: 0.7142\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8304 - val_loss: 0.6873\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7761 - val_loss: 0.6443\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7864 - val_loss: 0.6765\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7438 - val_loss: 0.6554\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7402 - val_loss: 0.6276\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7185 - val_loss: 0.6367\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7232 - val_loss: 0.6122\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6887 - val_loss: 0.6610\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6943 - val_loss: 0.6239\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6615 - val_loss: 0.6137\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6560 - val_loss: 0.6353\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6515 - val_loss: 0.6192\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6113 - val_loss: 0.6058\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6287 - val_loss: 0.6499\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6279 - val_loss: 0.6855\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5835 - val_loss: 0.6528\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.6322\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5670 - val_loss: 0.6101\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5465 - val_loss: 0.6824\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5428 - val_loss: 0.6584\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5260 - val_loss: 0.6405\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4869 - val_loss: 0.6629\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5199 - val_loss: 0.6586\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 1.3068 - val_loss: 0.7893\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8950 - val_loss: 0.7313\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8199 - val_loss: 0.6801\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 0.6673\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7478 - val_loss: 0.6454\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7438 - val_loss: 0.6717\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7177 - val_loss: 0.6365\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7148 - val_loss: 0.6441\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6802 - val_loss: 0.6698\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6879 - val_loss: 0.6318\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6742 - val_loss: 0.6337\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6618 - val_loss: 0.6385\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6496 - val_loss: 0.6251\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6211 - val_loss: 0.6390\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6023 - val_loss: 0.6049\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6034 - val_loss: 0.6655\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5876 - val_loss: 0.6660\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5805 - val_loss: 0.6588\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5703 - val_loss: 0.6295\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5786 - val_loss: 0.6271\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5489 - val_loss: 0.6634\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5382 - val_loss: 0.6276\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.6096\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.6357\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5024 - val_loss: 0.6407\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 1.3200 - val_loss: 0.8759\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8857 - val_loss: 0.7081\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8213 - val_loss: 0.7278\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7946 - val_loss: 0.6586\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7608 - val_loss: 0.6432\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7560 - val_loss: 0.6806\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7396 - val_loss: 0.6683\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7157 - val_loss: 0.6522\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 0.6458\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6861 - val_loss: 0.6271\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6758 - val_loss: 0.6374\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6757 - val_loss: 0.6172\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6609 - val_loss: 0.7234\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6550 - val_loss: 0.6911\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6520 - val_loss: 0.6301\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6586 - val_loss: 0.6183\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6216 - val_loss: 0.6179\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 0.6514\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5806 - val_loss: 0.6527\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 0.6459\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5848 - val_loss: 0.6557\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 0.6990\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 1.2551 - val_loss: 0.8192\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8898 - val_loss: 0.6893\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8014 - val_loss: 0.6888\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7991 - val_loss: 0.6937\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7677 - val_loss: 0.6424\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7394 - val_loss: 0.6355\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7069 - val_loss: 0.6375\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7001 - val_loss: 0.6782\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6444\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6852 - val_loss: 0.6487\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6841 - val_loss: 0.6688\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6517 - val_loss: 0.6651\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6221 - val_loss: 0.6896\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6286 - val_loss: 0.6591\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 0.6487\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6058 - val_loss: 0.6658\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 1.3280 - val_loss: 0.7850\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8810 - val_loss: 0.7468\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.8247 - val_loss: 0.7307\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7964 - val_loss: 0.6973\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7611 - val_loss: 0.6475\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7340 - val_loss: 0.6770\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7325 - val_loss: 0.6317\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7169 - val_loss: 0.6742\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7041 - val_loss: 0.6646\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6759 - val_loss: 0.6368\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6482 - val_loss: 0.6746\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 0.6274\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6376 - val_loss: 0.6575\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6432 - val_loss: 0.6436\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.6712\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5914 - val_loss: 0.6029\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5655 - val_loss: 0.6569\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5621 - val_loss: 0.6467\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5409 - val_loss: 0.6189\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5422 - val_loss: 0.6866\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.5162 - val_loss: 0.6484\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5072 - val_loss: 0.6618\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4891 - val_loss: 0.6553\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4894 - val_loss: 0.6372\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4801 - val_loss: 0.6847\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4641 - val_loss: 0.6365\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.4337 - val_loss: 0.8508\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8893 - val_loss: 0.7146\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.8376 - val_loss: 0.6890\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7914 - val_loss: 0.7143\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7792 - val_loss: 0.6411\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7659 - val_loss: 0.6485\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7417 - val_loss: 0.6542\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.7214 - val_loss: 0.6328\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.7115 - val_loss: 0.6859\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 0.6470\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6568 - val_loss: 0.6274\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6479 - val_loss: 0.6606\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.6310 - val_loss: 0.6567\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6211 - val_loss: 0.6505\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5992 - val_loss: 0.6660\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6073 - val_loss: 0.6157\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6044 - val_loss: 0.6750\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5531 - val_loss: 0.6885\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5693 - val_loss: 0.6348\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5631 - val_loss: 0.6442\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5183 - val_loss: 0.6777\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5102 - val_loss: 0.6734\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.5121 - val_loss: 0.6622\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4975 - val_loss: 0.6788\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4641 - val_loss: 0.6877\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4634 - val_loss: 0.7053\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 111188736.0000 - val_loss: 38580036.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 32357558.0000 - val_loss: 23586764.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 21339384.0000 - val_loss: 19767572.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16263703.0000 - val_loss: 16215644.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12151956.0000 - val_loss: 11892284.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 10004738.0000 - val_loss: 9655192.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8235890.0000 - val_loss: 7879013.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7206446.0000 - val_loss: 7600133.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6510089.5000 - val_loss: 6515718.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6634283.5000 - val_loss: 5512239.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5215309.0000 - val_loss: 6089490.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5058486.0000 - val_loss: 4320793.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3881865.0000 - val_loss: 6813281.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4292284.0000 - val_loss: 5808956.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3396419.7500 - val_loss: 3628908.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4121436.5000 - val_loss: 6029082.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3346826.5000 - val_loss: 3621839.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2554540.2500 - val_loss: 3018902.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2663368.5000 - val_loss: 2372768.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2253798.0000 - val_loss: 2300142.2500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2196910.5000 - val_loss: 3262922.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2487937.7500 - val_loss: 3106935.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2069507.7500 - val_loss: 2337011.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2239123.7500 - val_loss: 2355916.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1816383.3750 - val_loss: 1954694.6250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1889619.1250 - val_loss: 2230787.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2022900.0000 - val_loss: 2533491.2500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1499909.8750 - val_loss: 1770321.5000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1711176.2500 - val_loss: 1537138.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1455176.7500 - val_loss: 1557348.0000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1536471.1250 - val_loss: 1869801.0000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1584788.0000 - val_loss: 1332336.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1414416.6250 - val_loss: 1373615.1250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1637196.5000 - val_loss: 1346870.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1443466.5000 - val_loss: 1242425.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1336952.7500 - val_loss: 1846069.2500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1381443.5000 - val_loss: 1230360.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1409553.6250 - val_loss: 1891375.6250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1134723.3750 - val_loss: 1121512.0000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1158150.3750 - val_loss: 1150478.5000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1312300.6250 - val_loss: 1456957.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1215803.1250 - val_loss: 1062171.5000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1204793.3750 - val_loss: 1729992.7500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1295032.1250 - val_loss: 1109341.1250\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1077134.8750 - val_loss: 1322864.5000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1106263.1250 - val_loss: 2513409.2500\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1287158.2500 - val_loss: 4243010.5000\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1282958.6250 - val_loss: 1528606.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1418104.6250 - val_loss: 2556180.7500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1129829.7500 - val_loss: 1209440.5000\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1536741.1250 - val_loss: 1610676.7500\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 912636.2500 - val_loss: 927322.3750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 941917.5625 - val_loss: 986172.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 938557.8750 - val_loss: 879085.1250\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 772757.5000 - val_loss: 1057555.2500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 734511.0000 - val_loss: 785277.4375\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 671500.6875 - val_loss: 659114.1875\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 643016.7500 - val_loss: 1117416.2500\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1008525.8750 - val_loss: 767701.7500\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 879697.3750 - val_loss: 739495.7500\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 708310.2500 - val_loss: 1518213.3750\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 736937.1875 - val_loss: 862859.8125\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 706129.2500 - val_loss: 877258.0000\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 824324.2500 - val_loss: 977133.2500\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 747076.4375 - val_loss: 1114535.1250\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 903022.6875 - val_loss: 825523.6875\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 651550.6250 - val_loss: 714579.5000\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 131510184.0000 - val_loss: 51178860.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 40181336.0000 - val_loss: 32904696.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 25960278.0000 - val_loss: 22711074.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 17524444.0000 - val_loss: 14870800.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13139477.0000 - val_loss: 11932056.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10889699.0000 - val_loss: 11724016.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 9555739.0000 - val_loss: 10927423.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8148612.5000 - val_loss: 8743556.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7917666.0000 - val_loss: 7729503.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 7206592.0000 - val_loss: 7469253.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5709054.0000 - val_loss: 6782409.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4638578.0000 - val_loss: 5281812.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4725660.5000 - val_loss: 4751981.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4185172.5000 - val_loss: 4854929.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4434780.5000 - val_loss: 5411085.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4210521.0000 - val_loss: 3227961.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3791500.0000 - val_loss: 4510449.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3804761.2500 - val_loss: 3120139.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2982734.2500 - val_loss: 2963054.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2640223.7500 - val_loss: 2850243.2500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2923933.0000 - val_loss: 2971163.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2491656.2500 - val_loss: 2947731.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2363123.0000 - val_loss: 2720094.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2102961.2500 - val_loss: 2314076.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1961692.8750 - val_loss: 2611035.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2081047.7500 - val_loss: 2092831.7500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1783942.0000 - val_loss: 2324038.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2110192.7500 - val_loss: 2529333.0000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1943382.6250 - val_loss: 1931160.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1641253.1250 - val_loss: 1865952.8750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1529774.5000 - val_loss: 1528671.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1499484.0000 - val_loss: 1578043.6250\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1417796.8750 - val_loss: 1698776.3750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1787969.7500 - val_loss: 1547682.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1212598.3750 - val_loss: 1712491.1250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1598083.3750 - val_loss: 2406344.5000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1596062.6250 - val_loss: 1290738.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1901337.7500 - val_loss: 1797278.6250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1396261.6250 - val_loss: 1809409.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1309767.5000 - val_loss: 1225144.3750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1190769.7500 - val_loss: 1203245.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1119698.0000 - val_loss: 1284434.7500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1425084.7500 - val_loss: 1496123.5000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1228190.5000 - val_loss: 1125270.5000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1248773.5000 - val_loss: 1138613.5000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1057600.5000 - val_loss: 1255448.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 939373.2500 - val_loss: 1032592.6875\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1295992.5000 - val_loss: 1902665.8750\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 941605.4375 - val_loss: 946247.3125\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1300246.0000 - val_loss: 1318966.8750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1120498.0000 - val_loss: 1157320.2500\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 938067.1250 - val_loss: 910421.9375\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 750849.8750 - val_loss: 1205319.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 899997.4375 - val_loss: 870776.0000\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 740017.9375 - val_loss: 1020391.9375\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 808639.6875 - val_loss: 1021437.8125\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 723268.6250 - val_loss: 993831.8125\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 687525.0625 - val_loss: 693626.9375\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 605628.8750 - val_loss: 620583.3125\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 574336.1875 - val_loss: 703395.5000\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 653271.5625 - val_loss: 624981.4375\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 543867.7500 - val_loss: 575979.0625\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 501776.1562 - val_loss: 578324.0000\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 696707.3125 - val_loss: 492537.7500\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 451248.8750 - val_loss: 401062.5938\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 505307.3438 - val_loss: 508261.8438\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 392136.1250 - val_loss: 453000.8750\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 386245.2812 - val_loss: 509476.9688\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 345002.8750 - val_loss: 337809.0000\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 278990.8125 - val_loss: 486091.2500\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 325888.2500 - val_loss: 450113.2500\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 307773.0938 - val_loss: 433772.1562\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 292500.3750 - val_loss: 285537.7812\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 239183.4219 - val_loss: 331806.3438\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 245991.6719 - val_loss: 276162.2812\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 225591.7500 - val_loss: 336107.3438\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 320539.8750 - val_loss: 185739.2031\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 243828.1406 - val_loss: 253520.0312\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 276355.5938 - val_loss: 322969.3438\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 265222.2188 - val_loss: 303601.1875\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 245517.9219 - val_loss: 209871.6719\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 213186.5469 - val_loss: 183418.9688\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 254853.1094 - val_loss: 213111.6562\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 211176.8438 - val_loss: 326432.9688\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 207430.8906 - val_loss: 249295.7500\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 173969.3750 - val_loss: 100994.4766\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 172489.7656 - val_loss: 283958.0000\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 145550.9062 - val_loss: 118546.3828\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 152869.6094 - val_loss: 222885.8906\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 153743.6094 - val_loss: 135982.3281\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132543.7188 - val_loss: 117737.3906\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 115074.7266 - val_loss: 76079.3672\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 113551.3828 - val_loss: 122306.4297\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 166663.6875 - val_loss: 267101.0000\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 244090.3906 - val_loss: 484388.6250\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 259098.6094 - val_loss: 248854.1719\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 299106.1250 - val_loss: 424010.2500\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 175234.7031 - val_loss: 45420.9492\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 108013.1172 - val_loss: 197911.1250\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 140700.2656 - val_loss: 22131.8066\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 173144.1875 - val_loss: 707761.3125\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 257575.1406 - val_loss: 164719.9531\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 194956.8594 - val_loss: 35106.1523\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122523.3438 - val_loss: 174980.5312\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 103074.9844 - val_loss: 92978.8281\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 135409.3125 - val_loss: 25141.6680\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 112655.0234 - val_loss: 87370.3438\n",
      "Epoch 108/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 62972.0625 - val_loss: 104071.1484\n",
      "Epoch 109/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 78074.4688 - val_loss: 169276.0156\n",
      "Epoch 110/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122428.9062 - val_loss: 321657.4062\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 130324184.0000 - val_loss: 43912184.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 32427726.0000 - val_loss: 30284934.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 24553180.0000 - val_loss: 22140738.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 18442602.0000 - val_loss: 15800597.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 14044291.0000 - val_loss: 13582448.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 10573064.0000 - val_loss: 10963520.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9928282.0000 - val_loss: 9613140.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7999690.0000 - val_loss: 8100495.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7112554.5000 - val_loss: 7774884.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 5712829.0000 - val_loss: 5465388.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4767024.0000 - val_loss: 5577791.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4757949.0000 - val_loss: 5113575.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4461900.0000 - val_loss: 6166140.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4095726.2500 - val_loss: 4357883.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3888814.2500 - val_loss: 3918048.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3900950.7500 - val_loss: 4031357.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3231383.0000 - val_loss: 3368825.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2914418.0000 - val_loss: 3105214.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3223173.5000 - val_loss: 3385945.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2732221.5000 - val_loss: 2841258.2500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2601226.7500 - val_loss: 3207134.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2603138.5000 - val_loss: 2855175.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2919665.0000 - val_loss: 3393803.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1982082.0000 - val_loss: 2821156.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2250619.0000 - val_loss: 2139601.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2304724.7500 - val_loss: 2039915.7500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2094521.7500 - val_loss: 2356259.5000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1852642.5000 - val_loss: 3959252.2500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2167518.2500 - val_loss: 1685895.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1613848.2500 - val_loss: 1879563.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1624128.7500 - val_loss: 1656732.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1401876.3750 - val_loss: 1604737.3750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2039525.5000 - val_loss: 1725656.3750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1349198.6250 - val_loss: 1731258.8750\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1520001.8750 - val_loss: 1846681.3750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1361501.2500 - val_loss: 1321448.3750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1218109.7500 - val_loss: 1518978.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1208494.0000 - val_loss: 1246185.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1290584.1250 - val_loss: 1723335.6250\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1320418.3750 - val_loss: 1526029.6250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1006417.6875 - val_loss: 1492596.1250\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1430122.0000 - val_loss: 1259503.3750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1455439.6250 - val_loss: 1238089.6250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 991979.4375 - val_loss: 946114.7500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1133437.3750 - val_loss: 1098468.8750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1033618.8750 - val_loss: 1136040.3750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1148060.8750 - val_loss: 1588966.6250\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1160342.0000 - val_loss: 1081137.7500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1244350.2500 - val_loss: 976356.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 968941.3125 - val_loss: 998902.7500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1228234.2500 - val_loss: 1208439.3750\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1163198.8750 - val_loss: 968623.4375\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 957162.4375 - val_loss: 786306.0000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 867013.7500 - val_loss: 792883.2500\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 729039.5000 - val_loss: 952343.5000\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 666360.1250 - val_loss: 1330200.8750\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 876405.2500 - val_loss: 867628.3125\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 635834.0625 - val_loss: 799105.6250\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 758685.5625 - val_loss: 786936.6875\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 958244.0000 - val_loss: 643491.4375\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 744645.3125 - val_loss: 914210.8125\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 743527.3750 - val_loss: 844577.0625\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 686117.8125 - val_loss: 911674.1250\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 553425.2500 - val_loss: 708515.0000\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 688316.1250 - val_loss: 645463.5000\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 855967.6250 - val_loss: 685345.6875\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 708574.4375 - val_loss: 798666.8750\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 660634.8125 - val_loss: 774265.5000\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 617375.5000 - val_loss: 697257.0625\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 662565.5625 - val_loss: 641819.8750\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 521136.2188 - val_loss: 647113.7500\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 662951.7500 - val_loss: 763515.1875\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 636950.6875 - val_loss: 663700.5000\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 583646.2500 - val_loss: 542660.3125\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 579553.8750 - val_loss: 865729.6250\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 708803.6875 - val_loss: 771927.7500\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 542329.1250 - val_loss: 609569.1875\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 458656.1250 - val_loss: 411504.6562\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 441636.8750 - val_loss: 505199.7812\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 634248.9375 - val_loss: 423091.0312\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 481369.6250 - val_loss: 818694.1250\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 479000.9375 - val_loss: 1032573.0000\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 445691.7500 - val_loss: 682240.1875\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 367133.4062 - val_loss: 532725.8125\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 424194.1875 - val_loss: 630460.3125\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 366711.6875 - val_loss: 373778.0312\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 352315.4062 - val_loss: 524013.6875\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 334537.1875 - val_loss: 244257.0781\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 275397.5000 - val_loss: 385651.3750\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 203240.3750 - val_loss: 193990.8594\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 250984.6875 - val_loss: 236121.8438\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 211207.6562 - val_loss: 256049.1562\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 188749.1562 - val_loss: 203469.7031\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 220842.1094 - val_loss: 187039.9531\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 160470.7656 - val_loss: 208812.0625\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 282579.3750 - val_loss: 260236.0781\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 181607.5781 - val_loss: 148879.9531\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 178343.9375 - val_loss: 344754.1562\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 223086.9219 - val_loss: 166113.6250\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 153542.7812 - val_loss: 164143.4062\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 147305.4062 - val_loss: 192603.2344\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133005.7969 - val_loss: 206199.3594\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 136364.2812 - val_loss: 280106.6562\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134387.2500 - val_loss: 188037.7188\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 117430.1641 - val_loss: 388895.0938\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 254855.8594 - val_loss: 213634.7656\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 150114.3594 - val_loss: 296262.2812\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 11ms/step - loss: 127689712.0000 - val_loss: 51754332.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 38647564.0000 - val_loss: 32592858.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 25831222.0000 - val_loss: 19522538.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 17511624.0000 - val_loss: 14973041.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 15748820.0000 - val_loss: 15402831.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 12438885.0000 - val_loss: 11763446.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 9708082.0000 - val_loss: 9668302.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8762197.0000 - val_loss: 9196138.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7863427.5000 - val_loss: 8952704.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7387661.0000 - val_loss: 8344200.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6282610.0000 - val_loss: 6430492.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5356921.5000 - val_loss: 8274898.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5760746.5000 - val_loss: 5523927.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 4709676.0000 - val_loss: 5682933.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4207810.0000 - val_loss: 4888465.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3944169.7500 - val_loss: 4895662.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4086947.0000 - val_loss: 3990844.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3533392.7500 - val_loss: 3748644.2500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3578656.7500 - val_loss: 3491838.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 3249209.7500 - val_loss: 3104331.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3016923.0000 - val_loss: 3140720.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3059978.7500 - val_loss: 5096897.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2954152.2500 - val_loss: 3035243.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2800641.0000 - val_loss: 2778900.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2621304.7500 - val_loss: 2422901.0000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2544849.5000 - val_loss: 3858495.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2232856.0000 - val_loss: 2632548.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2097524.2500 - val_loss: 2377238.0000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2010327.1250 - val_loss: 2135847.5000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2307883.2500 - val_loss: 2952777.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2250360.2500 - val_loss: 2803772.7500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1872102.8750 - val_loss: 3002244.0000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3466300.2500 - val_loss: 2348491.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1995481.7500 - val_loss: 1779335.8750\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1950841.5000 - val_loss: 1907288.5000\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1596575.5000 - val_loss: 1598162.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1435414.3750 - val_loss: 1428456.6250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1273583.3750 - val_loss: 1399345.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1521012.0000 - val_loss: 4607233.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2090610.6250 - val_loss: 1729460.2500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1535208.7500 - val_loss: 1400408.0000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1728226.0000 - val_loss: 1319944.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1395273.1250 - val_loss: 1398904.7500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1216094.2500 - val_loss: 1532546.8750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1571477.6250 - val_loss: 1473877.3750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1149682.6250 - val_loss: 1943919.6250\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1525022.5000 - val_loss: 1887407.5000\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1863915.6250 - val_loss: 1597047.0000\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1375629.2500 - val_loss: 1508894.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1964503.3750 - val_loss: 2523723.7500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1766226.1250 - val_loss: 1333521.0000\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1023798.4375 - val_loss: 1074126.5000\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1616844.1250 - val_loss: 1600640.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 992867.7500 - val_loss: 822107.0625\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 843620.3125 - val_loss: 1101995.7500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 974144.5625 - val_loss: 943521.1250\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 812779.2500 - val_loss: 2146624.0000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1116537.5000 - val_loss: 916427.0000\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1247226.3750 - val_loss: 1051193.7500\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1297626.0000 - val_loss: 1711988.2500\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 993414.7500 - val_loss: 841086.8125\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 670654.6875 - val_loss: 783562.1250\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 809645.7500 - val_loss: 749039.1875\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1124512.5000 - val_loss: 1739173.1250\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1122481.5000 - val_loss: 925523.9375\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 846938.8125 - val_loss: 872246.5000\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 874943.1875 - val_loss: 1189130.6250\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 936171.1250 - val_loss: 909693.8750\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 897226.0625 - val_loss: 845138.9375\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 742505.0625 - val_loss: 1484056.5000\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1160707.6250 - val_loss: 777087.4375\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 853099.1875 - val_loss: 755834.3125\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 678863.0000 - val_loss: 1041967.0000\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 125739432.0000 - val_loss: 39932940.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 34267956.0000 - val_loss: 28360702.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 23349928.0000 - val_loss: 21224622.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16556476.0000 - val_loss: 16450578.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 14412823.0000 - val_loss: 12757165.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 11402180.0000 - val_loss: 13400685.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 8897029.0000 - val_loss: 10632755.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 7970620.5000 - val_loss: 8971833.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6608679.5000 - val_loss: 6953037.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7365433.5000 - val_loss: 8302667.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5736082.5000 - val_loss: 5639766.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5253718.0000 - val_loss: 7905873.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4929968.5000 - val_loss: 4244269.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3840933.0000 - val_loss: 4438009.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3851502.7500 - val_loss: 4052297.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3795673.2500 - val_loss: 4058745.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3599995.0000 - val_loss: 6322183.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3324222.7500 - val_loss: 2939696.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2834087.0000 - val_loss: 3265966.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2754726.5000 - val_loss: 3012313.2500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2318708.2500 - val_loss: 3062519.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2826479.7500 - val_loss: 2721848.2500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2417662.0000 - val_loss: 2246047.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2263921.7500 - val_loss: 2048604.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1831398.2500 - val_loss: 1732679.1250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1672508.2500 - val_loss: 2169469.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1917098.2500 - val_loss: 2087400.6250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1788026.8750 - val_loss: 2082315.7500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1706135.2500 - val_loss: 1861354.6250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1421677.1250 - val_loss: 2361653.7500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1423561.7500 - val_loss: 1387796.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1584300.6250 - val_loss: 2582882.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1468824.6250 - val_loss: 1879965.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1479757.7500 - val_loss: 1438460.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1158749.2500 - val_loss: 1403878.1250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1203029.5000 - val_loss: 2440185.2500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1429516.2500 - val_loss: 2006779.1250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1184995.5000 - val_loss: 1267718.0000\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1241864.2500 - val_loss: 2951693.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1283892.8750 - val_loss: 1405352.7500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1427179.8750 - val_loss: 2004704.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1222360.2500 - val_loss: 984030.4375\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1178200.2500 - val_loss: 1309907.1250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 975327.8750 - val_loss: 867953.8750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 936454.6875 - val_loss: 1097114.5000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 932855.7500 - val_loss: 865501.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 886336.7500 - val_loss: 1480181.8750\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1194879.8750 - val_loss: 1022814.4375\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1012980.0000 - val_loss: 980016.5625\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1025387.6875 - val_loss: 993581.2500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 795952.6875 - val_loss: 996551.8125\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1092681.0000 - val_loss: 1077692.3750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 684684.7500 - val_loss: 635043.1875\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 784330.3750 - val_loss: 882326.7500\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 623806.2500 - val_loss: 914728.8125\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 670665.0000 - val_loss: 636072.0625\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 652328.7500 - val_loss: 682581.1250\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 877207.5000 - val_loss: 812726.7500\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 697306.1875 - val_loss: 1169896.3750\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 545587.1250 - val_loss: 463482.7188\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 513545.0000 - val_loss: 519355.3750\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 469573.5625 - val_loss: 383460.9375\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 408649.3438 - val_loss: 382360.3125\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 451767.1875 - val_loss: 488747.9688\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 355489.2188 - val_loss: 449105.1562\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 273171.3750 - val_loss: 305359.3750\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 362137.1875 - val_loss: 425600.7500\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 415845.1875 - val_loss: 438406.2188\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 284368.8438 - val_loss: 268015.9688\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 318162.0625 - val_loss: 203142.3281\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 307827.9062 - val_loss: 234354.0938\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238363.3281 - val_loss: 226151.1562\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 282109.1875 - val_loss: 357209.9688\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 231128.6250 - val_loss: 169396.3750\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 217303.3125 - val_loss: 281425.2812\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 250296.1406 - val_loss: 312136.4375\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 203186.0625 - val_loss: 432123.0312\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 228797.7344 - val_loss: 265168.7188\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 229983.0312 - val_loss: 263054.5938\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 197781.9688 - val_loss: 287148.1250\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 174522.4219 - val_loss: 164711.2031\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138146.1875 - val_loss: 279621.2812\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 203773.6875 - val_loss: 87268.3516\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 155411.6406 - val_loss: 198985.8906\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138794.5469 - val_loss: 145013.7188\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 171052.5625 - val_loss: 114787.3750\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134715.3125 - val_loss: 220163.8125\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 185374.8906 - val_loss: 108878.6094\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 146461.4531 - val_loss: 108941.3516\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 165056.2031 - val_loss: 231519.7500\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 145343.2500 - val_loss: 231406.5781\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 196586.9219 - val_loss: 50806.6328\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 316194.3125 - val_loss: 231321.7031\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 252655.4375 - val_loss: 78649.8281\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 106941.6875 - val_loss: 95142.0000\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 181038.7812 - val_loss: 32763.6973\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 97429.6484 - val_loss: 280212.5312\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 84271.7031 - val_loss: 119809.3672\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 133135.1094 - val_loss: 100066.1562\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131178.8281 - val_loss: 208467.4531\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135026.8750 - val_loss: 147478.5312\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 105733.5938 - val_loss: 105236.5000\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 104696.3594 - val_loss: 69666.4844\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 87277.7344 - val_loss: 113966.6484\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 155034.3906 - val_loss: 63295.4141\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 101895.2109 - val_loss: 82944.1406\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 116196512.0000 - val_loss: 45163776.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 37034116.0000 - val_loss: 29141712.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 24452792.0000 - val_loss: 21930912.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 17967924.0000 - val_loss: 18170222.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 15258837.0000 - val_loss: 15074000.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13034596.0000 - val_loss: 13243809.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10018573.0000 - val_loss: 8508915.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8247266.0000 - val_loss: 8329719.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7208966.0000 - val_loss: 7464540.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6503347.0000 - val_loss: 7700711.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5587546.0000 - val_loss: 7440265.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5498424.5000 - val_loss: 6116990.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 5149933.0000 - val_loss: 5098200.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4836983.5000 - val_loss: 4710880.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3977428.5000 - val_loss: 5955312.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3380183.2500 - val_loss: 3871794.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3677826.7500 - val_loss: 4615554.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3168885.2500 - val_loss: 3692338.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3207202.7500 - val_loss: 4736712.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2841076.2500 - val_loss: 2624008.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3056047.2500 - val_loss: 2439557.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2610775.7500 - val_loss: 2737553.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2272409.2500 - val_loss: 2718946.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1979092.2500 - val_loss: 2164208.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2139031.7500 - val_loss: 2652374.5000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2051618.6250 - val_loss: 2184349.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2120505.7500 - val_loss: 1886427.6250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1986356.6250 - val_loss: 2141596.5000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2154182.0000 - val_loss: 2301321.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1644449.3750 - val_loss: 1977951.3750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1910030.6250 - val_loss: 1746321.3750\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1529107.8750 - val_loss: 1894571.5000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1545122.5000 - val_loss: 1827804.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1312931.5000 - val_loss: 1809364.6250\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1415704.0000 - val_loss: 1612043.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1257234.7500 - val_loss: 1396365.6250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1425825.5000 - val_loss: 1485698.0000\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1569776.6250 - val_loss: 2138262.5000\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1378777.2500 - val_loss: 1594925.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1690768.0000 - val_loss: 1986458.7500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1740508.2500 - val_loss: 2099114.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1282137.8750 - val_loss: 1974450.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1294200.7500 - val_loss: 1171990.8750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1376927.5000 - val_loss: 1364801.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 968131.5625 - val_loss: 1209384.0000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1204126.0000 - val_loss: 1224225.3750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1215674.3750 - val_loss: 1082033.3750\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1161212.0000 - val_loss: 1347699.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1150576.5000 - val_loss: 1200769.3750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1261937.7500 - val_loss: 1210195.8750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1124365.1250 - val_loss: 979050.9375\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1158453.1250 - val_loss: 1311657.7500\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 946642.7500 - val_loss: 1353375.3750\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 887169.9375 - val_loss: 1415541.0000\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 940107.4375 - val_loss: 892081.1875\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 951434.2500 - val_loss: 1828923.5000\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 920325.4375 - val_loss: 757467.5000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 797427.0000 - val_loss: 1158599.3750\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1506216.5000 - val_loss: 973468.2500\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1024614.6875 - val_loss: 857630.8125\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1077251.1250 - val_loss: 1011784.1875\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1000442.5625 - val_loss: 814665.3125\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 761404.6875 - val_loss: 798758.5000\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 886376.2500 - val_loss: 1529658.8750\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1081732.0000 - val_loss: 845750.3750\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 820269.2500 - val_loss: 1517641.6250\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 766097.5000 - val_loss: 650143.3125\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 578635.6250 - val_loss: 574426.5625\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 732835.9375 - val_loss: 918760.1250\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 751987.2500 - val_loss: 769546.0000\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 908710.0000 - val_loss: 708588.0000\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 618983.9375 - val_loss: 700715.3750\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 490097.2812 - val_loss: 562498.5625\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 613898.7500 - val_loss: 728710.1250\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 685496.5000 - val_loss: 660604.9375\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1012529.1250 - val_loss: 1866911.1250\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 860353.1250 - val_loss: 726765.6250\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 619775.8750 - val_loss: 625321.4375\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 534337.4375 - val_loss: 527768.0625\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 607986.6250 - val_loss: 1101088.5000\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 676166.5625 - val_loss: 632106.1875\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 637071.9375 - val_loss: 613019.2500\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 455436.2188 - val_loss: 507436.1875\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 549287.8125 - val_loss: 579309.8125\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 448003.6875 - val_loss: 568458.1250\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 552526.7500 - val_loss: 624359.6250\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 510545.8750 - val_loss: 555514.4375\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 437109.1250 - val_loss: 528255.2500\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 506704.4375 - val_loss: 511891.8125\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 413040.1250 - val_loss: 554021.0000\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 508493.6562 - val_loss: 586443.1875\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 628343.5625 - val_loss: 882664.1250\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 521091.7188 - val_loss: 466523.0312\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 427135.1250 - val_loss: 543650.6875\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 402994.6875 - val_loss: 479606.4062\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 455608.5000 - val_loss: 412048.5312\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 511434.6250 - val_loss: 396049.9375\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 396909.8438 - val_loss: 489328.6562\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 531200.8125 - val_loss: 378125.4375\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 397030.1875 - val_loss: 393029.4688\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 403289.8750 - val_loss: 289707.9375\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 301378.3438 - val_loss: 384473.6875\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 282634.5938 - val_loss: 294816.1250\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 323976.5000 - val_loss: 201811.3281\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 285459.7188 - val_loss: 255284.8281\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 253580.7812 - val_loss: 177041.8281\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 377851.3125 - val_loss: 208002.1250\n",
      "Epoch 108/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 197915.8594 - val_loss: 187194.6250\n",
      "Epoch 109/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 224103.1719 - val_loss: 145646.1562\n",
      "Epoch 110/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 193005.4375 - val_loss: 312456.9375\n",
      "Epoch 111/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 169302.6719 - val_loss: 123256.6641\n",
      "Epoch 112/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 207695.0000 - val_loss: 206019.8281\n",
      "Epoch 113/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 195759.3125 - val_loss: 98893.5703\n",
      "Epoch 114/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 140930.8125 - val_loss: 134842.0938\n",
      "Epoch 115/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 183829.6250 - val_loss: 107456.0078\n",
      "Epoch 116/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136779.1406 - val_loss: 80762.1328\n",
      "Epoch 117/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 152419.8125 - val_loss: 135753.5938\n",
      "Epoch 118/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 186804.7500 - val_loss: 144411.0156\n",
      "Epoch 119/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 115964.5859 - val_loss: 91359.3594\n",
      "Epoch 120/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 107497.4688 - val_loss: 68934.4531\n",
      "Epoch 121/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 144687.8438 - val_loss: 235211.6406\n",
      "Epoch 122/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 146031.0000 - val_loss: 209840.5781\n",
      "Epoch 123/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135162.9375 - val_loss: 118337.4219\n",
      "Epoch 124/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 139943.9375 - val_loss: 212670.6094\n",
      "Epoch 125/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 143360.0469 - val_loss: 89129.2578\n",
      "Epoch 126/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 139550.4375 - val_loss: 76387.5703\n",
      "Epoch 127/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 169535.7656 - val_loss: 182193.1094\n",
      "Epoch 128/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 79549.0312 - val_loss: 56154.7812\n",
      "Epoch 129/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 90185.5781 - val_loss: 170942.0781\n",
      "Epoch 130/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 84799.3047 - val_loss: 129136.5078\n",
      "Epoch 131/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117845.5938 - val_loss: 88282.4844\n",
      "Epoch 132/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 61801.1055 - val_loss: 131276.8594\n",
      "Epoch 133/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 73525.0000 - val_loss: 184398.2031\n",
      "Epoch 134/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 101243.3984 - val_loss: 74109.0938\n",
      "Epoch 135/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 62771.7852 - val_loss: 90150.6641\n",
      "Epoch 136/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 95250.4766 - val_loss: 40793.6719\n",
      "Epoch 137/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 77281.2734 - val_loss: 264937.3750\n",
      "Epoch 138/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 103022.3906 - val_loss: 60578.6406\n",
      "Epoch 139/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 79801.1875 - val_loss: 112271.2109\n",
      "Epoch 140/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 64719.0469 - val_loss: 47892.5039\n",
      "Epoch 141/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 87464.6797 - val_loss: 49430.8672\n",
      "Epoch 142/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 50817.4258 - val_loss: 36103.0898\n",
      "Epoch 143/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 81056.1562 - val_loss: 181906.3594\n",
      "Epoch 144/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 120663.5391 - val_loss: 59998.1641\n",
      "Epoch 145/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 65175.3750 - val_loss: 53944.8320\n",
      "Epoch 146/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 42103.5234 - val_loss: 31738.5898\n",
      "Epoch 147/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 69444.5391 - val_loss: 59218.8672\n",
      "Epoch 148/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 64442.4648 - val_loss: 129723.7109\n",
      "Epoch 149/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 51444.9609 - val_loss: 29578.8418\n",
      "Epoch 150/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 62403.7930 - val_loss: 49298.6562\n",
      "Epoch 151/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 53044.2070 - val_loss: 30181.6582\n",
      "Epoch 152/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 46520.7148 - val_loss: 17163.6367\n",
      "Epoch 153/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 74625.1875 - val_loss: 58584.2266\n",
      "Epoch 154/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 40792.2656 - val_loss: 119272.2891\n",
      "Epoch 155/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 52817.9922 - val_loss: 22932.0098\n",
      "Epoch 156/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 44987.3242 - val_loss: 64723.9648\n",
      "Epoch 157/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 43761.9102 - val_loss: 64181.9648\n",
      "Epoch 158/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 48227.5273 - val_loss: 47458.2969\n",
      "Epoch 159/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 36711.6289 - val_loss: 77925.0391\n",
      "Epoch 160/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 37328.8438 - val_loss: 29344.8027\n",
      "Epoch 161/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 50694.3398 - val_loss: 51054.1250\n",
      "Epoch 162/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 47907.2422 - val_loss: 24503.2734\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 129815312.0000 - val_loss: 42302940.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 35939376.0000 - val_loss: 28085858.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 22674200.0000 - val_loss: 21028298.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 18204298.0000 - val_loss: 20593948.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 14779192.0000 - val_loss: 12817335.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 10699430.0000 - val_loss: 12757837.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8962878.0000 - val_loss: 9736160.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7522749.5000 - val_loss: 8112986.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6556391.0000 - val_loss: 9528935.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6355661.0000 - val_loss: 5985512.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5209293.5000 - val_loss: 5592036.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4803201.5000 - val_loss: 4436634.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4073474.2500 - val_loss: 4794315.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3763154.7500 - val_loss: 3957970.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3501258.0000 - val_loss: 4276417.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2950877.2500 - val_loss: 4908563.0000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2795099.2500 - val_loss: 2690579.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2481104.2500 - val_loss: 2636521.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2591417.2500 - val_loss: 2259449.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2108579.2500 - val_loss: 2064685.8750\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1901582.8750 - val_loss: 2347749.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1848382.3750 - val_loss: 1773645.6250\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1434814.1250 - val_loss: 1651665.3750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1930816.5000 - val_loss: 1554333.0000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1242434.5000 - val_loss: 1223480.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1033258.0000 - val_loss: 1615084.7500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1252108.7500 - val_loss: 1235858.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 968303.0000 - val_loss: 1492237.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 834887.2500 - val_loss: 907703.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 753451.1250 - val_loss: 1014728.9375\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 811636.1875 - val_loss: 1059277.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 848887.6250 - val_loss: 980816.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 668281.8125 - val_loss: 707784.5625\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 502655.1250 - val_loss: 601348.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 496375.8750 - val_loss: 1115326.5000\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 581701.3125 - val_loss: 611889.1875\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 492847.2812 - val_loss: 625540.5000\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 375838.0312 - val_loss: 776131.1250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 500144.5000 - val_loss: 508818.5312\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 373415.8750 - val_loss: 416408.1562\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 254483.8906 - val_loss: 470272.1562\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 317004.1250 - val_loss: 441717.2500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 315483.6562 - val_loss: 678987.2500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 340190.1250 - val_loss: 242575.6094\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 275158.7500 - val_loss: 451354.0625\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 313116.5312 - val_loss: 315118.8750\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 357209.0312 - val_loss: 349482.7188\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 210531.7031 - val_loss: 235607.0312\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 239900.7188 - val_loss: 413218.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 210050.3438 - val_loss: 261053.0156\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 201072.3438 - val_loss: 280865.4062\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 191306.4375 - val_loss: 292340.8750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 219024.0781 - val_loss: 516048.3438\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 233462.1094 - val_loss: 230578.6719\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 177188.1719 - val_loss: 194067.3125\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 163402.7188 - val_loss: 306788.0312\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 215658.9062 - val_loss: 139566.5312\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 196981.3750 - val_loss: 158519.8125\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 344125.7812 - val_loss: 213966.7969\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 298033.1562 - val_loss: 629983.6875\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 280551.4062 - val_loss: 188621.1250\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 198175.8281 - val_loss: 166747.9062\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 344102.4688 - val_loss: 665668.1875\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 281844.7500 - val_loss: 245795.3594\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138874.0625 - val_loss: 116563.6016\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 162384.3906 - val_loss: 173737.2188\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 175571.2656 - val_loss: 286963.8438\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 173192.1719 - val_loss: 408870.4375\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 156072.3125 - val_loss: 182498.7344\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 236620.2812 - val_loss: 198696.5625\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 151179.0625 - val_loss: 332239.8750\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 205625.2969 - val_loss: 354425.9062\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 244351.6406 - val_loss: 167520.0000\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 144369.5000 - val_loss: 62584.4219\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 156590.0469 - val_loss: 354027.5000\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 265291.8125 - val_loss: 140212.3750\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 97085.6797 - val_loss: 193190.6562\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130640.8906 - val_loss: 86737.5000\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 115886.5547 - val_loss: 136569.6094\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 119498.1250 - val_loss: 58785.9102\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 187912.0312 - val_loss: 101951.8828\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117640.4844 - val_loss: 130897.2031\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 148656.0000 - val_loss: 254766.1250\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 98982.4062 - val_loss: 133989.6875\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 137259.7656 - val_loss: 159966.8438\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123782.6250 - val_loss: 283919.7188\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 164118.2656 - val_loss: 228718.1719\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 103100.0625 - val_loss: 80361.4766\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 93701.6797 - val_loss: 148763.7656\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122273.9844 - val_loss: 210371.9844\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 10ms/step - loss: 138734304.0000 - val_loss: 49972788.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 36591756.0000 - val_loss: 32871756.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 22965762.0000 - val_loss: 21270862.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 16596268.0000 - val_loss: 15153952.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 13547579.0000 - val_loss: 12645000.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 12205047.0000 - val_loss: 10720197.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 9807814.0000 - val_loss: 8574356.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 8144280.0000 - val_loss: 8577232.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7126454.5000 - val_loss: 9579060.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 6950186.0000 - val_loss: 6919337.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5463302.0000 - val_loss: 5701805.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5276382.0000 - val_loss: 5136548.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4386582.0000 - val_loss: 5072145.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4980743.0000 - val_loss: 6030250.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5177965.5000 - val_loss: 4103082.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4038718.7500 - val_loss: 3471379.7500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3921006.2500 - val_loss: 4245325.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3421922.2500 - val_loss: 3448235.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3307070.0000 - val_loss: 2975833.5000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2722278.7500 - val_loss: 2982129.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2976990.2500 - val_loss: 3417658.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2432469.5000 - val_loss: 2762446.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2772543.7500 - val_loss: 3232615.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2507115.0000 - val_loss: 2278697.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2702853.5000 - val_loss: 2309203.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2371919.2500 - val_loss: 2004985.3750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2090548.2500 - val_loss: 3499935.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 2525420.5000 - val_loss: 2130144.7500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1828277.5000 - val_loss: 2315383.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2375700.5000 - val_loss: 2060687.2500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2034347.3750 - val_loss: 3108297.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1902124.5000 - val_loss: 1623074.3750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2143759.2500 - val_loss: 2343434.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2129063.5000 - val_loss: 2325923.2500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2333120.0000 - val_loss: 1483732.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1878481.3750 - val_loss: 2125527.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1717250.7500 - val_loss: 2041817.6250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2039904.5000 - val_loss: 1431357.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1839767.1250 - val_loss: 1885175.6250\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1413829.6250 - val_loss: 1351409.3750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1382532.8750 - val_loss: 2166619.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1516938.6250 - val_loss: 3564583.2500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1602232.5000 - val_loss: 1052278.3750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1541501.8750 - val_loss: 1487808.8750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1196704.0000 - val_loss: 1217265.8750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1102350.5000 - val_loss: 1796080.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1353809.2500 - val_loss: 1163299.5000\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1093534.5000 - val_loss: 1013582.4375\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 918393.8750 - val_loss: 1210216.0000\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1063989.5000 - val_loss: 1124845.0000\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1308842.5000 - val_loss: 911344.2500\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 880790.0000 - val_loss: 851460.7500\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1144420.2500 - val_loss: 814780.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1048187.8750 - val_loss: 867364.3125\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 821929.7500 - val_loss: 906234.0625\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 888794.8125 - val_loss: 832814.7500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 857687.5625 - val_loss: 866720.1875\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 808848.1250 - val_loss: 825302.0000\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1129791.3750 - val_loss: 1028464.5625\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1085296.6250 - val_loss: 1966846.6250\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1253946.6250 - val_loss: 939014.6250\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1057062.6250 - val_loss: 1027942.9375\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 932076.1250 - val_loss: 831373.5625\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 107861072.0000 - val_loss: 46878356.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 33897344.0000 - val_loss: 36966308.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 24263512.0000 - val_loss: 24825888.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 17905638.0000 - val_loss: 12904879.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12011878.0000 - val_loss: 11488709.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 10492982.0000 - val_loss: 12326209.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 8795662.0000 - val_loss: 9603705.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 7238882.5000 - val_loss: 7633037.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 7451946.0000 - val_loss: 7321466.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5610447.0000 - val_loss: 7163872.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5028919.0000 - val_loss: 5932244.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 4848551.5000 - val_loss: 4139916.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4018512.0000 - val_loss: 4320838.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4030993.7500 - val_loss: 6724155.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3602325.2500 - val_loss: 3160570.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3068610.7500 - val_loss: 3366113.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3013515.5000 - val_loss: 3100151.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2627602.7500 - val_loss: 4458979.0000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2373487.2500 - val_loss: 3175759.7500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 2579180.7500 - val_loss: 2503959.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2081911.5000 - val_loss: 3154332.5000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2112352.7500 - val_loss: 2111905.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2335736.0000 - val_loss: 1986969.6250\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2578591.0000 - val_loss: 3079675.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2080833.3750 - val_loss: 1610416.5000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1788546.0000 - val_loss: 1830090.8750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1969695.3750 - val_loss: 4180484.7500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2107951.0000 - val_loss: 1626470.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1456587.6250 - val_loss: 1563810.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1617969.3750 - val_loss: 1496177.7500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1651813.7500 - val_loss: 1295859.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1553811.7500 - val_loss: 1528999.7500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1535359.5000 - val_loss: 2615741.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1751047.7500 - val_loss: 2059576.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 1483115.7500 - val_loss: 2522423.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 2132376.0000 - val_loss: 1610572.5000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1480471.0000 - val_loss: 1698502.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1194830.2500 - val_loss: 1267520.1250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1224230.1250 - val_loss: 1160332.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1257057.5000 - val_loss: 1650848.5000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1275950.8750 - val_loss: 1645001.2500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1118435.6250 - val_loss: 939398.1875\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 994223.8750 - val_loss: 1025420.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1063406.2500 - val_loss: 1446093.2500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1272570.5000 - val_loss: 792251.0625\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 873004.2500 - val_loss: 921219.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 706082.2500 - val_loss: 1033340.0000\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 815079.0625 - val_loss: 1035786.6875\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 836181.7500 - val_loss: 2129364.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1119569.0000 - val_loss: 687395.7500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1363233.3750 - val_loss: 785456.5625\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1091609.2500 - val_loss: 1496355.3750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 879052.3750 - val_loss: 847291.0000\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 799167.2500 - val_loss: 678915.8125\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 945288.8750 - val_loss: 2894602.7500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 772926.6875 - val_loss: 623377.9375\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 802171.2500 - val_loss: 919820.0000\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 597548.4375 - val_loss: 1197071.2500\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 550672.2500 - val_loss: 857469.8750\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 675675.4375 - val_loss: 636633.0000\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 617370.3125 - val_loss: 538543.9375\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 669932.5000 - val_loss: 1060625.3750\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 636529.0625 - val_loss: 836446.1250\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 502355.8750 - val_loss: 743851.0000\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 390885.4688 - val_loss: 645496.0000\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 351319.0000 - val_loss: 473269.9688\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 416553.2812 - val_loss: 306770.0625\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 275233.6562 - val_loss: 516733.1250\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 325117.2500 - val_loss: 400244.3750\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 275778.3125 - val_loss: 484752.3438\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 293504.1250 - val_loss: 260055.8281\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 307911.9375 - val_loss: 539340.6875\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 290422.5625 - val_loss: 233299.7500\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 316203.8750 - val_loss: 228365.2031\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 236849.8594 - val_loss: 176400.5938\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 196377.5469 - val_loss: 389065.3750\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 220556.8594 - val_loss: 274830.0000\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 174230.7344 - val_loss: 140787.6094\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 243368.4219 - val_loss: 233285.9062\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 192278.2031 - val_loss: 186565.2031\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 147114.5625 - val_loss: 128330.7344\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 253428.7500 - val_loss: 350563.3125\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 162232.7500 - val_loss: 341925.0938\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 251768.8281 - val_loss: 122628.4531\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 184734.8125 - val_loss: 279334.6875\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 266819.0000 - val_loss: 288898.6250\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 234850.3906 - val_loss: 113326.1562\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 197746.8125 - val_loss: 271147.8125\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 192424.8594 - val_loss: 122570.5781\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 158350.1562 - val_loss: 84100.3281\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 185854.8281 - val_loss: 371773.6250\n",
      "Epoch 92/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129818.7812 - val_loss: 234334.6250\n",
      "Epoch 93/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 232038.6875 - val_loss: 408447.5625\n",
      "Epoch 94/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 186098.7031 - val_loss: 80960.6875\n",
      "Epoch 95/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 175824.0156 - val_loss: 272602.8125\n",
      "Epoch 96/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 189251.9375 - val_loss: 96237.8047\n",
      "Epoch 97/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 154320.9844 - val_loss: 200517.6406\n",
      "Epoch 98/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137439.4062 - val_loss: 340754.7188\n",
      "Epoch 99/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 173299.6719 - val_loss: 99437.6172\n",
      "Epoch 100/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 144763.9062 - val_loss: 75490.5000\n",
      "Epoch 101/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 163427.4531 - val_loss: 79390.8594\n",
      "Epoch 102/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 148718.5156 - val_loss: 50700.6250\n",
      "Epoch 103/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 139011.3125 - val_loss: 290301.0625\n",
      "Epoch 104/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 178698.0469 - val_loss: 142503.5312\n",
      "Epoch 105/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134828.5312 - val_loss: 57602.6094\n",
      "Epoch 106/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 90400.2188 - val_loss: 141124.0312\n",
      "Epoch 107/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130500.4688 - val_loss: 266672.5000\n",
      "Epoch 108/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 99764.1328 - val_loss: 163980.1562\n",
      "Epoch 109/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 182022.8594 - val_loss: 150124.4062\n",
      "Epoch 110/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 116768.2656 - val_loss: 61666.1250\n",
      "Epoch 111/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 102813.4062 - val_loss: 40774.6328\n",
      "Epoch 112/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 176870.8750 - val_loss: 133217.6875\n",
      "Epoch 113/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 112382.7188 - val_loss: 105014.8047\n",
      "Epoch 114/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 81819.9688 - val_loss: 53721.2734\n",
      "Epoch 115/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 105978.2656 - val_loss: 252827.5000\n",
      "Epoch 116/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124074.7344 - val_loss: 150151.7656\n",
      "Epoch 117/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128477.6094 - val_loss: 162113.7656\n",
      "Epoch 118/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132039.3750 - val_loss: 143038.2188\n",
      "Epoch 119/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 63213.2344 - val_loss: 47969.6523\n",
      "Epoch 120/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 71162.9453 - val_loss: 49028.5508\n",
      "Epoch 121/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 96402.4062 - val_loss: 289544.3438\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 129153992.0000 - val_loss: 47764088.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 37155728.0000 - val_loss: 31327208.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 24592684.0000 - val_loss: 21003694.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 18013114.0000 - val_loss: 14716216.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 12836998.0000 - val_loss: 11810315.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 9885392.0000 - val_loss: 9544104.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 8288525.5000 - val_loss: 9739715.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6845314.0000 - val_loss: 7641935.5000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 6173147.5000 - val_loss: 6742073.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5885945.0000 - val_loss: 5990313.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4901310.0000 - val_loss: 5177721.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 4264241.0000 - val_loss: 4618804.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3911701.7500 - val_loss: 4293761.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3772675.0000 - val_loss: 3529130.2500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 3515329.0000 - val_loss: 4621020.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3755764.0000 - val_loss: 3439973.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 3388453.2500 - val_loss: 3256372.0000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2895813.0000 - val_loss: 3597401.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2489148.7500 - val_loss: 3633117.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2191169.0000 - val_loss: 2328296.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2063359.3750 - val_loss: 2459754.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2319410.2500 - val_loss: 3145117.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2966976.0000 - val_loss: 3238675.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 2254754.7500 - val_loss: 4205159.5000\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 2176015.2500 - val_loss: 2380026.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1797459.8750 - val_loss: 2196734.0000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1972983.1250 - val_loss: 1992395.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1525476.6250 - val_loss: 1862652.5000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1608764.0000 - val_loss: 1984897.1250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1393056.5000 - val_loss: 1752444.8750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 1777091.0000 - val_loss: 2296563.7500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1527831.7500 - val_loss: 1625261.3750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1759470.1250 - val_loss: 1821933.2500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1126564.6250 - val_loss: 1247788.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1284388.6250 - val_loss: 2321649.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1511001.2500 - val_loss: 1393289.7500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 1115787.3750 - val_loss: 1205453.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1120265.1250 - val_loss: 1989990.6250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1025802.3125 - val_loss: 1145591.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 927344.5625 - val_loss: 1426844.5000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 846494.0000 - val_loss: 1070306.3750\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1083565.5000 - val_loss: 1223032.0000\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 980491.8750 - val_loss: 1395705.1250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 887052.8125 - val_loss: 1069442.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 815750.4375 - val_loss: 1192038.3750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 818987.0625 - val_loss: 1120790.1250\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 844161.1250 - val_loss: 709943.6875\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 688415.5000 - val_loss: 986638.3750\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 823692.4375 - val_loss: 726859.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 641258.6250 - val_loss: 677304.5000\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 844320.5000 - val_loss: 1303738.6250\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 687548.5625 - val_loss: 606624.9375\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 455595.6562 - val_loss: 730903.1875\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 394241.2188 - val_loss: 461168.1250\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 433108.3750 - val_loss: 706635.2500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 452823.0625 - val_loss: 661225.7500\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 401776.1875 - val_loss: 304125.3125\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 284379.1875 - val_loss: 294477.4062\n",
      "Epoch 59/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 215394.3281 - val_loss: 371393.0938\n",
      "Epoch 60/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 312566.2812 - val_loss: 682634.3750\n",
      "Epoch 61/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 286240.5312 - val_loss: 252878.7031\n",
      "Epoch 62/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 235234.8594 - val_loss: 469817.1562\n",
      "Epoch 63/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 277468.7500 - val_loss: 303618.5625\n",
      "Epoch 64/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 252161.3906 - val_loss: 354053.0625\n",
      "Epoch 65/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 303107.6250 - val_loss: 508732.3438\n",
      "Epoch 66/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 230264.7812 - val_loss: 266906.3125\n",
      "Epoch 67/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 201787.3125 - val_loss: 367337.9062\n",
      "Epoch 68/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 233734.5312 - val_loss: 136021.3906\n",
      "Epoch 69/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 236800.0312 - val_loss: 201807.4688\n",
      "Epoch 70/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 238516.8125 - val_loss: 184438.0000\n",
      "Epoch 71/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 207416.9531 - val_loss: 120648.4844\n",
      "Epoch 72/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 141142.4375 - val_loss: 155201.9062\n",
      "Epoch 73/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 147674.4844 - val_loss: 331772.8438\n",
      "Epoch 74/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 192603.6562 - val_loss: 202907.3594\n",
      "Epoch 75/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 163785.6875 - val_loss: 150114.8750\n",
      "Epoch 76/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 208741.4219 - val_loss: 284949.8125\n",
      "Epoch 77/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 235987.8281 - val_loss: 169811.2344\n",
      "Epoch 78/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 149829.5156 - val_loss: 277988.1250\n",
      "Epoch 79/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 148611.2500 - val_loss: 182222.7031\n",
      "Epoch 80/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 145800.6875 - val_loss: 167065.0469\n",
      "Epoch 81/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 221656.2500 - val_loss: 35602.7500\n",
      "Epoch 82/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 192346.6250 - val_loss: 180056.4531\n",
      "Epoch 83/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 199971.0625 - val_loss: 152141.4062\n",
      "Epoch 84/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 204460.6562 - val_loss: 86431.7656\n",
      "Epoch 85/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131919.5312 - val_loss: 326945.0625\n",
      "Epoch 86/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 135850.4844 - val_loss: 131113.9062\n",
      "Epoch 87/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 152835.9062 - val_loss: 186766.7969\n",
      "Epoch 88/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123595.8203 - val_loss: 471245.6250\n",
      "Epoch 89/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 166758.9688 - val_loss: 286856.1562\n",
      "Epoch 90/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 102902.2188 - val_loss: 86082.1328\n",
      "Epoch 91/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 160786.9531 - val_loss: 328841.7500\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 156.0997 - val_loss: 149.6540\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 139.7321 - val_loss: 143.8895\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 143.5376 - val_loss: 141.5730\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 139.7385 - val_loss: 137.6919\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.3106 - val_loss: 134.9022\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.1813 - val_loss: 135.5766\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131.7040 - val_loss: 133.2770\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.1234 - val_loss: 134.1702\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.7559 - val_loss: 135.7824\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.0201 - val_loss: 133.9850\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.0540 - val_loss: 135.9420\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.8614 - val_loss: 134.7191\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.9641 - val_loss: 130.5133\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.5212 - val_loss: 133.2296\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 129.2916 - val_loss: 131.8125\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.7126 - val_loss: 133.0008\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.8095 - val_loss: 130.3115\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.7867 - val_loss: 133.0744\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.3261 - val_loss: 133.6321\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.8378 - val_loss: 130.2538\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.2190 - val_loss: 129.9596\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 124.9549 - val_loss: 130.5465\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.2982 - val_loss: 131.2207\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.3625 - val_loss: 143.9536\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.5392 - val_loss: 134.0769\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.6297 - val_loss: 131.2703\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.3079 - val_loss: 131.0111\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.7596 - val_loss: 130.3605\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.1316 - val_loss: 131.4190\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.0922 - val_loss: 128.8542\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9505 - val_loss: 129.3281\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.0692 - val_loss: 131.5384\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.9986 - val_loss: 129.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.1625 - val_loss: 129.8836\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.1101 - val_loss: 131.9029\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.4188 - val_loss: 131.0604\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.2635 - val_loss: 129.6299\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9400 - val_loss: 130.5901\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.1722 - val_loss: 128.4368\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.1927 - val_loss: 130.7263\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.9031 - val_loss: 128.5863\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.3105 - val_loss: 129.8893\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.8199 - val_loss: 130.3589\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.9385 - val_loss: 128.8917\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.5072 - val_loss: 129.1222\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.3809 - val_loss: 129.6256\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.4286 - val_loss: 131.0497\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.7851 - val_loss: 128.8840\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.0000 - val_loss: 129.3223\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 152.8302 - val_loss: 148.5136\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.1130 - val_loss: 139.7897\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.4774 - val_loss: 140.5548\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.7300 - val_loss: 136.3578\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.9462 - val_loss: 137.2472\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9877 - val_loss: 134.5183\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.8287 - val_loss: 133.7345\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.8403 - val_loss: 133.5834\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.5108 - val_loss: 133.5041\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.0856 - val_loss: 134.1513\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7132 - val_loss: 130.7714\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.4595 - val_loss: 130.9597\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5701 - val_loss: 133.8306\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.5561 - val_loss: 131.5803\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.5280 - val_loss: 136.0395\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.3630 - val_loss: 133.5667\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.7340 - val_loss: 134.1295\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.3985 - val_loss: 131.6910\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9750 - val_loss: 131.0132\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.2025 - val_loss: 135.8147\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.0252 - val_loss: 130.1003\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3221 - val_loss: 135.9184\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.7861 - val_loss: 131.1469\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9436 - val_loss: 134.5865\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.7479 - val_loss: 131.3451\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2477 - val_loss: 129.8274\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.8082 - val_loss: 131.0724\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7152 - val_loss: 130.9347\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9985 - val_loss: 132.3549\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2081 - val_loss: 129.9721\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.9605 - val_loss: 131.2944\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5189 - val_loss: 130.2452\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8860 - val_loss: 130.9123\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3646 - val_loss: 129.3624\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1487 - val_loss: 129.5073\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4356 - val_loss: 130.8512\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7849 - val_loss: 129.8595\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8802 - val_loss: 128.7252\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7761 - val_loss: 129.7846\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7362 - val_loss: 129.4989\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.7484 - val_loss: 129.7295\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.1610 - val_loss: 130.0466\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.1102 - val_loss: 129.0242\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0984 - val_loss: 130.9892\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.6767 - val_loss: 129.9143\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9426 - val_loss: 128.9302\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.3650 - val_loss: 128.7184\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.0102 - val_loss: 129.4642\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8608 - val_loss: 129.0742\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.2614 - val_loss: 132.3992\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.5724 - val_loss: 129.8224\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8707 - val_loss: 129.1265\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.0747 - val_loss: 129.2682\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7151 - val_loss: 129.6888\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8674 - val_loss: 129.2294\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.8516 - val_loss: 132.0733\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 121.4121 - val_loss: 129.8245\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 167.6325 - val_loss: 166.9792\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 163.5527 - val_loss: 163.0278\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 149.9929 - val_loss: 138.6364\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.3298 - val_loss: 143.3367\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 140.9481 - val_loss: 135.6311\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.1866 - val_loss: 140.5796\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.7298 - val_loss: 137.3175\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0440 - val_loss: 135.1432\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.4858 - val_loss: 132.8947\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.4269 - val_loss: 132.8280\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.8037 - val_loss: 133.0998\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.3668 - val_loss: 132.7290\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.4709 - val_loss: 134.3997\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9709 - val_loss: 138.1964\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.4213 - val_loss: 133.4638\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3607 - val_loss: 131.8307\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.4817 - val_loss: 132.0487\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.7846 - val_loss: 131.9479\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.8298 - val_loss: 132.1316\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.1608 - val_loss: 129.9845\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.5344 - val_loss: 131.2886\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5398 - val_loss: 131.7387\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7135 - val_loss: 130.0852\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0122 - val_loss: 130.8454\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6199 - val_loss: 129.7889\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7718 - val_loss: 129.7392\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2477 - val_loss: 131.1436\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8159 - val_loss: 131.0119\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.6966 - val_loss: 131.0923\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.0995 - val_loss: 132.4253\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6640 - val_loss: 129.0987\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.1633 - val_loss: 130.4924\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5590 - val_loss: 129.1447\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.2581 - val_loss: 130.1665\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3521 - val_loss: 129.5507\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4635 - val_loss: 130.2233\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0726 - val_loss: 130.1880\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.2395 - val_loss: 131.5320\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8646 - val_loss: 132.3396\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9024 - val_loss: 130.4439\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5287 - val_loss: 132.1724\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 149.7057 - val_loss: 149.6758\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.9002 - val_loss: 136.2755\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.2212 - val_loss: 138.2092\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0532 - val_loss: 134.9480\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.0657 - val_loss: 134.4186\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.3545 - val_loss: 138.0670\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.2506 - val_loss: 136.6571\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9878 - val_loss: 133.1129\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.5153 - val_loss: 133.7910\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0991 - val_loss: 133.4420\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.6259 - val_loss: 134.5347\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.6047 - val_loss: 131.1039\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5960 - val_loss: 131.0139\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3338 - val_loss: 133.2301\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3336 - val_loss: 133.5129\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.5418 - val_loss: 131.4164\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.3894 - val_loss: 132.0158\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5329 - val_loss: 130.8845\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.3678 - val_loss: 132.6286\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9563 - val_loss: 130.5390\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.1221 - val_loss: 130.5774\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.4859 - val_loss: 130.4364\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9055 - val_loss: 131.0568\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.7709 - val_loss: 129.5107\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.8428 - val_loss: 130.4539\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2093 - val_loss: 129.2440\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9420 - val_loss: 129.8832\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.1802 - val_loss: 129.3602\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5055 - val_loss: 131.3445\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7198 - val_loss: 129.5016\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4003 - val_loss: 130.9825\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.3704 - val_loss: 129.4503\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6585 - val_loss: 130.0602\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5828 - val_loss: 129.4086\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0707 - val_loss: 130.6140\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2297 - val_loss: 131.4339\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 145.1806 - val_loss: 140.3067\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 136.1662 - val_loss: 137.7207\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.6192 - val_loss: 136.7629\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.0266 - val_loss: 133.5885\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.5061 - val_loss: 136.7273\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.5892 - val_loss: 132.5109\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.6551 - val_loss: 136.7892\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.2525 - val_loss: 133.7820\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 137.8120 - val_loss: 136.2901\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.3166 - val_loss: 132.3335\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.5050 - val_loss: 133.1272\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.4700 - val_loss: 134.2401\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.7874 - val_loss: 134.7062\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.4134 - val_loss: 135.6211\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.5447 - val_loss: 132.3936\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.2653 - val_loss: 134.0735\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3510 - val_loss: 131.6835\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4374 - val_loss: 130.1727\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.4195 - val_loss: 131.6540\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6970 - val_loss: 132.0518\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.4655 - val_loss: 130.7535\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9669 - val_loss: 129.7084\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6266 - val_loss: 131.3234\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9620 - val_loss: 130.5135\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5182 - val_loss: 133.5493\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4304 - val_loss: 130.7156\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.9176 - val_loss: 128.8926\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.0576 - val_loss: 130.0449\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7343 - val_loss: 131.7601\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.6983 - val_loss: 130.4559\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4118 - val_loss: 130.3582\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5168 - val_loss: 128.6838\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5458 - val_loss: 131.7382\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5296 - val_loss: 128.5296\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5641 - val_loss: 130.3893\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6557 - val_loss: 128.9199\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7303 - val_loss: 131.9075\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8159 - val_loss: 128.3943\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.8980 - val_loss: 129.6428\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.4550 - val_loss: 131.9935\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.3468 - val_loss: 128.0215\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7884 - val_loss: 128.7936\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.5311 - val_loss: 130.2304\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.3643 - val_loss: 130.5178\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9246 - val_loss: 130.5216\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0289 - val_loss: 127.9330\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.9330 - val_loss: 128.9193\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.7303 - val_loss: 128.2586\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.9947 - val_loss: 128.7653\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.2816 - val_loss: 129.4289\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.4930 - val_loss: 128.2474\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.0120 - val_loss: 130.4608\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.1065 - val_loss: 129.8571\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 121.5405 - val_loss: 129.5116\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.8369 - val_loss: 127.9921\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 120.6932 - val_loss: 129.2204\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 157.7540 - val_loss: 151.5569\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 152.0285 - val_loss: 147.2833\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 142.4123 - val_loss: 141.8093\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 142.4289 - val_loss: 141.1077\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 142.3106 - val_loss: 141.6040\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 142.3986 - val_loss: 141.7074\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.8686 - val_loss: 141.4132\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.6162 - val_loss: 140.9922\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.8498 - val_loss: 141.5212\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.7927 - val_loss: 142.9130\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 138.8630 - val_loss: 142.5152\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.9129 - val_loss: 135.9475\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 134.6160 - val_loss: 137.0062\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.8726 - val_loss: 136.3890\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.3061 - val_loss: 133.9337\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.5009 - val_loss: 135.9812\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 132.1196 - val_loss: 134.0023\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.0329 - val_loss: 134.5066\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.3179 - val_loss: 134.3698\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.2125 - val_loss: 133.8955\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 130.1202 - val_loss: 133.8739\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7693 - val_loss: 134.2532\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.3401 - val_loss: 130.4073\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9812 - val_loss: 131.2889\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.9644 - val_loss: 130.1211\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.7146 - val_loss: 130.7379\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.2474 - val_loss: 132.8401\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.9874 - val_loss: 132.0768\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.7794 - val_loss: 130.2629\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.7307 - val_loss: 130.3901\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.3374 - val_loss: 130.0077\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0879 - val_loss: 128.3895\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.0725 - val_loss: 129.6030\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6526 - val_loss: 129.5355\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.5517 - val_loss: 133.9908\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.9791 - val_loss: 130.7115\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.4893 - val_loss: 130.3431\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.1853 - val_loss: 131.4115\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.6390 - val_loss: 130.9930\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.8680 - val_loss: 128.4601\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 123.7093 - val_loss: 129.7961\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 124.0935 - val_loss: 130.8340\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 157.6376 - val_loss: 147.8654\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138.9453 - val_loss: 138.2553\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 134.3158 - val_loss: 136.7779\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.2889 - val_loss: 133.2806\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.4712 - val_loss: 135.3592\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 130.5301 - val_loss: 131.6321\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.3276 - val_loss: 134.5500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.2678 - val_loss: 133.2826\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.0303 - val_loss: 133.7484\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.8657 - val_loss: 132.5788\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.4182 - val_loss: 131.0184\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.5202 - val_loss: 134.1366\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.8728 - val_loss: 134.2343\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131.1452 - val_loss: 135.0240\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.3245 - val_loss: 132.2432\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.1775 - val_loss: 133.4780\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 130.5019 - val_loss: 131.3326\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.4277 - val_loss: 131.1977\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.0298 - val_loss: 130.4816\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.7258 - val_loss: 133.3532\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.1042 - val_loss: 130.6470\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.8939 - val_loss: 131.9854\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.8421 - val_loss: 130.7170\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.5993 - val_loss: 131.4137\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.4497 - val_loss: 129.4723\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2008 - val_loss: 129.1073\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.5128 - val_loss: 133.4938\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.8567 - val_loss: 131.6144\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.6090 - val_loss: 130.7381\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.8669 - val_loss: 130.9189\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.7817 - val_loss: 132.8945\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.3021 - val_loss: 129.0993\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.5268 - val_loss: 131.8917\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.3970 - val_loss: 133.4338\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.2115 - val_loss: 127.8908\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.7579 - val_loss: 129.2643\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.1130 - val_loss: 130.5396\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.8254 - val_loss: 129.0685\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 122.1889 - val_loss: 128.5990\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.5546 - val_loss: 129.4376\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 123.5102 - val_loss: 128.7815\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.8787 - val_loss: 130.5831\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.3610 - val_loss: 130.4878\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.8029 - val_loss: 129.2347\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 122.4188 - val_loss: 130.3712\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 13ms/step - loss: 145.0100 - val_loss: 142.3998\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 136.5914 - val_loss: 138.3761\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.7410 - val_loss: 134.8289\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.5814 - val_loss: 136.5886\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 133.5355 - val_loss: 137.2556\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.6546 - val_loss: 135.4536\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132.7577 - val_loss: 138.0192\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.8062 - val_loss: 136.1574\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.5382 - val_loss: 134.3458\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 131.9671 - val_loss: 137.6974\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.8329 - val_loss: 135.6824\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.9027 - val_loss: 133.1774\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.1551 - val_loss: 131.8623\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.4990 - val_loss: 132.7880\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.0052 - val_loss: 131.7714\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 128.2713 - val_loss: 131.9398\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.4087 - val_loss: 128.6121\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.1852 - val_loss: 134.3754\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.7052 - val_loss: 130.9402\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 127.1391 - val_loss: 134.6078\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 129.1360 - val_loss: 132.2897\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.6013 - val_loss: 130.5096\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.5216 - val_loss: 133.5688\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 126.7544 - val_loss: 130.9545\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.8435 - val_loss: 130.5455\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.2622 - val_loss: 130.3458\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 124.4682 - val_loss: 129.5336\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 1s 6ms/step - loss: 151.4590 - val_loss: 147.2682\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 143.8426 - val_loss: 140.2830\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 141.3126 - val_loss: 143.6217\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138.5223 - val_loss: 142.9024\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 136.9168 - val_loss: 139.6721\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134.0923 - val_loss: 136.6218\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.5634 - val_loss: 138.4510\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 130.5198 - val_loss: 134.4046\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.5384 - val_loss: 131.7873\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.6434 - val_loss: 131.9213\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.5084 - val_loss: 133.3094\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132.6413 - val_loss: 135.0962\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.0593 - val_loss: 133.7592\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.2524 - val_loss: 132.2156\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.5932 - val_loss: 132.2039\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.7411 - val_loss: 131.2825\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.6189 - val_loss: 131.3296\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.9224 - val_loss: 132.0670\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.1853 - val_loss: 132.4291\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.7093 - val_loss: 132.8106\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.8055 - val_loss: 132.3364\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.9861 - val_loss: 132.1354\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.6419 - val_loss: 133.2887\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.3526 - val_loss: 132.1970\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.5030 - val_loss: 130.3969\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.9265 - val_loss: 129.2846\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.9455 - val_loss: 131.4996\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 125.2794 - val_loss: 130.4323\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.0788 - val_loss: 130.7430\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.3826 - val_loss: 129.6259\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 125.9156 - val_loss: 131.7627\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.8247 - val_loss: 131.3035\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.5583 - val_loss: 131.7924\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.0902 - val_loss: 132.5514\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.3229 - val_loss: 129.3752\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.0829 - val_loss: 130.1757\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 164.8788 - val_loss: 149.2626\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 142.6466 - val_loss: 139.9473\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 137.3486 - val_loss: 138.1461\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134.6944 - val_loss: 138.1274\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 135.3521 - val_loss: 137.3994\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 132.9542 - val_loss: 135.7313\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 138.1758 - val_loss: 141.5505\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134.5246 - val_loss: 133.8099\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.8821 - val_loss: 136.7090\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132.3981 - val_loss: 135.7073\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.8446 - val_loss: 133.7641\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.4069 - val_loss: 135.9029\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 131.6768 - val_loss: 133.2182\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 133.4518 - val_loss: 133.7850\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 129.4236 - val_loss: 132.1247\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.5331 - val_loss: 132.3661\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.3452 - val_loss: 133.6417\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.4526 - val_loss: 131.4541\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.7182 - val_loss: 130.4387\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.5793 - val_loss: 130.7149\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.2604 - val_loss: 129.3466\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 126.1383 - val_loss: 129.7023\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.2745 - val_loss: 130.5835\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.5743 - val_loss: 133.2079\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.5758 - val_loss: 132.5156\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.6098 - val_loss: 130.6495\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.8400 - val_loss: 130.4170\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.1427 - val_loss: 133.3716\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 127.4215 - val_loss: 130.5756\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 128.1279 - val_loss: 131.2956\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.0320 - val_loss: 131.8555\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',300,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),300,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c961ec8-129c-4f36-a2dc-04b0b9661a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 926us/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 972us/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 936us/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 961us/step\n",
      "12/12 [==============================] - 0s 975us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22204233455907127, 0.24363793993498115)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, test_X)\n",
    "mase_predictions = bagging_predict2(pred2, test_X)\n",
    "mape_predictions = bagging_predict2(pred3, test_X)\n",
    "concat_I = np.concatenate([smape_predictions, mase_predictions,mape_predictions],axis=0)\n",
    "fin_pred_I = np.median(concat_I,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "mean_squared_error(test_y.flatten(),fin_pred_I.flatten()),mean_absolute_error(test_y.flatten(),fin_pred_I.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02005ee4-9b13-4a5a-a2c3-d25441395630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin_pred_I.reshape(-1,24)).to_csv(\"../result7_new/NBEATs_B/pred_mid_I.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat_I[i].reshape(-1,24)).to_csv(f\"../result7_new/NBEATs_B/pred_I{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a104b3-22fb-46e4-855e-ca5e5202a204",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 일반블락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7fa618-a25e-48c5-9544-ca3e091f28ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 13:34:23.283580: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-30 13:34:23.283624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-08-30 13:34:23.284210: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0933 - val_loss: 0.7103\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8303 - val_loss: 0.6923\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8062 - val_loss: 0.6695\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7675 - val_loss: 0.6504\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7564 - val_loss: 0.6403\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7356 - val_loss: 0.6313\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7371 - val_loss: 0.7413\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7363 - val_loss: 0.6210\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7088 - val_loss: 0.6454\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6880 - val_loss: 0.6325\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6879 - val_loss: 0.6368\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6767 - val_loss: 0.6605\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6760 - val_loss: 0.6239\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6534 - val_loss: 0.6319\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6326 - val_loss: 0.6402\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6202 - val_loss: 0.6148\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6052 - val_loss: 0.6443\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5894 - val_loss: 0.6265\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5743 - val_loss: 0.6451\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5595 - val_loss: 0.6686\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5324 - val_loss: 0.6565\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5427 - val_loss: 0.6597\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4988 - val_loss: 0.6273\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4708 - val_loss: 0.6230\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4846 - val_loss: 0.6812\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4731 - val_loss: 0.6539\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.1364 - val_loss: 0.7341\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8492 - val_loss: 0.6626\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8172 - val_loss: 0.6753\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7805 - val_loss: 0.6663\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7651 - val_loss: 0.6796\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7455 - val_loss: 0.6262\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7281 - val_loss: 0.6143\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7309 - val_loss: 0.6316\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7123 - val_loss: 0.6486\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7017 - val_loss: 0.6284\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6867 - val_loss: 0.6220\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6650 - val_loss: 0.6533\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6574 - val_loss: 0.6535\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6475 - val_loss: 0.6429\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6347 - val_loss: 0.6321\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6082 - val_loss: 0.6711\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5969 - val_loss: 0.6299\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0709 - val_loss: 0.7252\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8543 - val_loss: 0.6934\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8049 - val_loss: 0.6763\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7747 - val_loss: 0.6343\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7648 - val_loss: 0.6270\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7527 - val_loss: 0.6229\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7416 - val_loss: 0.6187\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7256 - val_loss: 0.6591\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7172 - val_loss: 0.6082\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7004 - val_loss: 0.6154\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6839 - val_loss: 0.6233\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6750 - val_loss: 0.6287\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6681 - val_loss: 0.6277\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6573 - val_loss: 0.6318\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6311 - val_loss: 0.6249\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6246 - val_loss: 0.6446\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5867 - val_loss: 0.6513\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5773 - val_loss: 0.6265\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5599 - val_loss: 0.6422\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0519 - val_loss: 0.7704\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8478 - val_loss: 0.6719\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7986 - val_loss: 0.7055\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7652 - val_loss: 0.6421\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7505 - val_loss: 0.6195\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7418 - val_loss: 0.6928\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7244 - val_loss: 0.6627\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7040 - val_loss: 0.6555\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7208 - val_loss: 0.6518\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7008 - val_loss: 0.6192\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6703 - val_loss: 0.6353\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6735 - val_loss: 0.6229\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6347 - val_loss: 0.6327\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6268 - val_loss: 0.6486\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6114 - val_loss: 0.6270\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5791 - val_loss: 0.6381\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5933 - val_loss: 0.6309\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5652 - val_loss: 0.6326\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5380 - val_loss: 0.6407\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5261 - val_loss: 0.6459\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0769 - val_loss: 0.7522\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8623 - val_loss: 0.6995\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8064 - val_loss: 0.6756\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7815 - val_loss: 0.6133\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7548 - val_loss: 0.6393\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7358 - val_loss: 0.6103\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7186 - val_loss: 0.6497\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7165 - val_loss: 0.6452\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7041 - val_loss: 0.6409\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6837 - val_loss: 0.6179\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6873 - val_loss: 0.6318\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6602 - val_loss: 0.6248\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6440 - val_loss: 0.6543\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6192 - val_loss: 0.6202\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5924 - val_loss: 0.6819\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5906 - val_loss: 0.6231\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0611 - val_loss: 0.6914\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8231 - val_loss: 0.6568\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7848 - val_loss: 0.6528\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7745 - val_loss: 0.7228\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7495 - val_loss: 0.6275\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7287 - val_loss: 0.6731\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7133 - val_loss: 0.6284\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7016 - val_loss: 0.6233\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6929 - val_loss: 0.6366\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6761 - val_loss: 0.6665\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6745 - val_loss: 0.6446\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6745 - val_loss: 0.6882\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6356 - val_loss: 0.6588\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6518 - val_loss: 0.6821\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6182 - val_loss: 0.6332\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5974 - val_loss: 0.6567\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5732 - val_loss: 0.6660\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5766 - val_loss: 0.6818\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0761 - val_loss: 0.7015\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8575 - val_loss: 0.6905\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8045 - val_loss: 0.6789\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7857 - val_loss: 0.6642\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7645 - val_loss: 0.6522\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7433 - val_loss: 0.6346\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7292 - val_loss: 0.6338\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7089 - val_loss: 0.6139\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7002 - val_loss: 0.6620\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7003 - val_loss: 0.6370\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6637 - val_loss: 0.6434\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6638 - val_loss: 0.6212\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6479 - val_loss: 0.6352\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6344 - val_loss: 0.6600\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6136 - val_loss: 0.6566\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5917 - val_loss: 0.6560\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5930 - val_loss: 0.6418\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5798 - val_loss: 0.6780\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0612 - val_loss: 0.6814\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.8393 - val_loss: 0.6860\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8011 - val_loss: 0.6789\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7775 - val_loss: 0.6299\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7477 - val_loss: 0.6272\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7354 - val_loss: 0.6351\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7159 - val_loss: 0.6162\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7240 - val_loss: 0.6128\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6935 - val_loss: 0.6115\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6818 - val_loss: 0.6264\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6807 - val_loss: 0.6023\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6549 - val_loss: 0.6129\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6423 - val_loss: 0.6269\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6356 - val_loss: 0.6136\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6208 - val_loss: 0.6418\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5961 - val_loss: 0.6217\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5972 - val_loss: 0.6322\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5799 - val_loss: 0.6540\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5601 - val_loss: 0.6476\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5374 - val_loss: 0.6171\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5215 - val_loss: 0.6381\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0619 - val_loss: 0.7134\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8520 - val_loss: 0.6924\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8095 - val_loss: 0.6465\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7763 - val_loss: 0.6508\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7547 - val_loss: 0.6204\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7417 - val_loss: 0.6331\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7303 - val_loss: 0.6195\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7278 - val_loss: 0.6526\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7066 - val_loss: 0.6519\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6843 - val_loss: 0.6530\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6838 - val_loss: 0.6726\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6603 - val_loss: 0.6590\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6541 - val_loss: 0.6384\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6184 - val_loss: 0.6403\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6111 - val_loss: 0.6496\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5786 - val_loss: 0.6528\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5753 - val_loss: 0.6435\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 1.0682 - val_loss: 0.7616\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8529 - val_loss: 0.6852\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7867 - val_loss: 0.6703\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7679 - val_loss: 0.6614\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7571 - val_loss: 0.6261\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7317 - val_loss: 0.6168\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7202 - val_loss: 0.6164\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7020 - val_loss: 0.6430\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6935 - val_loss: 0.6518\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.7205 - val_loss: 0.6455\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6548 - val_loss: 0.6214\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6524 - val_loss: 0.6599\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6299 - val_loss: 0.6209\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.6196 - val_loss: 0.6556\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5966 - val_loss: 0.6231\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5769 - val_loss: 0.6030\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5698 - val_loss: 0.6300\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5487 - val_loss: 0.6640\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5542 - val_loss: 0.6236\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.5074 - val_loss: 0.6410\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4941 - val_loss: 0.6549\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4816 - val_loss: 0.6623\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4641 - val_loss: 0.6359\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4373 - val_loss: 0.6666\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4361 - val_loss: 0.6779\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4435 - val_loss: 0.7178\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 59357536.0000 - val_loss: 28044494.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 18452350.0000 - val_loss: 15018413.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 13413185.0000 - val_loss: 11019790.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 8506725.0000 - val_loss: 7618660.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6765817.5000 - val_loss: 6876006.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5625974.0000 - val_loss: 6475926.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4179286.7500 - val_loss: 4689041.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3624748.5000 - val_loss: 3769909.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2971000.0000 - val_loss: 3099382.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2587307.2500 - val_loss: 3273516.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2263975.0000 - val_loss: 2442301.5000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2022665.7500 - val_loss: 2759293.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1797231.3750 - val_loss: 2030206.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1628968.7500 - val_loss: 2095712.5000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1642773.7500 - val_loss: 2340838.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1622237.3750 - val_loss: 2412787.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1433236.7500 - val_loss: 1770861.2500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1193057.5000 - val_loss: 1913334.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1131569.6250 - val_loss: 1636506.1250\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1113448.2500 - val_loss: 1601229.0000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1054835.7500 - val_loss: 1528789.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1019986.0000 - val_loss: 1553244.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1037899.3125 - val_loss: 1329617.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 897254.7500 - val_loss: 1367388.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 890093.6875 - val_loss: 1247232.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 822326.6875 - val_loss: 1363336.1250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 795911.2500 - val_loss: 1317261.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 704841.2500 - val_loss: 1119639.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 674668.7500 - val_loss: 1342061.6250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 711448.4375 - val_loss: 1349808.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 598920.7500 - val_loss: 1404642.8750\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 663315.5625 - val_loss: 1239642.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 655171.1875 - val_loss: 1173562.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 623840.7500 - val_loss: 1339347.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 735958.7500 - val_loss: 1030381.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 700070.4375 - val_loss: 1197761.3750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 624124.4375 - val_loss: 1017795.3125\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 692372.8125 - val_loss: 1092824.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 734769.7500 - val_loss: 1354594.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 671230.5625 - val_loss: 1327141.1250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 600967.3750 - val_loss: 887791.5625\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 679425.3750 - val_loss: 1196443.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 699125.1250 - val_loss: 1207374.0000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 683970.0000 - val_loss: 955014.7500\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 548039.0625 - val_loss: 1001393.3125\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 577295.1250 - val_loss: 907621.6250\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 653282.8750 - val_loss: 1506038.8750\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 711140.5000 - val_loss: 1052063.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 632291.1250 - val_loss: 989992.6250\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 532649.0625 - val_loss: 1000013.1250\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 598382.2500 - val_loss: 1102352.2500\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 58568508.0000 - val_loss: 25567478.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 20132994.0000 - val_loss: 12151428.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 11877702.0000 - val_loss: 10758977.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 9797081.0000 - val_loss: 7281684.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6173201.5000 - val_loss: 5071511.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5068307.5000 - val_loss: 6013182.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4357302.5000 - val_loss: 3985634.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3416425.5000 - val_loss: 3774774.2500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2968927.5000 - val_loss: 3026550.2500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2590490.7500 - val_loss: 2759413.7500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2291339.7500 - val_loss: 2847103.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1944181.3750 - val_loss: 2669031.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1700472.5000 - val_loss: 2611629.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1631628.0000 - val_loss: 1810776.8750\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1433703.5000 - val_loss: 1739790.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1439170.2500 - val_loss: 2506120.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1251527.5000 - val_loss: 1814845.3750\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1211067.5000 - val_loss: 1643228.8750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1054565.6250 - val_loss: 1791175.0000\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1028900.3125 - val_loss: 1591244.5000\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 952544.4375 - val_loss: 1575028.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1052915.2500 - val_loss: 1283360.5000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 860742.4375 - val_loss: 1404178.3750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 865484.2500 - val_loss: 1237427.3750\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 760611.4375 - val_loss: 1167965.7500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 724674.4375 - val_loss: 1236317.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 791160.9375 - val_loss: 1513326.6250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 697558.6250 - val_loss: 1084645.5000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 741758.9375 - val_loss: 1252448.2500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 665042.2500 - val_loss: 1068127.7500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 782817.5000 - val_loss: 965425.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 672054.6875 - val_loss: 1076335.7500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 701605.0625 - val_loss: 1352973.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 693113.7500 - val_loss: 1198682.8750\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 630704.3750 - val_loss: 1441733.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 740914.7500 - val_loss: 1248223.5000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 563209.9375 - val_loss: 1018129.7500\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 640473.4375 - val_loss: 827650.8125\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 567304.5000 - val_loss: 953362.8750\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 595041.0000 - val_loss: 1065054.7500\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 608082.3750 - val_loss: 1117021.1250\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 697649.4375 - val_loss: 1039187.3125\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 619951.6250 - val_loss: 1218933.7500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 703706.2500 - val_loss: 1062096.8750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 646682.4375 - val_loss: 818546.1875\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 613419.6250 - val_loss: 1118690.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 733832.3750 - val_loss: 1227377.7500\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 592990.6250 - val_loss: 1069957.2500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 704131.0000 - val_loss: 926203.2500\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 640724.3750 - val_loss: 1002106.3750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 715598.6250 - val_loss: 1173840.6250\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 809553.3750 - val_loss: 1111552.1250\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 790027.5625 - val_loss: 1203302.1250\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 759176.7500 - val_loss: 936405.3125\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 681282.1875 - val_loss: 1176636.2500\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 58648972.0000 - val_loss: 22341052.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 17818364.0000 - val_loss: 14758851.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 11448339.0000 - val_loss: 9008837.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7426369.5000 - val_loss: 6014897.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6227558.0000 - val_loss: 5799111.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4516816.5000 - val_loss: 4718281.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3529825.5000 - val_loss: 3844831.2500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3116710.7500 - val_loss: 3557879.2500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2661264.5000 - val_loss: 2921113.0000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2237704.5000 - val_loss: 2625697.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2009126.8750 - val_loss: 2251981.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1849320.5000 - val_loss: 2324235.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1610219.8750 - val_loss: 2281409.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1400829.3750 - val_loss: 1948493.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1393648.3750 - val_loss: 1392176.1250\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1270893.0000 - val_loss: 1689804.3750\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1189712.7500 - val_loss: 1576481.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1113989.6250 - val_loss: 1527296.3750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1019601.8750 - val_loss: 1673547.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1013433.2500 - val_loss: 1649653.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 909908.4375 - val_loss: 1470714.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 920259.6875 - val_loss: 1562387.2500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 815959.3750 - val_loss: 1458957.3750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 716865.9375 - val_loss: 1436214.7500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 847525.1875 - val_loss: 1338635.0000\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 826830.8125 - val_loss: 1344321.5000\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 743330.7500 - val_loss: 1383681.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 661533.6875 - val_loss: 1267127.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 770898.1250 - val_loss: 1088608.3750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 722345.2500 - val_loss: 1072857.1250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 670881.7500 - val_loss: 1030368.4375\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 591872.5625 - val_loss: 1066027.6250\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 639304.3750 - val_loss: 1048009.8125\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 598787.3125 - val_loss: 1009767.7500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 605723.9375 - val_loss: 1012151.7500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 656990.0625 - val_loss: 715987.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 620544.2500 - val_loss: 1157529.3750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 694796.7500 - val_loss: 1391215.7500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 635383.2500 - val_loss: 1087418.2500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 658867.3125 - val_loss: 1018549.9375\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 617346.5000 - val_loss: 1118240.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 755244.7500 - val_loss: 1121350.2500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 781037.8125 - val_loss: 1297628.7500\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 556059.0000 - val_loss: 764617.8750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 654398.3125 - val_loss: 975100.5000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 765543.0625 - val_loss: 977386.0625\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 56179056.0000 - val_loss: 20598904.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 17572248.0000 - val_loss: 13477387.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 11148484.0000 - val_loss: 9587485.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7715274.0000 - val_loss: 7946842.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6045529.5000 - val_loss: 6193952.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4963986.0000 - val_loss: 4067272.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3822643.0000 - val_loss: 3771155.7500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3233089.7500 - val_loss: 3375953.7500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2698252.2500 - val_loss: 3032956.2500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2145605.0000 - val_loss: 2670383.7500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1878748.8750 - val_loss: 2360938.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1788702.2500 - val_loss: 2471017.0000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1691350.7500 - val_loss: 2235214.0000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1434339.5000 - val_loss: 1744246.6250\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1314298.8750 - val_loss: 1554145.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1113076.7500 - val_loss: 1599909.8750\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1007946.2500 - val_loss: 1702176.6250\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1058071.6250 - val_loss: 1488298.3750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1064420.3750 - val_loss: 1665619.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 913263.1250 - val_loss: 1319682.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 915980.0625 - val_loss: 1362796.0000\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 812051.7500 - val_loss: 1140937.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 731455.5625 - val_loss: 1220720.5000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 850334.0625 - val_loss: 1226006.2500\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 771923.1875 - val_loss: 1182764.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 778177.8125 - val_loss: 1091914.1250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 731890.4375 - val_loss: 1367491.1250\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 702111.7500 - val_loss: 1156058.2500\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 763181.0000 - val_loss: 1203562.7500\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 637626.2500 - val_loss: 1130661.5000\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 647624.3750 - val_loss: 1306534.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 648159.6875 - val_loss: 1599431.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 701558.8750 - val_loss: 1039369.6875\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 652639.2500 - val_loss: 1181598.5000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 636317.9375 - val_loss: 1217668.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 693591.2500 - val_loss: 1005096.1875\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 590624.4375 - val_loss: 1112282.1250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 581905.8750 - val_loss: 1017457.6250\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 605740.3750 - val_loss: 1404728.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 774554.1250 - val_loss: 1423953.1250\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 583791.8125 - val_loss: 823397.8750\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 561956.3125 - val_loss: 1236336.3750\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 723976.4375 - val_loss: 1434902.3750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 621167.0000 - val_loss: 1143894.3750\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 684521.5625 - val_loss: 1149951.7500\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 735930.1875 - val_loss: 1797391.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 633175.4375 - val_loss: 1092041.3750\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 630055.4375 - val_loss: 1296192.7500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 640888.8125 - val_loss: 1027293.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 699873.8750 - val_loss: 1191882.3750\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 694304.3125 - val_loss: 1112849.0000\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 53507084.0000 - val_loss: 26510612.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 20037742.0000 - val_loss: 18691012.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 13237198.0000 - val_loss: 12621921.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 8285331.5000 - val_loss: 7889841.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6757114.0000 - val_loss: 5983975.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 5065534.0000 - val_loss: 5137778.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4333220.0000 - val_loss: 4063753.7500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3460869.5000 - val_loss: 3762478.7500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2726365.0000 - val_loss: 2781688.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2513427.0000 - val_loss: 3424372.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2241599.2500 - val_loss: 2889701.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1944355.3750 - val_loss: 2285541.7500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1704427.7500 - val_loss: 2116132.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1560507.5000 - val_loss: 1918112.8750\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1414179.0000 - val_loss: 2179645.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1190646.3750 - val_loss: 1555556.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1136829.3750 - val_loss: 2029085.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1082643.7500 - val_loss: 1639179.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1014718.6875 - val_loss: 1443205.1250\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 888499.3125 - val_loss: 1421239.1250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 914398.1250 - val_loss: 1473795.8750\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 890326.1875 - val_loss: 1415150.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 871205.1250 - val_loss: 1290042.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 762079.9375 - val_loss: 1034511.1250\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 727665.3750 - val_loss: 1432434.3750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 754703.8750 - val_loss: 1938003.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 717526.9375 - val_loss: 1234489.5000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 736198.1875 - val_loss: 1201924.3750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 626740.6250 - val_loss: 1037525.4375\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 614946.8750 - val_loss: 1011384.0625\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 644081.5625 - val_loss: 1001850.2500\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 726715.2500 - val_loss: 1214584.2500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 696353.0000 - val_loss: 1186397.0000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 619605.5000 - val_loss: 1035308.5625\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 617423.1250 - val_loss: 981172.8750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 669496.9375 - val_loss: 1296031.0000\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 635568.2500 - val_loss: 1459701.0000\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 803124.1250 - val_loss: 1140911.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 596396.1875 - val_loss: 1341149.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 723203.0000 - val_loss: 1214495.8750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 697044.8125 - val_loss: 1121817.5000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 640375.5000 - val_loss: 1248060.7500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 703629.2500 - val_loss: 1173413.5000\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 668723.2500 - val_loss: 1584190.5000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 727548.1875 - val_loss: 1137623.5000\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 55961308.0000 - val_loss: 21006224.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 20109106.0000 - val_loss: 17062106.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 13031001.0000 - val_loss: 10759531.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 8019466.0000 - val_loss: 6599222.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5956315.5000 - val_loss: 5575249.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4603033.5000 - val_loss: 4655927.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3671261.5000 - val_loss: 3719572.7500\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2987970.7500 - val_loss: 3602181.7500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2464351.2500 - val_loss: 2753033.7500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2172532.5000 - val_loss: 2543972.0000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2086868.0000 - val_loss: 2140530.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1769177.3750 - val_loss: 2803309.7500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1521587.5000 - val_loss: 2131153.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1372278.7500 - val_loss: 1933465.1250\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1265731.5000 - val_loss: 1788354.7500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1084574.1250 - val_loss: 1661753.6250\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1126586.3750 - val_loss: 1557023.5000\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 997201.7500 - val_loss: 1360396.7500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 934656.8750 - val_loss: 1453565.6250\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 907242.4375 - val_loss: 1732273.7500\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 868690.0625 - val_loss: 1355163.3750\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 822723.7500 - val_loss: 1330655.7500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 813725.5625 - val_loss: 1406539.7500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 683025.2500 - val_loss: 1377466.3750\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 723728.0000 - val_loss: 1241408.1250\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 722017.7500 - val_loss: 1146917.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 742863.0000 - val_loss: 1091084.0000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 663911.3750 - val_loss: 1021685.3125\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 708926.5625 - val_loss: 1079485.8750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 598881.2500 - val_loss: 1223593.1250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 579871.4375 - val_loss: 911744.9375\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 724534.2500 - val_loss: 1361962.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 666821.5625 - val_loss: 1053202.7500\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 612873.1875 - val_loss: 1461839.5000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 622085.7500 - val_loss: 1025878.6250\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 569241.9375 - val_loss: 1089125.7500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 647530.3750 - val_loss: 1033648.5625\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 673120.7500 - val_loss: 1058939.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 588637.4375 - val_loss: 1390288.2500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 649622.6875 - val_loss: 893923.0000\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 658483.5625 - val_loss: 1806953.7500\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 721277.2500 - val_loss: 1082822.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 601072.3750 - val_loss: 940574.4375\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 608247.1875 - val_loss: 965388.5625\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 626223.9375 - val_loss: 892220.1875\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 591926.8750 - val_loss: 1020552.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 561375.1250 - val_loss: 1054972.1250\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 663945.6875 - val_loss: 889150.2500\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 676303.2500 - val_loss: 1277040.8750\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 617645.8750 - val_loss: 1120201.7500\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 753128.2500 - val_loss: 1185294.0000\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 684150.7500 - val_loss: 1430930.8750\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 667608.3750 - val_loss: 1121529.8750\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 759913.0000 - val_loss: 1105036.6250\n",
      "Epoch 55/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 699760.9375 - val_loss: 1243652.7500\n",
      "Epoch 56/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 814258.6875 - val_loss: 1190393.3750\n",
      "Epoch 57/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 643821.1250 - val_loss: 1311589.3750\n",
      "Epoch 58/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 764903.6250 - val_loss: 1251202.1250\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 55142332.0000 - val_loss: 25198648.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 19246074.0000 - val_loss: 12828008.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 12124658.0000 - val_loss: 9186074.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 8446907.0000 - val_loss: 7089044.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 6237829.5000 - val_loss: 6506759.5000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5158652.5000 - val_loss: 5011343.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4214229.0000 - val_loss: 3804405.0000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3425545.0000 - val_loss: 3761966.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2702614.2500 - val_loss: 3287580.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2599073.2500 - val_loss: 2705047.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2032449.5000 - val_loss: 3129618.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1955221.1250 - val_loss: 2665732.7500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1685287.5000 - val_loss: 1975465.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1610365.3750 - val_loss: 2080923.3750\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1447879.8750 - val_loss: 1883081.2500\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1436071.3750 - val_loss: 2728538.5000\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1357987.8750 - val_loss: 1622920.1250\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1123356.8750 - val_loss: 1986793.5000\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1140767.5000 - val_loss: 1422386.3750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1070896.6250 - val_loss: 1501532.6250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 899375.0625 - val_loss: 1649889.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1031332.4375 - val_loss: 1744220.8750\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 844795.3750 - val_loss: 1447608.8750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 807260.6250 - val_loss: 1260768.3750\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 773688.6250 - val_loss: 1121503.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 733217.8750 - val_loss: 1248694.8750\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 802073.9375 - val_loss: 1131536.7500\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 627259.4375 - val_loss: 1029704.5000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 700377.4375 - val_loss: 1113224.1250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 656908.3750 - val_loss: 1403187.2500\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 611769.8750 - val_loss: 1199005.3750\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 643594.1875 - val_loss: 1143367.7500\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 577796.0625 - val_loss: 1241349.3750\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 637316.4375 - val_loss: 1091545.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 546118.1875 - val_loss: 1079098.2500\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 685246.8125 - val_loss: 1184824.7500\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 601544.3125 - val_loss: 1037492.0000\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 699457.8750 - val_loss: 1231058.6250\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 54041920.0000 - val_loss: 22305996.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 18770686.0000 - val_loss: 16117774.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 11571280.0000 - val_loss: 9294314.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 9065149.0000 - val_loss: 9816772.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6345884.0000 - val_loss: 5829783.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 5303050.0000 - val_loss: 4669701.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4244490.5000 - val_loss: 4636552.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3317671.7500 - val_loss: 3692068.2500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2938764.0000 - val_loss: 4124076.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2362810.5000 - val_loss: 2737631.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2147278.5000 - val_loss: 2753327.7500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1831201.0000 - val_loss: 2299158.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1706260.5000 - val_loss: 2051588.3750\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1557764.5000 - val_loss: 1942794.1250\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1426092.7500 - val_loss: 1806704.5000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1315678.2500 - val_loss: 1873563.6250\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1246848.6250 - val_loss: 1683354.3750\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1246395.3750 - val_loss: 1516201.6250\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1062271.3750 - val_loss: 1459941.6250\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 967627.4375 - val_loss: 1566804.6250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 950177.4375 - val_loss: 1416854.7500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 925472.0625 - val_loss: 1830913.2500\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 844500.6250 - val_loss: 1523166.3750\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 883728.0000 - val_loss: 1574182.1250\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 799151.3125 - val_loss: 1355702.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 805468.4375 - val_loss: 1294309.1250\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 795691.7500 - val_loss: 1106493.5000\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 770427.7500 - val_loss: 1290841.8750\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 638730.8750 - val_loss: 945105.8750\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 754813.8125 - val_loss: 1312067.1250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 819792.4375 - val_loss: 1160328.1250\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 655260.5625 - val_loss: 1136474.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 695292.5625 - val_loss: 1235837.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 769565.6250 - val_loss: 1102267.2500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 641098.0000 - val_loss: 1245745.3750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 710167.3125 - val_loss: 1248532.3750\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 771048.2500 - val_loss: 1583100.3750\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 777670.6250 - val_loss: 1231762.0000\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 712510.0000 - val_loss: 840399.7500\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 621753.9375 - val_loss: 1315055.3750\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 684083.8750 - val_loss: 1386629.0000\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 688357.2500 - val_loss: 1084698.2500\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 714072.3125 - val_loss: 1467126.6250\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 882317.0625 - val_loss: 1407472.5000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 800887.7500 - val_loss: 1537583.0000\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 802778.5000 - val_loss: 1243062.5000\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 728833.2500 - val_loss: 1343975.5000\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 733179.2500 - val_loss: 1288058.6250\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 801517.6875 - val_loss: 1370737.5000\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 55195884.0000 - val_loss: 25303896.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 17509390.0000 - val_loss: 14843800.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 11794195.0000 - val_loss: 9387014.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 7641008.0000 - val_loss: 7817708.5000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6083067.5000 - val_loss: 6195257.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4809481.5000 - val_loss: 5351449.0000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4055807.0000 - val_loss: 4759263.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 3515316.2500 - val_loss: 3465247.7500\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2751036.2500 - val_loss: 3006882.5000\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2432492.5000 - val_loss: 3058242.5000\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2065437.5000 - val_loss: 2771923.2500\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1858285.0000 - val_loss: 2279891.5000\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1809849.2500 - val_loss: 2691196.7500\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1565415.5000 - val_loss: 2437597.7500\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1585833.3750 - val_loss: 2687049.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1327631.1250 - val_loss: 2410696.7500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1363640.0000 - val_loss: 1708130.6250\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1175031.1250 - val_loss: 2086916.2500\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1163330.5000 - val_loss: 1791079.2500\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1086109.3750 - val_loss: 1471967.6250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1054289.5000 - val_loss: 1467121.2500\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 909640.0000 - val_loss: 1258811.6250\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 815810.2500 - val_loss: 1404784.2500\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 881730.7500 - val_loss: 1274635.6250\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 818959.4375 - val_loss: 1441029.8750\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 759076.7500 - val_loss: 1041037.8125\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 890053.0625 - val_loss: 1423306.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 786106.8125 - val_loss: 1280454.0000\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 633588.6875 - val_loss: 1496454.0000\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 789880.0000 - val_loss: 1491535.6250\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 683822.2500 - val_loss: 1274401.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 627373.5625 - val_loss: 1285264.5000\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 616637.9375 - val_loss: 1114971.5000\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 721842.4375 - val_loss: 1122568.2500\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 625002.5000 - val_loss: 1084659.8750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 591370.9375 - val_loss: 884076.1250\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 650123.8750 - val_loss: 976104.6250\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 591569.7500 - val_loss: 1230253.2500\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 551535.2500 - val_loss: 1264053.5000\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 649998.7500 - val_loss: 955934.8125\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 572865.0000 - val_loss: 1059608.1250\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 695949.2500 - val_loss: 1062464.1250\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 724321.1875 - val_loss: 1585160.8750\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 624216.7500 - val_loss: 1080340.0000\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 693049.0000 - val_loss: 1267959.3750\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 583650.4375 - val_loss: 1122893.6250\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 59543328.0000 - val_loss: 24746886.0000\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 19726768.0000 - val_loss: 18101724.0000\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 12472532.0000 - val_loss: 12370186.0000\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 8919862.0000 - val_loss: 7843597.0000\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 6511890.5000 - val_loss: 7066960.0000\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 4826764.0000 - val_loss: 5039173.5000\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 4321910.0000 - val_loss: 5020192.5000\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3892474.2500 - val_loss: 3796097.0000\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 3342389.7500 - val_loss: 3342290.2500\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 2859991.0000 - val_loss: 3292536.2500\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2346832.0000 - val_loss: 2864440.0000\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 2086084.0000 - val_loss: 3458299.2500\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1946062.6250 - val_loss: 2366718.5000\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1729258.8750 - val_loss: 2186062.0000\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1473852.2500 - val_loss: 2415630.0000\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1476192.8750 - val_loss: 2572969.2500\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1335458.6250 - val_loss: 1728987.7500\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1222521.7500 - val_loss: 1861017.3750\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1134893.1250 - val_loss: 1908798.8750\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1176233.5000 - val_loss: 1720590.6250\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 1033825.3125 - val_loss: 1317551.8750\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 903993.1250 - val_loss: 1378434.0000\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 942815.3125 - val_loss: 1616664.0000\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 863485.3125 - val_loss: 1573269.1250\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 817727.5625 - val_loss: 1396020.2500\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 826639.6875 - val_loss: 988459.2500\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 806996.3125 - val_loss: 1369075.8750\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 768495.2500 - val_loss: 1276662.1250\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 770701.1875 - val_loss: 1455518.6250\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 750422.2500 - val_loss: 1362440.8750\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 701862.5000 - val_loss: 1232147.5000\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 728500.0625 - val_loss: 1439855.8750\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 711304.6875 - val_loss: 1282511.6250\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 763480.8750 - val_loss: 1121151.0000\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 624978.3125 - val_loss: 1238259.3750\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 653572.5000 - val_loss: 1100157.3750\n",
      "'########################################################Model9\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 8ms/step - loss: 148.8747 - val_loss: 139.8928\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 139.7769 - val_loss: 141.6714\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.0603 - val_loss: 138.7636\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 139.2044 - val_loss: 139.3242\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 137.6228 - val_loss: 138.8694\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.3858 - val_loss: 137.9313\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.8757 - val_loss: 137.6538\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 136.7816 - val_loss: 139.3009\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 136.1155 - val_loss: 137.5106\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 136.7318 - val_loss: 136.8590\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 137.0336 - val_loss: 137.2919\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 135.9542 - val_loss: 136.4393\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 136.1634 - val_loss: 136.2212\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134.3749 - val_loss: 136.3160\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.6916 - val_loss: 135.6278\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 133.8636 - val_loss: 135.7521\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.2185 - val_loss: 133.5925\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.6172 - val_loss: 134.2536\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.9763 - val_loss: 133.6507\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.2045 - val_loss: 132.5409\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.5446 - val_loss: 134.0914\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.1818 - val_loss: 131.8975\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.5446 - val_loss: 133.4449\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.0389 - val_loss: 132.4199\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.4918 - val_loss: 131.6445\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.1327 - val_loss: 132.4428\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.3737 - val_loss: 132.5383\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.8318 - val_loss: 131.7022\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.1074 - val_loss: 132.3420\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.3590 - val_loss: 131.7003\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.7829 - val_loss: 133.4258\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.3144 - val_loss: 133.0297\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.0194 - val_loss: 132.1385\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.5508 - val_loss: 132.1150\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.5875 - val_loss: 132.5953\n",
      "'########################################################Model0\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 147.3381 - val_loss: 147.7689\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 145.8514 - val_loss: 142.5766\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.6074 - val_loss: 140.6802\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.5507 - val_loss: 140.4104\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.5897 - val_loss: 141.0808\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 139.3014 - val_loss: 139.7843\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.7817 - val_loss: 138.6027\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 135.9740 - val_loss: 136.8018\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.2727 - val_loss: 132.9515\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.8016 - val_loss: 135.9138\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132.5292 - val_loss: 135.6478\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.1365 - val_loss: 134.6271\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.1273 - val_loss: 132.3506\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.2894 - val_loss: 133.4880\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.7362 - val_loss: 131.1002\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.4248 - val_loss: 134.4775\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.2420 - val_loss: 130.3201\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.2180 - val_loss: 130.7117\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.6977 - val_loss: 130.8996\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.9996 - val_loss: 133.0582\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.5106 - val_loss: 130.3469\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.8648 - val_loss: 129.1906\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.4923 - val_loss: 128.8015\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.4102 - val_loss: 128.5289\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.9703 - val_loss: 128.8803\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.9761 - val_loss: 128.3295\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.9379 - val_loss: 128.7746\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.3116 - val_loss: 132.8080\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.0434 - val_loss: 130.0021\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.2989 - val_loss: 128.5742\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.7563 - val_loss: 130.4457\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.8612 - val_loss: 129.9882\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.9576 - val_loss: 129.4813\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.2813 - val_loss: 128.6950\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.0870 - val_loss: 127.2009\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.3746 - val_loss: 128.6996\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.5891 - val_loss: 129.1987\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.9543 - val_loss: 129.2110\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.8181 - val_loss: 128.6151\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.2495 - val_loss: 127.7318\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.5034 - val_loss: 128.2293\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.3413 - val_loss: 128.5610\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.5774 - val_loss: 129.0028\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.2254 - val_loss: 129.2833\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.2069 - val_loss: 128.3259\n",
      "'########################################################Model1\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 154.3754 - val_loss: 148.5778\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 145.8251 - val_loss: 145.8248\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 145.3615 - val_loss: 144.7512\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 145.5611 - val_loss: 146.4874\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 145.0942 - val_loss: 145.8209\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 144.7545 - val_loss: 145.4163\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 143.8307 - val_loss: 145.6956\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 143.6420 - val_loss: 144.3639\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 142.3000 - val_loss: 136.6282\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 135.9768 - val_loss: 138.1282\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 134.1884 - val_loss: 135.6112\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 135.0830 - val_loss: 135.7928\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.2921 - val_loss: 135.1386\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.8742 - val_loss: 135.8053\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.1876 - val_loss: 135.5165\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.7845 - val_loss: 134.7312\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.7331 - val_loss: 133.5254\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.7860 - val_loss: 134.9600\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.1771 - val_loss: 135.9722\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.3510 - val_loss: 133.7905\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.3889 - val_loss: 134.2722\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.1391 - val_loss: 133.3416\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.3170 - val_loss: 129.9455\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.7399 - val_loss: 130.7009\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.7819 - val_loss: 129.6307\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.9967 - val_loss: 130.5650\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.2944 - val_loss: 130.0780\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.6384 - val_loss: 129.7339\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.7464 - val_loss: 132.8746\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.4150 - val_loss: 130.2586\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.0954 - val_loss: 130.2784\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.2126 - val_loss: 130.5522\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.0792 - val_loss: 130.2183\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 119.6890 - val_loss: 132.6834\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.6871 - val_loss: 131.5627\n",
      "'########################################################Model2\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 153.2879 - val_loss: 144.5435\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 141.9351 - val_loss: 142.4534\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.2585 - val_loss: 138.9455\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.4455 - val_loss: 136.6012\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.7062 - val_loss: 140.6820\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.3745 - val_loss: 141.6049\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.9227 - val_loss: 138.3675\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 136.3000 - val_loss: 138.9600\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 136.4949 - val_loss: 136.2280\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 135.4174 - val_loss: 136.8895\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 135.0145 - val_loss: 135.5870\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 135.0297 - val_loss: 135.0192\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 134.0871 - val_loss: 135.1082\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 131.6055 - val_loss: 133.1110\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.6044 - val_loss: 133.1176\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.0895 - val_loss: 133.7942\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.8820 - val_loss: 131.9246\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.8846 - val_loss: 131.3795\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.3795 - val_loss: 130.7130\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.6256 - val_loss: 130.8327\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.6442 - val_loss: 130.9450\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.7246 - val_loss: 129.6170\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.9382 - val_loss: 130.2740\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.6069 - val_loss: 129.7859\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.4135 - val_loss: 129.3513\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.3451 - val_loss: 130.2456\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.5095 - val_loss: 131.9123\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.9797 - val_loss: 129.1091\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.6213 - val_loss: 129.1440\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.7265 - val_loss: 129.4941\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.4351 - val_loss: 128.9673\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.0466 - val_loss: 131.4651\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.0866 - val_loss: 129.1482\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.1120 - val_loss: 132.0243\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 119.3825 - val_loss: 131.0031\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 118.3358 - val_loss: 129.5196\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117.6115 - val_loss: 127.6075\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117.2326 - val_loss: 129.6841\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117.2854 - val_loss: 129.3376\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 116.6130 - val_loss: 128.2071\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 117.3851 - val_loss: 129.8142\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 116.4231 - val_loss: 128.6838\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 116.2374 - val_loss: 130.5077\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 116.0189 - val_loss: 129.7018\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 116.1780 - val_loss: 129.6240\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 115.0633 - val_loss: 129.3136\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 115.1733 - val_loss: 130.8830\n",
      "'########################################################Model3\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 168.6620 - val_loss: 169.7250\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 168.7165 - val_loss: 168.1953\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 168.9142 - val_loss: 167.0651\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 167.7739 - val_loss: 166.6908\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 166.7166 - val_loss: 165.5102\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 163.8619 - val_loss: 160.4831\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 148.9889 - val_loss: 141.9915\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 141.6777 - val_loss: 141.0784\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 142.1454 - val_loss: 140.9435\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 141.6090 - val_loss: 141.8865\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 141.3154 - val_loss: 142.6822\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 141.2203 - val_loss: 141.8263\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 142.0248 - val_loss: 141.0794\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 141.1302 - val_loss: 140.9336\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.4327 - val_loss: 136.6036\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 138.1898 - val_loss: 140.6290\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 136.3624 - val_loss: 141.6778\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 135.9595 - val_loss: 134.3509\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.8111 - val_loss: 134.2922\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 133.4709 - val_loss: 135.9279\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.8579 - val_loss: 134.0909\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.6644 - val_loss: 132.5484\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.9386 - val_loss: 130.5287\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.0447 - val_loss: 133.2052\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.0128 - val_loss: 130.1188\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.5517 - val_loss: 131.6163\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.5789 - val_loss: 129.0800\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.8982 - val_loss: 129.8977\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.5827 - val_loss: 130.6998\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.3792 - val_loss: 129.9278\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.5474 - val_loss: 129.4362\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.3016 - val_loss: 129.4585\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.7494 - val_loss: 130.1369\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.2580 - val_loss: 129.8568\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.6813 - val_loss: 129.2020\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.9658 - val_loss: 129.6332\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.8884 - val_loss: 129.8253\n",
      "'########################################################Model4\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 9ms/step - loss: 150.5047 - val_loss: 146.3030\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 148.1219 - val_loss: 143.4356\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.6848 - val_loss: 141.5391\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.0009 - val_loss: 139.7316\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.0256 - val_loss: 138.6298\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 137.2192 - val_loss: 136.8737\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.3492 - val_loss: 135.6825\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 135.9909 - val_loss: 140.3194\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 139.6797 - val_loss: 141.8611\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 138.2182 - val_loss: 138.7006\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 132.5717 - val_loss: 137.1336\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 132.2829 - val_loss: 135.1805\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.6662 - val_loss: 136.1434\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.0917 - val_loss: 132.2024\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.9528 - val_loss: 132.2096\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.9564 - val_loss: 131.5342\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.2861 - val_loss: 132.3004\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.2094 - val_loss: 132.9508\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.9721 - val_loss: 131.4377\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.9456 - val_loss: 132.5659\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.9160 - val_loss: 131.2204\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 126.6287 - val_loss: 132.1377\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 126.4686 - val_loss: 132.4426\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 126.3100 - val_loss: 131.8047\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.8333 - val_loss: 133.2066\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.6293 - val_loss: 130.8667\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.0284 - val_loss: 130.7801\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.8017 - val_loss: 132.1703\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.1039 - val_loss: 131.2263\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 124.6891 - val_loss: 131.1742\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.7388 - val_loss: 131.4417\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.1635 - val_loss: 130.6820\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.6329 - val_loss: 132.8592\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 122.8821 - val_loss: 131.5216\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 122.5771 - val_loss: 131.9952\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.9584 - val_loss: 131.1917\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 122.0851 - val_loss: 132.5675\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 121.2067 - val_loss: 131.4346\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 120.9827 - val_loss: 135.0920\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.0318 - val_loss: 130.9121\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.3186 - val_loss: 132.3641\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.1465 - val_loss: 131.9937\n",
      "'########################################################Model5\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 12ms/step - loss: 146.8902 - val_loss: 140.2380\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 137.6937 - val_loss: 136.7717\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 136.2002 - val_loss: 135.5974\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 134.2189 - val_loss: 135.8336\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 133.4624 - val_loss: 134.8551\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 133.8280 - val_loss: 135.0417\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 133.3059 - val_loss: 133.1895\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 132.1323 - val_loss: 134.5445\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.8348 - val_loss: 135.4768\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 131.8504 - val_loss: 136.5271\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.3925 - val_loss: 134.4075\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.2797 - val_loss: 135.8491\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 133.7035 - val_loss: 136.9786\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.6224 - val_loss: 132.6164\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.8886 - val_loss: 135.0293\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.5838 - val_loss: 133.1436\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.4912 - val_loss: 132.7478\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.7056 - val_loss: 133.2737\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.0303 - val_loss: 132.3064\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.7975 - val_loss: 132.4559\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.3154 - val_loss: 133.4652\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 129.3162 - val_loss: 132.7379\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 128.6811 - val_loss: 134.4997\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.6595 - val_loss: 132.3256\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.2343 - val_loss: 131.5369\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.8459 - val_loss: 131.8911\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.6093 - val_loss: 130.8422\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.1739 - val_loss: 129.9100\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 122.3163 - val_loss: 129.6934\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.8017 - val_loss: 131.5491\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 121.6141 - val_loss: 130.9871\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.0561 - val_loss: 130.0944\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.1624 - val_loss: 130.6299\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.3122 - val_loss: 129.7049\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.1304 - val_loss: 130.9547\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.4157 - val_loss: 130.6365\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 121.7282 - val_loss: 131.0370\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.5059 - val_loss: 129.5656\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.8208 - val_loss: 129.5135\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.9125 - val_loss: 129.6809\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.3558 - val_loss: 130.9241\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.2239 - val_loss: 130.9184\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.1422 - val_loss: 129.9980\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.6504 - val_loss: 129.1485\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 118.6623 - val_loss: 131.4212\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.3736 - val_loss: 131.2276\n",
      "Epoch 47/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.0901 - val_loss: 129.8397\n",
      "Epoch 48/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 118.1669 - val_loss: 130.1846\n",
      "Epoch 49/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.0750 - val_loss: 129.6934\n",
      "Epoch 50/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 116.7517 - val_loss: 130.4637\n",
      "Epoch 51/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.0684 - val_loss: 129.2908\n",
      "Epoch 52/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.5859 - val_loss: 130.3624\n",
      "Epoch 53/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.6735 - val_loss: 129.5195\n",
      "Epoch 54/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 116.5766 - val_loss: 130.5687\n",
      "'########################################################Model6\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 3s 11ms/step - loss: 153.8526 - val_loss: 148.9582\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.9527 - val_loss: 147.7855\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.9609 - val_loss: 152.7643\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.9989 - val_loss: 148.9297\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.6725 - val_loss: 147.3156\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.2314 - val_loss: 147.4353\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 147.8931 - val_loss: 148.5609\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 147.6633 - val_loss: 147.7361\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 147.5186 - val_loss: 148.0690\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 146.2210 - val_loss: 146.2676\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 143.1544 - val_loss: 137.7438\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 133.4390 - val_loss: 135.1949\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 133.1417 - val_loss: 136.0613\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 132.9624 - val_loss: 133.2903\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.3443 - val_loss: 131.9608\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 128.5415 - val_loss: 133.0267\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 128.7660 - val_loss: 132.1150\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.3058 - val_loss: 131.3016\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 126.5028 - val_loss: 130.7477\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.8110 - val_loss: 132.2728\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 124.5839 - val_loss: 131.0580\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 124.3381 - val_loss: 130.4582\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 124.7297 - val_loss: 131.2939\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.5204 - val_loss: 128.6926\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.6012 - val_loss: 129.8423\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.5701 - val_loss: 130.4379\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.8016 - val_loss: 128.2850\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 123.0298 - val_loss: 131.2812\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 121.8823 - val_loss: 128.7667\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.7413 - val_loss: 131.0105\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.9177 - val_loss: 130.4640\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 122.2764 - val_loss: 130.6912\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.2378 - val_loss: 128.7984\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.5149 - val_loss: 128.0720\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 121.1232 - val_loss: 129.4266\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.7517 - val_loss: 130.5297\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.1500 - val_loss: 129.8425\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.1119 - val_loss: 129.1444\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.4722 - val_loss: 129.5005\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.6632 - val_loss: 128.7788\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.4684 - val_loss: 129.6972\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.4804 - val_loss: 130.8312\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.9668 - val_loss: 128.9959\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.3848 - val_loss: 130.6303\n",
      "'########################################################Model7\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 3s 12ms/step - loss: 154.9793 - val_loss: 147.7739\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 149.5004 - val_loss: 149.3421\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 148.7403 - val_loss: 148.2449\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 148.4378 - val_loss: 146.5991\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 147.8746 - val_loss: 148.8793\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 148.2325 - val_loss: 150.2041\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 148.1263 - val_loss: 146.8746\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 147.1448 - val_loss: 146.8430\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 142.2513 - val_loss: 142.2492\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 140.0281 - val_loss: 140.2926\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 134.9235 - val_loss: 136.8260\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 131.2836 - val_loss: 134.1269\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 131.7233 - val_loss: 134.3771\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.8982 - val_loss: 134.5856\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 130.5622 - val_loss: 133.6322\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 129.2583 - val_loss: 131.2139\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 128.1392 - val_loss: 132.3201\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.4964 - val_loss: 131.5118\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.9032 - val_loss: 133.0356\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 125.9389 - val_loss: 131.4533\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 124.1591 - val_loss: 129.9786\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.4044 - val_loss: 130.4041\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.2643 - val_loss: 130.8175\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.7599 - val_loss: 130.8843\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.0711 - val_loss: 129.5009\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 121.6427 - val_loss: 129.4023\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.8087 - val_loss: 130.1496\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.8670 - val_loss: 130.8763\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.6355 - val_loss: 129.8835\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.7904 - val_loss: 131.9657\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.8799 - val_loss: 129.0056\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.5831 - val_loss: 130.2105\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.7402 - val_loss: 130.2605\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.3080 - val_loss: 130.2978\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.1340 - val_loss: 129.7644\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.6437 - val_loss: 129.6618\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 118.2161 - val_loss: 131.0112\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117.4935 - val_loss: 129.8441\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 117.6136 - val_loss: 129.6440\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 117.5080 - val_loss: 130.6840\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 117.3857 - val_loss: 130.0746\n",
      "'########################################################Model8\n",
      "Epoch 1/300\n",
      "72/72 [==============================] - 2s 11ms/step - loss: 152.1324 - val_loss: 153.0349\n",
      "Epoch 2/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 150.0846 - val_loss: 148.7693\n",
      "Epoch 3/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 144.7160 - val_loss: 142.6597\n",
      "Epoch 4/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 134.4622 - val_loss: 136.7164\n",
      "Epoch 5/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 135.1988 - val_loss: 133.6274\n",
      "Epoch 6/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.4189 - val_loss: 131.8715\n",
      "Epoch 7/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.6453 - val_loss: 132.2278\n",
      "Epoch 8/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 132.3165 - val_loss: 135.0050\n",
      "Epoch 9/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 131.8360 - val_loss: 135.2057\n",
      "Epoch 10/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 130.7270 - val_loss: 134.9999\n",
      "Epoch 11/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.0250 - val_loss: 133.7101\n",
      "Epoch 12/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 129.7471 - val_loss: 131.7131\n",
      "Epoch 13/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 128.0976 - val_loss: 134.3208\n",
      "Epoch 14/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 128.1893 - val_loss: 130.4947\n",
      "Epoch 15/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 129.0681 - val_loss: 131.4900\n",
      "Epoch 16/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 127.7966 - val_loss: 131.3327\n",
      "Epoch 17/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 127.9860 - val_loss: 132.0846\n",
      "Epoch 18/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 126.6608 - val_loss: 130.8119\n",
      "Epoch 19/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 126.7537 - val_loss: 131.3528\n",
      "Epoch 20/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.8863 - val_loss: 130.8026\n",
      "Epoch 21/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 125.8585 - val_loss: 131.1171\n",
      "Epoch 22/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 127.1355 - val_loss: 131.3496\n",
      "Epoch 23/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 126.0808 - val_loss: 130.4571\n",
      "Epoch 24/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.5218 - val_loss: 130.6731\n",
      "Epoch 25/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 124.6418 - val_loss: 130.9122\n",
      "Epoch 26/300\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 125.2071 - val_loss: 131.1685\n",
      "Epoch 27/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 123.8630 - val_loss: 129.8234\n",
      "Epoch 28/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 122.7855 - val_loss: 131.1952\n",
      "Epoch 29/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 123.5410 - val_loss: 131.1053\n",
      "Epoch 30/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 121.7044 - val_loss: 130.0129\n",
      "Epoch 31/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 120.5654 - val_loss: 129.2686\n",
      "Epoch 32/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.4785 - val_loss: 128.7671\n",
      "Epoch 33/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 120.7025 - val_loss: 129.4559\n",
      "Epoch 34/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 119.9137 - val_loss: 129.7950\n",
      "Epoch 35/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.8097 - val_loss: 129.9827\n",
      "Epoch 36/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.5402 - val_loss: 128.5825\n",
      "Epoch 37/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 119.1998 - val_loss: 129.7186\n",
      "Epoch 38/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.9500 - val_loss: 129.3885\n",
      "Epoch 39/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.2550 - val_loss: 129.9447\n",
      "Epoch 40/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 118.1486 - val_loss: 129.9243\n",
      "Epoch 41/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 118.2590 - val_loss: 130.6885\n",
      "Epoch 42/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.7450 - val_loss: 129.0104\n",
      "Epoch 43/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.4717 - val_loss: 129.9954\n",
      "Epoch 44/300\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 117.6248 - val_loss: 130.0383\n",
      "Epoch 45/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.7907 - val_loss: 131.0508\n",
      "Epoch 46/300\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 117.3238 - val_loss: 129.8058\n",
      "'########################################################Model9\n"
     ]
    }
   ],
   "source": [
    "model_num = 10\n",
    "\n",
    "mase_models_G = train_bagging_models_G(model_num, MASE(y_train,24),300,10,8,0.001)\n",
    "mape_models_G = train_bagging_models_G(model_num,'mape',300,10,8,0.001)\n",
    "smape_models_G = train_bagging_models_G(model_num, SMAPE(),300,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8e382f-5a29-464f-abe0-eeee880e371b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2244488404387937, 0.24109146095902362)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1,_=smape_models_G\n",
    "pred2,_=mase_models_G\n",
    "pred3,_=mape_models_G\n",
    "\n",
    "smape_predictions_G = bagging_predict2(pred1, test_X)\n",
    "mase_predictions_G = bagging_predict2(pred2, test_X)\n",
    "mape_predictions_G = bagging_predict2(pred3, test_X)\n",
    "concat_G = np.concatenate([smape_predictions_G, mase_predictions_G,mape_predictions_G],axis=0)\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "mean_squared_error(test_y.flatten(),fin_pred_G.flatten()),mean_absolute_error(test_y.flatten(),fin_pred_G.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed0c5ab-e5ea-4c88-b295-0002063f1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(fin_pred_G.reshape(-1,24)).to_csv(\"../result7_new/NBEATs_B/pred_mid_G.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat_G[i].reshape(-1,24)).to_csv(f\"../result7_new/NBEATs_B/pred_G{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
