{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 03:49:23.112584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 03:49:23.189075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-09 03:49:23.189093: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-09 03:49:23.544337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-09 03:49:23.544387: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-09 03:49:23.544392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target_X= pd.read_csv(\"../data/tem_train_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "target_y =pd.read_csv(\"../data/tem_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_X= pd.read_csv(\"../data/tem_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "test_y =pd.read_csv(\"../data/tem_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)\n",
    "\n",
    "X_train=target_X.astype(np.float32)\n",
    "y_train=target_y.astype(np.float32)\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "# SMAPE 용\n",
    "def train_bagging_models_smape(num_models, loss_fn , epochs_, patience_,batch_size_):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 03:51:27.443247: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-09 03:51:27.443291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-09 03:51:27.443859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 7s 79ms/step - loss: 1.7387 - val_loss: 1.1453\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 1.0665 - val_loss: 0.8050\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.7685 - val_loss: 0.6034\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.6171 - val_loss: 0.4986\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.5364 - val_loss: 0.4968\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.5236 - val_loss: 0.4550\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4856 - val_loss: 0.4660\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.4937 - val_loss: 0.4634\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4896 - val_loss: 0.4811\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4736 - val_loss: 0.4607\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4660 - val_loss: 0.4662\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4637 - val_loss: 0.4239\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4587 - val_loss: 0.4368\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4602 - val_loss: 0.4553\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4515 - val_loss: 0.4363\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4456 - val_loss: 0.4246\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4470 - val_loss: 0.4201\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4428 - val_loss: 0.4293\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4407 - val_loss: 0.4390\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4431 - val_loss: 0.4374\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4428 - val_loss: 0.4392\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.4395 - val_loss: 0.4303\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4368 - val_loss: 0.4244\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4296 - val_loss: 0.4321\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4399 - val_loss: 0.4627\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.4328 - val_loss: 0.4289\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4354Restoring model weights from the end of the best epoch: 17.\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4354 - val_loss: 0.4217\n",
      "Epoch 27: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 89ms/step - loss: 1.5590 - val_loss: 0.9969\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.9449 - val_loss: 0.7661\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.7074 - val_loss: 0.5528\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.5896 - val_loss: 0.5282\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.5294 - val_loss: 0.4952\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5025 - val_loss: 0.4399\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4874 - val_loss: 0.4533\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4788 - val_loss: 0.4717\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4718 - val_loss: 0.4571\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4657 - val_loss: 0.4383\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4564 - val_loss: 0.4377\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4540 - val_loss: 0.4242\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4555 - val_loss: 0.4346\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4504 - val_loss: 0.4359\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4490 - val_loss: 0.4299\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4534 - val_loss: 0.4320\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4541 - val_loss: 0.4679\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4472 - val_loss: 0.4262\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4430 - val_loss: 0.4334\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4404 - val_loss: 0.4663\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4461 - val_loss: 0.4421\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4380 - val_loss: 0.4206\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4426 - val_loss: 0.4380\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4367 - val_loss: 0.4370\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4380 - val_loss: 0.4325\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4242 - val_loss: 0.4296\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4261 - val_loss: 0.4246\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4234 - val_loss: 0.4318\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4182 - val_loss: 0.4658\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4158 - val_loss: 0.4493\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4206 - val_loss: 0.4326\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4168Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4168 - val_loss: 0.4502\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 106ms/step - loss: 1.7534 - val_loss: 1.1948\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 1.0865 - val_loss: 0.8043\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.7846 - val_loss: 0.6187\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.6328 - val_loss: 0.5236\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.5492 - val_loss: 0.4948\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5143 - val_loss: 0.4429\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4921 - val_loss: 0.4476\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4926 - val_loss: 0.4463\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4786 - val_loss: 0.4336\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4695 - val_loss: 0.4347\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4716 - val_loss: 0.4535\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4673 - val_loss: 0.4568\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4587 - val_loss: 0.4278\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4597 - val_loss: 0.4487\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4619 - val_loss: 0.4687\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4590 - val_loss: 0.4253\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4476 - val_loss: 0.4370\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4440 - val_loss: 0.4384\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4375 - val_loss: 0.4162\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4423 - val_loss: 0.4376\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4445 - val_loss: 0.4568\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4516 - val_loss: 0.4309\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4376 - val_loss: 0.4264\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4375 - val_loss: 0.4353\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4371 - val_loss: 0.4261\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4413 - val_loss: 0.4279\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4275 - val_loss: 0.4304\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4392 - val_loss: 0.4350\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4132Restoring model weights from the end of the best epoch: 19.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4132 - val_loss: 0.4334\n",
      "Epoch 29: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 1.6305 - val_loss: 1.0778\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 1.0221 - val_loss: 0.7669\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7505 - val_loss: 0.5917\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6089 - val_loss: 0.5076\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5408 - val_loss: 0.4771\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5039 - val_loss: 0.4633\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4861 - val_loss: 0.4533\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4881 - val_loss: 0.4441\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4759 - val_loss: 0.4452\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4706 - val_loss: 0.4445\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4643 - val_loss: 0.4619\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4622 - val_loss: 0.4316\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4538 - val_loss: 0.4567\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4556 - val_loss: 0.4465\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4562 - val_loss: 0.4484\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4423 - val_loss: 0.4475\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4458 - val_loss: 0.4393\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4394 - val_loss: 0.4480\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4507 - val_loss: 0.4285\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4341 - val_loss: 0.4134\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4324 - val_loss: 0.4255\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4300 - val_loss: 0.4384\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4289 - val_loss: 0.4413\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4353 - val_loss: 0.4182\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4239 - val_loss: 0.4406\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4237 - val_loss: 0.4654\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4306 - val_loss: 0.4550\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4183 - val_loss: 0.4222\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4208 - val_loss: 0.4846\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4107Restoring model weights from the end of the best epoch: 20.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4107 - val_loss: 0.4319\n",
      "Epoch 30: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 104ms/step - loss: 1.5529 - val_loss: 0.9793\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.9081 - val_loss: 0.6851\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.6831 - val_loss: 0.5355\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5714 - val_loss: 0.5059\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5254 - val_loss: 0.4523\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4983 - val_loss: 0.4465\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4819 - val_loss: 0.4404\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4713 - val_loss: 0.4437\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4715 - val_loss: 0.4789\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4674 - val_loss: 0.4645\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4740 - val_loss: 0.4449\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4648 - val_loss: 0.4322\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4553 - val_loss: 0.4534\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4562 - val_loss: 0.4600\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4579 - val_loss: 0.4304\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4471 - val_loss: 0.4306\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4428 - val_loss: 0.4868\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4528 - val_loss: 0.4510\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4495 - val_loss: 0.4451\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4476 - val_loss: 0.4391\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4356 - val_loss: 0.4347\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4406 - val_loss: 0.4399\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4503 - val_loss: 0.4389\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4325 - val_loss: 0.4267\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4346 - val_loss: 0.4359\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4368 - val_loss: 0.4210\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4250 - val_loss: 0.4463\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4289 - val_loss: 0.4340\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4285 - val_loss: 0.4127\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4154 - val_loss: 0.4395\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4202 - val_loss: 0.4312\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4247 - val_loss: 0.4479\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4156 - val_loss: 0.4472\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4083 - val_loss: 0.4444\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4104 - val_loss: 0.4780\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4096 - val_loss: 0.4417\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4035 - val_loss: 0.4350\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4017 - val_loss: 0.4425\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4053Restoring model weights from the end of the best epoch: 29.\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4053 - val_loss: 0.4452\n",
      "Epoch 39: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 593011.2500 - val_loss: 983378.1250\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 348922.0625 - val_loss: 135394.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 168090.6094 - val_loss: 168508.0312\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 155382.9219 - val_loss: 56024.9102\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 97626.9922 - val_loss: 29273.8867\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 90675.6641 - val_loss: 310650.6875\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 121464.9141 - val_loss: 97503.4766\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 89727.3906 - val_loss: 205793.5469\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 102830.5156 - val_loss: 125835.2031\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 89596.1875 - val_loss: 26944.4570\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 48094.1406 - val_loss: 71482.0859\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 34712.0156 - val_loss: 112340.9531\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 29833.5605 - val_loss: 76765.0781\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 44628.0312 - val_loss: 46694.0273\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 26585.2617 - val_loss: 57780.3945\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 27509.9922 - val_loss: 26538.2539\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 28027.3184 - val_loss: 65900.3438\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 37622.1055 - val_loss: 31800.8242\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 42889.7422 - val_loss: 28435.9082\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 27894.6133 - val_loss: 68472.0781\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 39955.6992 - val_loss: 27615.7539\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 22171.4941 - val_loss: 11253.3584\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 19934.1074 - val_loss: 13350.2881\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 19682.8633 - val_loss: 35022.1406\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8473.3477 - val_loss: 20407.0762\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 15999.2646 - val_loss: 28988.5996\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 25001.3555 - val_loss: 19975.3594\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 23771.3086 - val_loss: 29521.1035\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 20254.8945 - val_loss: 39335.4961\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 13103.2764 - val_loss: 21906.5527\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 29555.3594 - val_loss: 40651.3438\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 26140.9863 - val_loss: 4370.9839\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11361.7725 - val_loss: 27662.9219\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 25473.2949 - val_loss: 12498.0850\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11071.2598 - val_loss: 29109.4492\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14165.0762 - val_loss: 18904.7207\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 22009.8418 - val_loss: 40170.4453\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 14527.7754 - val_loss: 20129.9355\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 19590.8184 - val_loss: 38505.8867\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 32199.6445 - val_loss: 2165.2380\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 11713.1416 - val_loss: 30091.8555\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 14499.5391 - val_loss: 17289.5059\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 16523.6797 - val_loss: 5027.3145\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9583.1885 - val_loss: 12166.3203\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5103.0229 - val_loss: 6506.4395\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 9255.6475 - val_loss: 7414.0381\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 11160.0996 - val_loss: 29903.8242\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 15980.8496 - val_loss: 4018.2327\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 8960.7607 - val_loss: 22992.0371\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 12346.7803Restoring model weights from the end of the best epoch: 40.\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 12346.7803 - val_loss: 2176.1382\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 1026600.7500 - val_loss: 920556.9375\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 355989.0000 - val_loss: 592707.3750\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 228647.5781 - val_loss: 117079.8594\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 91521.6250 - val_loss: 46271.1211\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 93750.5000 - val_loss: 149276.3438\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 61038.0977 - val_loss: 229146.7812\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 86064.6875 - val_loss: 15684.0430\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 68030.8438 - val_loss: 27462.6758\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 75727.4922 - val_loss: 18011.1074\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 48549.8086 - val_loss: 120467.2812\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 39867.9219 - val_loss: 32088.1133\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 138960.7656 - val_loss: 97506.9766\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 79739.2656 - val_loss: 71668.9688\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 53875.8359 - val_loss: 215261.2969\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 66068.6172 - val_loss: 27712.9844\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 35897.5625 - val_loss: 102187.7734\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 32302.9062Restoring model weights from the end of the best epoch: 7.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32302.9062 - val_loss: 22652.4551\n",
      "Epoch 17: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 859129.8750 - val_loss: 419745.5000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 523954.2188 - val_loss: 239718.4219\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 486405.9688 - val_loss: 216872.8281\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 218462.3906 - val_loss: 172914.6875\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 121217.4375 - val_loss: 101324.5000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 91785.9219 - val_loss: 26974.5371\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 48406.5742 - val_loss: 97872.1406\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 47053.6680 - val_loss: 154060.2969\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 54834.2773 - val_loss: 29088.9648\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 31494.4121 - val_loss: 120192.8125\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 79707.2188 - val_loss: 38762.9375\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 27209.8320 - val_loss: 42936.6914\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 41589.5859 - val_loss: 9054.5859\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 34237.5625 - val_loss: 22342.4668\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 32477.5391 - val_loss: 29880.8477\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 34781.3711 - val_loss: 48038.2617\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 17735.9922 - val_loss: 16236.7246\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 19763.3242 - val_loss: 10960.8623\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 17357.2656 - val_loss: 60047.6914\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 28594.4297 - val_loss: 36003.4414\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 19834.2500 - val_loss: 19022.2715\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 24307.6875 - val_loss: 29208.1758\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 24287.2539Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 24287.2539 - val_loss: 24520.1602\n",
      "Epoch 23: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 107ms/step - loss: 804794.1875 - val_loss: 1277445.8750\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 659577.6875 - val_loss: 122912.0938\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 154903.1562 - val_loss: 632135.2500\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 269033.3125 - val_loss: 532129.9375\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 266771.0000 - val_loss: 276318.5625\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 117915.2656 - val_loss: 249983.8281\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 112486.2266 - val_loss: 142802.7969\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 73963.4844 - val_loss: 25873.7070\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 55536.3438 - val_loss: 169643.9062\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 59175.7930 - val_loss: 27016.8398\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 48382.6836 - val_loss: 64883.8906\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 65414.2773 - val_loss: 93917.8984\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 45616.9844 - val_loss: 57984.3945\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 47070.3789 - val_loss: 64132.2617\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 26154.2266 - val_loss: 69821.8906\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 30811.2461 - val_loss: 30807.1797\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 29446.6816 - val_loss: 99663.5000\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 40699.0469 - val_loss: 20982.1328\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 26133.4609 - val_loss: 18310.0781\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 21486.6582 - val_loss: 21895.7930\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 9004.8330 - val_loss: 11417.5830\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 20983.7617 - val_loss: 11582.0166\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 17328.7188 - val_loss: 60923.6406\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 22502.1191 - val_loss: 31359.0762\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 11070.4336 - val_loss: 55068.0742\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 24817.1699 - val_loss: 8865.9863\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 21559.7188 - val_loss: 7759.0640\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 13234.9492 - val_loss: 31968.8730\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 18011.9766 - val_loss: 36350.1016\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 14782.9619 - val_loss: 22983.3809\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 13672.3496 - val_loss: 20807.4277\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 12718.2139 - val_loss: 13369.1748\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 10299.0791 - val_loss: 58376.3086\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 19715.5078 - val_loss: 27064.3789\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 13341.0068 - val_loss: 6051.6479\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 5718.8101 - val_loss: 13365.6475\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 11784.7783 - val_loss: 17898.3223\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 14013.6172 - val_loss: 18938.7207\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 13422.2285 - val_loss: 10505.1816\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 14210.5703 - val_loss: 15466.3242\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 8290.5527 - val_loss: 44489.6406\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 16277.5938 - val_loss: 10225.1504\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 11430.6123 - val_loss: 11970.1904\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 15188.0156 - val_loss: 31822.7871\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 12427.7979Restoring model weights from the end of the best epoch: 35.\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 12427.7979 - val_loss: 8533.0908\n",
      "Epoch 45: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 122ms/step - loss: 759614.0000 - val_loss: 220904.1562\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 254221.6250 - val_loss: 271566.9375\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 174276.7812 - val_loss: 128282.7031\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 119508.5781 - val_loss: 139136.2188\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 100576.2031 - val_loss: 319800.5312\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 156286.4688 - val_loss: 68420.7812\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 74006.2812 - val_loss: 54269.7227\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 144019.8750 - val_loss: 298054.7188\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 117547.3203 - val_loss: 149002.0312\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 56828.3516 - val_loss: 38500.0156\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 38082.2930 - val_loss: 27258.3359\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 31753.0684 - val_loss: 40432.5859\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 24913.5039 - val_loss: 71992.7188\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 31393.9238 - val_loss: 27395.7930\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 33920.2734 - val_loss: 28226.8867\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 20576.5117 - val_loss: 59361.3789\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 23211.2969 - val_loss: 28913.3301\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 22906.0762 - val_loss: 20152.3027\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 13826.0947 - val_loss: 41055.6016\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 20470.3281 - val_loss: 59605.2539\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 16078.1543 - val_loss: 7923.3174\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 20794.0137 - val_loss: 73994.2812\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 28930.7188 - val_loss: 14010.0156\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 16581.6172 - val_loss: 11695.5312\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 17337.1836 - val_loss: 23411.8359\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 17223.0859 - val_loss: 10844.8066\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9496.9473 - val_loss: 20171.4297\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 21066.6934 - val_loss: 30023.3652\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 16388.0234 - val_loss: 15898.2158\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 14300.2666 - val_loss: 45347.2031\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 25223.8223Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 25223.8223 - val_loss: 23375.2539\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 104ms/step - loss: 187.2890 - val_loss: 185.9312\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 184.5387 - val_loss: 184.3928\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 182.7138 - val_loss: 182.0673\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 181.7177 - val_loss: 183.1351\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 181.5569 - val_loss: 182.5340\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 181.4765 - val_loss: 182.2315\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 181.4061 - val_loss: 182.6724\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 181.2037 - val_loss: 181.9009\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 181.2104 - val_loss: 182.9862\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 181.2115 - val_loss: 182.3056\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 181.1116 - val_loss: 181.2829\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 181.2115 - val_loss: 181.6169\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 181.1078 - val_loss: 181.5918\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 181.0889 - val_loss: 181.5229\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 180.9898 - val_loss: 181.6564\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 181.1838 - val_loss: 181.7683\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 181.2464 - val_loss: 181.4891\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 180.9594 - val_loss: 181.6677\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 181.2447 - val_loss: 181.6996\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 181.2068 - val_loss: 181.3245\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 181.2255Restoring model weights from the end of the best epoch: 11.\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 181.2255 - val_loss: 182.3497\n",
      "Epoch 21: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 118ms/step - loss: 188.5901 - val_loss: 191.1567\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 99.4222 - val_loss: 62.1804\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 58.3185 - val_loss: 55.1361\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 50.5108 - val_loss: 49.4387\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 48.1910 - val_loss: 47.7603\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 46.7005 - val_loss: 48.4304\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 46.0945 - val_loss: 49.0701\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 45.8685 - val_loss: 47.9934\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 45.3903 - val_loss: 47.3708\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 45.0018 - val_loss: 47.4179\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.8758 - val_loss: 47.8609\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 45.0725 - val_loss: 47.4349\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 45.1437 - val_loss: 47.0818\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 44.8363 - val_loss: 47.4307\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 44.7629 - val_loss: 47.0371\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 44.5985 - val_loss: 46.9296\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 44.4496 - val_loss: 47.3790\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.4444 - val_loss: 47.7569\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.5124 - val_loss: 47.1860\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.4720 - val_loss: 47.1035\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 44.3062 - val_loss: 47.3071\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 44.2090 - val_loss: 47.3762\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 44.3124 - val_loss: 47.4240\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 44.1796 - val_loss: 46.8484\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.5505 - val_loss: 47.2889\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 44.7843 - val_loss: 47.3738\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.3394 - val_loss: 46.9634\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 44.2250 - val_loss: 47.0090\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 44.1649 - val_loss: 46.9667\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 44.0229 - val_loss: 47.1505\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 44.1713 - val_loss: 47.8834\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 43.9619 - val_loss: 47.1087\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 43.8820 - val_loss: 47.6015\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 44.0786Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 44.0786 - val_loss: 47.0499\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 126ms/step - loss: 85.4599 - val_loss: 56.3685\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 55.1754 - val_loss: 45.1514\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 46.5238 - val_loss: 39.4290\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 42.1371 - val_loss: 36.3686\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 39.2423 - val_loss: 34.7198\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 37.6687 - val_loss: 33.8670\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 36.6048 - val_loss: 33.3995\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 36.0694 - val_loss: 33.6361\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 35.6834 - val_loss: 33.6749\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 35.8923 - val_loss: 33.4917\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 35.3503 - val_loss: 34.5485\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 35.5998 - val_loss: 33.4144\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 35.1800 - val_loss: 32.4677\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 35.1133 - val_loss: 32.0553\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 34.5019 - val_loss: 32.7454\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 34.9778 - val_loss: 33.0410\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 35.1466 - val_loss: 34.1853\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 34.7329 - val_loss: 33.0143\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 35.1505 - val_loss: 32.2813\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 34.6760 - val_loss: 32.8581\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 34.1377 - val_loss: 31.7435\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 34.6892 - val_loss: 32.6400\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 34.2104 - val_loss: 32.3877\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 34.0343 - val_loss: 33.8982\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 33.9225 - val_loss: 33.4124\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 33.9184 - val_loss: 33.2173\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 34.6435 - val_loss: 31.9713\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 33.5975 - val_loss: 32.1506\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 33.6677 - val_loss: 33.1968\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 33.4451 - val_loss: 32.4090\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.8692Restoring model weights from the end of the best epoch: 21.\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 33.8692 - val_loss: 32.7946\n",
      "Epoch 31: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 124ms/step - loss: 92.2340 - val_loss: 66.4940\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 68.0428 - val_loss: 60.2197\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 62.6131 - val_loss: 51.3193\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 54.6735 - val_loss: 44.4343\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 50.0786 - val_loss: 40.2659\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 48.4282 - val_loss: 41.8330\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 46.9329 - val_loss: 37.5740\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 45.8173 - val_loss: 37.8935\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 45.3952 - val_loss: 36.9148\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 45.0763 - val_loss: 36.5699\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 44.5802 - val_loss: 37.5207\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 44.1393 - val_loss: 36.6667\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 44.1651 - val_loss: 37.0065\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 44.8016 - val_loss: 36.0053\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 44.9861 - val_loss: 35.6609\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 44.3977 - val_loss: 36.3370\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 44.3052 - val_loss: 36.9177\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 43.7881 - val_loss: 36.1222\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 43.9137 - val_loss: 37.7213\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 44.6263 - val_loss: 36.5751\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 44.7583 - val_loss: 35.8599\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 43.3358 - val_loss: 44.7904\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 45.8877 - val_loss: 36.4147\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 36.8471 - val_loss: 34.1405\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 36.6195 - val_loss: 34.6769\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 35.6064 - val_loss: 32.6212\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 35.4459 - val_loss: 32.6243\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 35.3516 - val_loss: 32.5117\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 35.0978 - val_loss: 32.3307\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 35.1943 - val_loss: 33.0073\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 34.5364 - val_loss: 32.9935\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 34.4055 - val_loss: 32.8358\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 34.4114 - val_loss: 33.2357\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 34.7472 - val_loss: 34.6442\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 35.7840 - val_loss: 32.4767\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 34.1780 - val_loss: 32.2136\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 34.8722 - val_loss: 31.8571\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 34.7243 - val_loss: 32.0428\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 34.3963 - val_loss: 31.4281\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 34.1969 - val_loss: 31.2728\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 34.2942 - val_loss: 32.9528\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 34.3947 - val_loss: 33.8127\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 34.8720 - val_loss: 32.3386\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 34.0730 - val_loss: 32.3499\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 35.0543 - val_loss: 33.3333\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 33.9500 - val_loss: 32.5331\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 33.8874 - val_loss: 32.0010\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 33.8442 - val_loss: 33.0800\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 34.1011 - val_loss: 32.0593\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.8948Restoring model weights from the end of the best epoch: 40.\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 33.8948 - val_loss: 33.2333\n",
      "Epoch 50: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 134ms/step - loss: 85.3250 - val_loss: 57.0826\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 58.4663 - val_loss: 45.1646\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 47.8102 - val_loss: 40.8003\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 43.5257 - val_loss: 40.8301\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 10s 130ms/step - loss: 40.5171 - val_loss: 35.1842\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 38.4675 - val_loss: 35.3319\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 37.4441 - val_loss: 34.3375\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 36.6277 - val_loss: 34.7476\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 36.1621 - val_loss: 33.0699\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 35.6367 - val_loss: 34.3400\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 35.5049 - val_loss: 33.1501\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 35.2156 - val_loss: 32.2484\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 35.6462 - val_loss: 32.4455\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 35.1992 - val_loss: 32.6233\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 35.0754 - val_loss: 31.9496\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 35.1717 - val_loss: 32.6279\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 34.6972 - val_loss: 32.5893\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 34.7411 - val_loss: 32.4289\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 34.4664 - val_loss: 33.1850\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 34.4030 - val_loss: 32.3079\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 34.8891 - val_loss: 33.2217\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 34.4865 - val_loss: 32.0040\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 34.5241 - val_loss: 32.4496\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 34.2950 - val_loss: 32.3531\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 33.8444Restoring model weights from the end of the best epoch: 15.\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 33.8444 - val_loss: 32.2059\n",
      "Epoch 25: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 122ms/step - loss: 8.7632 - val_loss: 5.7577\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 5.4172 - val_loss: 4.0238\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 4.0115 - val_loss: 3.1026\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 3.2494 - val_loss: 2.7493\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 2.9418 - val_loss: 2.5635\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 2.7210 - val_loss: 2.3827\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 2.6271 - val_loss: 2.4403\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 2.5921 - val_loss: 2.6649\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.5683 - val_loss: 2.3581\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 2.5458 - val_loss: 2.5152\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.4668 - val_loss: 2.3778\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4656 - val_loss: 2.3511\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4299 - val_loss: 2.3691\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.4753 - val_loss: 2.4095\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 2.4289 - val_loss: 2.4063\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 2.4345 - val_loss: 2.3214\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4127 - val_loss: 2.3484\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3955 - val_loss: 2.4472\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4063 - val_loss: 2.3341\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3804 - val_loss: 2.3139\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3806 - val_loss: 2.3646\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 2.3709 - val_loss: 2.4870\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 2.3273 - val_loss: 2.3458\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 2.3021 - val_loss: 2.3008\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 2.3070 - val_loss: 2.3146\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2964 - val_loss: 2.3523\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.3073 - val_loss: 2.2446\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2938 - val_loss: 2.2858\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 2.2976 - val_loss: 2.5186\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.2382 - val_loss: 2.3953\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.2788 - val_loss: 2.3818\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.2731 - val_loss: 2.3599\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2743 - val_loss: 2.2752\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2902 - val_loss: 2.2636\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.1879 - val_loss: 2.3348\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.1733 - val_loss: 2.4184\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1633Restoring model weights from the end of the best epoch: 27.\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.1633 - val_loss: 2.2880\n",
      "Epoch 37: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 125ms/step - loss: 9.0052 - val_loss: 5.9388\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 5.5988 - val_loss: 4.3313\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 4.0692 - val_loss: 3.1005\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 3.2598 - val_loss: 2.6741\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.9709 - val_loss: 2.5206\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.7116 - val_loss: 2.3716\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 2.6275 - val_loss: 2.5249\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.5630 - val_loss: 2.3383\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 2.5389 - val_loss: 2.3795\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.5240 - val_loss: 2.6626\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.5187 - val_loss: 2.3780\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.5147 - val_loss: 2.3421\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 2.4630 - val_loss: 2.4550\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 2.5153 - val_loss: 2.5090\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 2.4438 - val_loss: 2.4785\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.4131 - val_loss: 2.3485\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 2.4019 - val_loss: 2.2708\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.3763 - val_loss: 2.2958\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 2.3361 - val_loss: 2.3883\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.4136 - val_loss: 2.3517\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3182 - val_loss: 2.2195\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2970 - val_loss: 2.2927\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2867 - val_loss: 2.3180\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 2.2919 - val_loss: 2.2397\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 2.2824 - val_loss: 2.2165\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 2.3108 - val_loss: 2.2371\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2229 - val_loss: 2.2541\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 2.2433 - val_loss: 2.2217\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 2.2046 - val_loss: 2.2785\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.2316 - val_loss: 2.2694\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 2.1630 - val_loss: 2.6438\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.2061 - val_loss: 2.2278\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.1603 - val_loss: 2.3499\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 2.1721 - val_loss: 2.2828\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1365Restoring model weights from the end of the best epoch: 25.\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 2.1365 - val_loss: 2.3614\n",
      "Epoch 35: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 118ms/step - loss: 8.7882 - val_loss: 5.9735\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 5.6881 - val_loss: 4.4548\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 4.2096 - val_loss: 3.2362\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 3.3559 - val_loss: 2.7415\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.9984 - val_loss: 2.5281\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 2.7684 - val_loss: 2.4239\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 2.6000 - val_loss: 2.3939\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 2.6151 - val_loss: 2.4196\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 2.5723 - val_loss: 2.3813\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.5063 - val_loss: 2.3400\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4802 - val_loss: 2.4501\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 2.5206 - val_loss: 2.3243\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 2.4629 - val_loss: 2.5629\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4406 - val_loss: 2.4425\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.4582 - val_loss: 2.3724\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 2.4459 - val_loss: 2.3496\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3993 - val_loss: 2.3584\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.4055 - val_loss: 2.3001\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 2.3978 - val_loss: 2.2873\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3860 - val_loss: 2.3033\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3650 - val_loss: 2.3402\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 2.3401 - val_loss: 2.3443\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 2.3694 - val_loss: 2.2618\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 2.3354 - val_loss: 2.4326\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 2.3365 - val_loss: 2.4469\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 2.3344 - val_loss: 2.3120\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.3022 - val_loss: 2.3739\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.2853 - val_loss: 2.3069\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 2.2932 - val_loss: 2.3039\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 2.2501 - val_loss: 2.4020\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 2.2906 - val_loss: 2.3696\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 2.2202 - val_loss: 2.3957\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.2365Restoring model weights from the end of the best epoch: 23.\n",
      "73/73 [==============================] - 7s 103ms/step - loss: 2.2365 - val_loss: 2.2818\n",
      "Epoch 33: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 95ms/step - loss: 8.2652 - val_loss: 5.2358\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.9170 - val_loss: 3.7871\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 3.7232 - val_loss: 2.8807\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 3.0404 - val_loss: 2.5286\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.7812 - val_loss: 2.5094\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.6555 - val_loss: 2.8509\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.7082 - val_loss: 2.5863\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.5407 - val_loss: 2.3610\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.5094 - val_loss: 2.3705\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 2.4871 - val_loss: 2.5404\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.5268 - val_loss: 2.4934\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.4906 - val_loss: 2.3271\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.4472 - val_loss: 2.4007\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.4406 - val_loss: 2.3181\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 2.4797 - val_loss: 2.8043\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.4219 - val_loss: 2.3803\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.4005 - val_loss: 2.3817\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 2.3778 - val_loss: 2.3163\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 2.3493 - val_loss: 2.4397\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3658 - val_loss: 2.3222\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 2.3985 - val_loss: 2.2990\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 2.3716 - val_loss: 2.2654\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 2.3297 - val_loss: 2.5269\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2981 - val_loss: 2.3218\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2536 - val_loss: 2.3049\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 2.3239 - val_loss: 2.3020\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 2.2777 - val_loss: 2.3986\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 2.2581 - val_loss: 2.2412\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.2180 - val_loss: 2.3279\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 2.2929 - val_loss: 2.2452\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 2.2486 - val_loss: 2.3007\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2169 - val_loss: 2.2897\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2118 - val_loss: 2.4062\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2029 - val_loss: 2.3038\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.1483 - val_loss: 2.3170\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 2.2020 - val_loss: 2.2719\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.1934 - val_loss: 2.2736\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1458Restoring model weights from the end of the best epoch: 28.\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.1458 - val_loss: 2.2672\n",
      "Epoch 38: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 8.5363 - val_loss: 5.5124\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 5.1477 - val_loss: 3.8075\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 3.8130 - val_loss: 3.0188\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 3.1134 - val_loss: 2.6642\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.8119 - val_loss: 2.5839\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.6948 - val_loss: 2.4559\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.5953 - val_loss: 2.4201\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.5811 - val_loss: 2.4099\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.5759 - val_loss: 2.5103\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.5086 - val_loss: 2.3624\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.4865 - val_loss: 2.3319\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 2.5011 - val_loss: 2.3560\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.4105 - val_loss: 2.2976\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.4265 - val_loss: 2.4230\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.4875 - val_loss: 2.3773\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.4838 - val_loss: 2.3201\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3960 - val_loss: 2.5669\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.4593 - val_loss: 2.4178\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3641 - val_loss: 2.2971\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3571 - val_loss: 2.3013\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 2.4092 - val_loss: 2.2833\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 2.3342 - val_loss: 2.3015\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 2.3475 - val_loss: 2.3160\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.3172 - val_loss: 2.2441\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3312 - val_loss: 2.2639\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2802 - val_loss: 2.3281\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 2.2993 - val_loss: 2.3846\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.3354 - val_loss: 2.2679\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 2.2532 - val_loss: 2.3441\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.1973 - val_loss: 2.2973\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 2.3223 - val_loss: 2.3569\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.3194 - val_loss: 2.3297\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.3480 - val_loss: 2.2974\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.3320 - val_loss: 2.2064\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2728 - val_loss: 2.2985\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 2.2515 - val_loss: 2.4571\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 2.2274 - val_loss: 2.3197\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 2.2396 - val_loss: 2.2520\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 2.2225 - val_loss: 2.2770\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 2.1763 - val_loss: 2.3007\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 2.1926 - val_loss: 2.3492\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.2577 - val_loss: 2.2878\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 2.1994 - val_loss: 2.2331\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 2.1948Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 2.1948 - val_loss: 2.3503\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 114.6789 - val_loss: 53.4519\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 50.8641 - val_loss: 30.6106\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 32.0752 - val_loss: 20.1341\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 21.9743 - val_loss: 15.5829\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 17.5609 - val_loss: 13.6569\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 15.2479 - val_loss: 11.6810\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 13.6277 - val_loss: 12.0878\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 13.1563 - val_loss: 11.5090\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 12.2687 - val_loss: 10.7161\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.6707 - val_loss: 10.8774\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.3820 - val_loss: 10.5120\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.6056 - val_loss: 10.2406\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.0904 - val_loss: 11.0462\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.9299 - val_loss: 9.7877\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.7717 - val_loss: 10.0432\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.4347 - val_loss: 9.7605\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.2031 - val_loss: 9.3041\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.4595 - val_loss: 9.7537\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.9108 - val_loss: 9.6963\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.8617 - val_loss: 10.0034\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.9280 - val_loss: 10.2208\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.8282 - val_loss: 11.3435\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 10.2753 - val_loss: 9.5761\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.6080 - val_loss: 9.1156\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.3088 - val_loss: 9.5926\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.3835 - val_loss: 9.4930\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.3168 - val_loss: 10.1066\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.7067 - val_loss: 9.6177\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.3544 - val_loss: 9.3673\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.1517 - val_loss: 9.3891\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.0830 - val_loss: 9.7577\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.2241 - val_loss: 9.1537\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.7939 - val_loss: 9.1981\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.2152Restoring model weights from the end of the best epoch: 24.\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.2152 - val_loss: 9.4458\n",
      "Epoch 34: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 98ms/step - loss: 133.9252 - val_loss: 64.6684\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 60.1937 - val_loss: 36.5621\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 36.7955 - val_loss: 24.2195\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 25.0735 - val_loss: 17.2722\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 19.3994 - val_loss: 13.8449\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 16.1657 - val_loss: 12.0604\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 14.5471 - val_loss: 11.6633\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 13.7055 - val_loss: 11.5178\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 12.8477 - val_loss: 11.1174\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 12.4522 - val_loss: 10.7620\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.8071 - val_loss: 10.5352\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.7607 - val_loss: 10.5690\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.5854 - val_loss: 10.6227\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 11.0710 - val_loss: 10.3070\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.9625 - val_loss: 11.2093\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.8839 - val_loss: 11.7242\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.5011 - val_loss: 10.2512\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.6794 - val_loss: 10.1450\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.1330 - val_loss: 9.7093\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2372 - val_loss: 10.3867\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.9692 - val_loss: 9.7938\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.2254 - val_loss: 10.4231\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.2154 - val_loss: 10.3270\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.0370 - val_loss: 9.4246\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 10.3067 - val_loss: 10.1188\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.7669 - val_loss: 9.4725\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.4174 - val_loss: 9.3555\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1936 - val_loss: 10.2083\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.8622 - val_loss: 11.1198\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1673 - val_loss: 9.8617\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.1770 - val_loss: 10.3298\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.2262 - val_loss: 9.3309\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.1762 - val_loss: 9.7752\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.0876 - val_loss: 10.0462\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0033 - val_loss: 10.3093\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.7201 - val_loss: 9.3303\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.3497 - val_loss: 10.3846\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.2445 - val_loss: 11.0256\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.7423 - val_loss: 9.4756\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.4982 - val_loss: 9.9032\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.2999 - val_loss: 10.0266\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.5731 - val_loss: 9.9276\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.0374 - val_loss: 9.6524\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.9833 - val_loss: 10.6974\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.9120 - val_loss: 9.4582\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.1228Restoring model weights from the end of the best epoch: 36.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 9.1228 - val_loss: 12.0932\n",
      "Epoch 46: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 93ms/step - loss: 124.2434 - val_loss: 50.7359\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 46.1493 - val_loss: 26.9823\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 29.2274 - val_loss: 18.5892\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 20.7754 - val_loss: 15.1538\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 16.8086 - val_loss: 12.4730\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 14.5059 - val_loss: 11.5462\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 13.3672 - val_loss: 11.2397\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 12.6998 - val_loss: 11.5089\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 12.0670 - val_loss: 11.3365\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 12.3006 - val_loss: 11.3153\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 11.7092 - val_loss: 13.3468\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 11.4858 - val_loss: 10.5937\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 11.5553 - val_loss: 11.1947\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 11.3236 - val_loss: 10.1807\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.9065 - val_loss: 10.3784\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.8374 - val_loss: 10.0312\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.7139 - val_loss: 9.8795\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 11.1039 - val_loss: 10.4909\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 11.4459 - val_loss: 13.9584\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 11.1496 - val_loss: 10.5033\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.8999 - val_loss: 11.1435\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.7892 - val_loss: 9.4097\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.1440 - val_loss: 9.5778\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.9997 - val_loss: 10.1094\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 9.8866 - val_loss: 10.2846\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 9.7454 - val_loss: 9.5685\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 9.6810 - val_loss: 9.9461\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 9.3784 - val_loss: 11.1992\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.9319 - val_loss: 9.5491\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 9.3907 - val_loss: 11.3697\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 9.2640 - val_loss: 9.6269\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 9.5673Restoring model weights from the end of the best epoch: 22.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 9.5673 - val_loss: 10.2676\n",
      "Epoch 32: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 119.0545 - val_loss: 52.9305\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 49.3300 - val_loss: 29.5159\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 30.7505 - val_loss: 20.4120\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 22.1665 - val_loss: 15.4129\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 17.3721 - val_loss: 12.7962\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 14.7949 - val_loss: 12.0345\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 13.5952 - val_loss: 11.3814\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 13.1408 - val_loss: 10.4427\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 12.3419 - val_loss: 10.6657\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 11.9034 - val_loss: 12.8089\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.4935 - val_loss: 10.6042\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 11.1957 - val_loss: 10.7031\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.2457 - val_loss: 10.8430\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 11.3797 - val_loss: 10.2849\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 11.1339 - val_loss: 11.1309\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 11.0534 - val_loss: 9.9966\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 10.7810 - val_loss: 9.5482\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 10.6945 - val_loss: 10.5303\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 10.7694 - val_loss: 9.8672\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 10.3841 - val_loss: 9.8768\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 10.5611 - val_loss: 10.8002\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 10.4692 - val_loss: 10.0124\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 10.4743 - val_loss: 9.7734\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 10.3441 - val_loss: 10.9061\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 9.9287 - val_loss: 10.1364\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 10.1220 - val_loss: 9.3506\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 9.9229 - val_loss: 9.8596\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 10.2124 - val_loss: 10.9410\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.7949 - val_loss: 10.4701\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.7721 - val_loss: 10.0557\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.4688 - val_loss: 9.5652\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.3576 - val_loss: 9.4509\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.1782 - val_loss: 10.2746\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 9.2467 - val_loss: 9.5630\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 9.3530 - val_loss: 9.2634\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.0633 - val_loss: 10.1105\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 9.0677 - val_loss: 9.9502\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 8.9540 - val_loss: 10.6851\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 8.7130 - val_loss: 9.8538\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 8.6261 - val_loss: 9.5274\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 8.6597 - val_loss: 9.8115\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 8.6917 - val_loss: 9.8733\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 8.8200 - val_loss: 10.3778\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 8.6189 - val_loss: 9.2948\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.6016Restoring model weights from the end of the best epoch: 35.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.6016 - val_loss: 9.9352\n",
      "Epoch 45: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 113ms/step - loss: 133.3540 - val_loss: 60.5337\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 56.4552 - val_loss: 34.1271\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 34.7165 - val_loss: 22.2139\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 24.2807 - val_loss: 16.5041\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 18.7519 - val_loss: 13.0907\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 15.8150 - val_loss: 11.6551\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 14.0108 - val_loss: 12.0693\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 13.4273 - val_loss: 11.9184\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 12.6525 - val_loss: 10.8083\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 12.4099 - val_loss: 10.4563\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 11.6030 - val_loss: 10.1745\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 11.4056 - val_loss: 11.2925\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 12.5883 - val_loss: 10.1428\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 11.4749 - val_loss: 10.2683\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 11.1031 - val_loss: 10.3858\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 10.6370 - val_loss: 9.6682\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 10.6407 - val_loss: 11.2269\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 10.3330 - val_loss: 9.8332\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 10.1099 - val_loss: 10.0155\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 10.1459 - val_loss: 9.8133\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 9.8420 - val_loss: 9.9133\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.6662 - val_loss: 9.9415\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 9.4773 - val_loss: 9.4533\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.6269 - val_loss: 9.3444\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 9.2735 - val_loss: 9.0349\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.6374 - val_loss: 10.2809\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.7258 - val_loss: 10.9005\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.7972 - val_loss: 10.2936\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 9.8224 - val_loss: 9.1448\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 9.2112 - val_loss: 9.4560\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.8875 - val_loss: 8.9286\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 8.6919 - val_loss: 9.8451\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.3672 - val_loss: 9.2339\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 8.5476 - val_loss: 8.8313\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 8.6983 - val_loss: 9.6881\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 8.5521 - val_loss: 9.3857\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.4982 - val_loss: 9.9166\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 10.7482 - val_loss: 10.1730\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 10.3604 - val_loss: 10.1583\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 9.5599 - val_loss: 10.8528\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 9.1832 - val_loss: 9.5750\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 8.6462 - val_loss: 9.2509\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 8.4023 - val_loss: 10.8154\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 8.0980Restoring model weights from the end of the best epoch: 34.\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 8.0980 - val_loss: 10.6797\n",
      "Epoch 44: early stopping\n",
      "'########################################################Model4\n"
     ]
    }
   ],
   "source": [
    "model_num = 5\n",
    "\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.001)\n",
    "mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.001)\n",
    "mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 33ms/step\n",
      "12/12 [==============================] - 0s 32ms/step\n",
      "12/12 [==============================] - 1s 33ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 29ms/step\n",
      "12/12 [==============================] - 1s 28ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 1s 26ms/step\n",
      "12/12 [==============================] - 1s 24ms/step\n",
      "12/12 [==============================] - 1s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 25ms/step\n",
      "12/12 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, test_X)\n",
    "mase_predictions = bagging_predict2(pred2, test_X)\n",
    "mape_predictions = bagging_predict2(pred3, test_X)\n",
    "mae_predictions = bagging_predict2(pred4, test_X)\n",
    "mse_predictions = bagging_predict2(pred5, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed0c7ce-ad6f-47e0-b18a-7e5a1e0a7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(fin_pred.reshape(-1,24)).to_csv(\"pred/lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93fd3e88-6523-4042-a996-ad1f65d9fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.1598744, 2.3276055)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([smape_predictions, mase_predictions,mape_predictions,mae_predictions,mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "pd.DataFrame(fin_pred.flatten()).to_csv(\"../result7/lstm.csv\")\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce6054d-bd3d-4981-90dc-91c0ab363196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.1570232, 2.3085225)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([mae_predictions, mase_predictions,mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "pd.DataFrame(fin_pred.flatten()).to_csv(\"../best7/LSTM.csv\")\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6a62d0-8e46-487c-a3a5-b525b4d8f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2115781, 2.3864274)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([smape_predictions, mase_predictions,mape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1291c905-5ea5-4b7f-9852-aed9fb1b361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.3362625, 2.4487581)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([smape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccfe8c32-66e3-41f3-81d4-fdf27365d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.028303, 15.428723)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f869349b-d29e-489f-97bc-d9b6df52b835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.172588, 2.3089058)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mase_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e5f77bb-cc32-44a1-8384-6ff5cb753cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2045054, 2.335651)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mae_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0678ca-6617-4379-bc86-4b882056dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2006576, 2.3652706)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30cdd8e4-0d65-4983-ad5c-5773907577b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.15889\n"
     ]
    }
   ],
   "source": [
    "concat_mase = np.concatenate([np.nan_to_num(np.array(mase_predictions), nan=0)])\n",
    "fin_pred_mase = np.median(concat_mase,axis=0)\n",
    "MASE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mase .flatten())).round(5)\n",
    "\n",
    "concat_mape = np.concatenate([np.nan_to_num(np.array(mape_predictions), nan=0)])\n",
    "fin_pred_mape = np.median(concat_mape,axis=0)\n",
    "MAPE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mape .flatten())).round(5)\n",
    "\n",
    "concat_smape = np.concatenate([np.nan_to_num(np.array(smape_predictions), nan=0)])\n",
    "fin_pred_smape = np.median(concat_smape,axis=0)\n",
    "sMAPE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_smape .flatten())).round(5)\n",
    "\n",
    "concat_mae = np.concatenate([np.nan_to_num(np.array(mae_predictions), nan=0)])\n",
    "fin_pred_mae = np.median(concat_mae,axis=0)\n",
    "MAE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mae .flatten())).round(5)\n",
    "\n",
    "concat_mse = np.concatenate([np.nan_to_num(np.array(mse_predictions), nan=0)])\n",
    "fin_pred_mse = np.median(concat_mse,axis=0)\n",
    "MSE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mse .flatten())).round(5)\n",
    "\n",
    "performance = np.array([MASE, MAPE,sMAPE,MAE,MSE])\n",
    "beta = 10  # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([fin_pred_mase.flatten().reshape(1,-1),\n",
    "                    fin_pred_mape.flatten().reshape(1,-1),\n",
    "                   fin_pred_smape.flatten().reshape(1,-1),\n",
    "                   fin_pred_mae.flatten().reshape(1,-1),\n",
    "                   fin_pred_mse.flatten().reshape(1,-1)],axis=0)\n",
    "\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd)\n",
    "print(np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction)).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a48d539f-953b-4d30-9c12-9b1195de25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ensemble_prediction).to_csv(\"../exp7/vanila_LSTM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80efe85-f798-42c9-b7c2-9756f51cf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fin_pred.reshape(-1,24)).to_csv(\"../result7_new/LSTM/pred_mid.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat[i].reshape(-1,24)).to_csv(f\"../result7_new/LSTM/pred{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
