{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526bc5-cd1a-4fac-a796-57902266c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636876fb-747b-4480-8ea0-8ff0618bd573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 16:57:15.808487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 16:57:15.964472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-11 16:57:15.964538: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-11 16:57:16.643953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-11 16:57:16.644017: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-11 16:57:16.644024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850e1af-7ef4-47f2-b130-6732c47014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X= pd.read_csv(\"../data/ele_train_input_7.csv\").iloc[:,1:].values.astype(np.float32) / 10000\n",
    "target_y =pd.read_csv(\"../data/ele_train_output_7.csv\").iloc[:,1:].values.astype(np.float32)/ 10000\n",
    "test_X= pd.read_csv(\"../data/ele_val_input_7.csv\").iloc[:,1:].values.astype(np.float32)/ 10000\n",
    "test_y =pd.read_csv(\"../data/ele_val_output_7.csv\").iloc[:,1:].values.astype(np.float32)/ 10000\n",
    "X_train=target_X.astype(np.float32)\n",
    "y_train=target_y.astype(np.float32)\n",
    "#y_train.astype(np.float32)\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3aff672-3e0d-4a08-9d95-0a0eacbdbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1],1,y_train.shape[1]\n",
    "\n",
    "#################################################################################\n",
    "# nbeats 모델 생성 함수\n",
    "def build_model(input_timesteps,features,output_timesteps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(input_timesteps, features)))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    #model.add(LSTM(unit, return_sequences=True))\n",
    "    # Use Lambda layer to select the last 'output_timesteps' outputs\n",
    "    model.add(Lambda(lambda x: x[:, -24:, :]))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "#################################################################################\n",
    "# 부트스트랩 샘플링\n",
    "# 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    input_timesteps,features,output_timesteps= hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = build_model(input_timesteps,features,output_timesteps)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #X_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "# SMAPE 용\n",
    "def train_bagging_models_smape(num_models, loss_fn , epochs_, patience_,batch_size_):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        #select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        #_bootstrap = X_train[select]\n",
    "        #y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 1, restore_best_weights=True)\n",
    "        history = model.fit(X_train, y_train, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=1, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9eb03-7500-4d76-af24-72f76cc67df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 16:57:23.348076: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-11 16:57:23.348132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ymlee2-desktop): /proc/driver/nvidia/version does not exist\n",
      "2024-10-11 16:57:23.349580: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 111ms/step - loss: 4.1611 - val_loss: 1.1223\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.9187 - val_loss: 0.9334\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.8938 - val_loss: 0.9168\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.8854 - val_loss: 0.9246\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.8619 - val_loss: 0.8749\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.8300 - val_loss: 0.8174\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.8012 - val_loss: 0.8040\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.7636 - val_loss: 0.7397\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.7156 - val_loss: 0.6704\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 0.6507 - val_loss: 0.6483\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.6390 - val_loss: 0.5995\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.6011 - val_loss: 0.5872\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5855 - val_loss: 0.5704\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.5773 - val_loss: 0.5657\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.5682 - val_loss: 0.5697\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5732 - val_loss: 0.5784\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5609 - val_loss: 0.5815\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5588 - val_loss: 0.5553\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.5496 - val_loss: 0.5334\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5408 - val_loss: 0.5261\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.5355 - val_loss: 0.5283\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5262 - val_loss: 0.5309\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5236 - val_loss: 0.5083\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.5074 - val_loss: 0.5024\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.5021 - val_loss: 0.4922\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4886 - val_loss: 0.5166\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4938 - val_loss: 0.4781\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4742 - val_loss: 0.4714\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4582 - val_loss: 0.4498\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4414 - val_loss: 0.4109\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.4115 - val_loss: 0.4568\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.4026 - val_loss: 0.3841\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3723 - val_loss: 0.3917\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3706 - val_loss: 0.3464\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3389 - val_loss: 0.3708\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.3438 - val_loss: 0.3556\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.3318 - val_loss: 0.3266\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3281 - val_loss: 0.3314\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3256 - val_loss: 0.3370\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.3193 - val_loss: 0.3271\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.3166 - val_loss: 0.3116\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 0.3061 - val_loss: 0.3103\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3104 - val_loss: 0.3411\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.3032 - val_loss: 0.3097\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2957 - val_loss: 0.3026\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 0.2931 - val_loss: 0.3512\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3047 - val_loss: 0.3069\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.2867 - val_loss: 0.2887\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.2945 - val_loss: 0.3047\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 0.2812 - val_loss: 0.2987\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2842 - val_loss: 0.3087\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2831 - val_loss: 0.3134\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2899 - val_loss: 0.2908\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2845 - val_loss: 0.2893\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2723 - val_loss: 0.2769\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2771 - val_loss: 0.3033\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2708 - val_loss: 0.2786\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2695 - val_loss: 0.2825\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2704 - val_loss: 0.3010\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2608 - val_loss: 0.2863\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2642 - val_loss: 0.2820\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2606 - val_loss: 0.2870\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.2562 - val_loss: 0.2722\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2630 - val_loss: 0.2848\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2534 - val_loss: 0.2748\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2596 - val_loss: 0.2678\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.2507 - val_loss: 0.2858\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2600 - val_loss: 0.2667\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2538 - val_loss: 0.2752\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2543 - val_loss: 0.3031\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2552 - val_loss: 0.2734\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2612 - val_loss: 0.2648\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.2505 - val_loss: 0.2663\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2481 - val_loss: 0.2739\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2492 - val_loss: 0.2876\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2556 - val_loss: 0.2633\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2466 - val_loss: 0.2602\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2515 - val_loss: 0.2703\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2410 - val_loss: 0.2599\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2421 - val_loss: 0.2762\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2486 - val_loss: 0.2600\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2545 - val_loss: 0.2582\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2483 - val_loss: 0.2815\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2487 - val_loss: 0.2531\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2483 - val_loss: 0.2952\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2397 - val_loss: 0.2600\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2405 - val_loss: 0.2553\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2371 - val_loss: 0.2730\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.2424 - val_loss: 0.2757\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2361 - val_loss: 0.2497\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2408 - val_loss: 0.2593\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.2383 - val_loss: 0.2685\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.2429 - val_loss: 0.2714\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2398 - val_loss: 0.2526\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2446 - val_loss: 0.2518\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.2335 - val_loss: 0.2543\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2381 - val_loss: 0.2688\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2389 - val_loss: 0.2619\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.2339 - val_loss: 0.2619\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2341Restoring model weights from the end of the best epoch: 90.\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.2341 - val_loss: 0.2639\n",
      "Epoch 100: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 97ms/step - loss: 4.9268 - val_loss: 1.4005\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.9553 - val_loss: 0.9477\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.9033 - val_loss: 0.9348\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8900 - val_loss: 0.9098\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8680 - val_loss: 0.8676\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.8324 - val_loss: 0.8283\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8035 - val_loss: 0.7849\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7660 - val_loss: 0.7437\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7268 - val_loss: 0.7022\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6742 - val_loss: 0.6327\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.6247 - val_loss: 0.6651\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.6135 - val_loss: 0.5888\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6042 - val_loss: 0.5784\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5827 - val_loss: 0.5728\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.5824 - val_loss: 0.5676\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.5638 - val_loss: 0.5775\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5585 - val_loss: 0.5403\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5593 - val_loss: 0.5517\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5414 - val_loss: 0.5347\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5312 - val_loss: 0.5704\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5313 - val_loss: 0.4849\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4888 - val_loss: 0.4646\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4945 - val_loss: 0.5915\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4993 - val_loss: 0.4587\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4735 - val_loss: 0.5363\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5068 - val_loss: 0.4652\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4750 - val_loss: 0.4449\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4445 - val_loss: 0.4335\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4623 - val_loss: 0.4457\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4450 - val_loss: 0.4327\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4396 - val_loss: 0.4195\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4251 - val_loss: 0.4380\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4213 - val_loss: 0.4194\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4432 - val_loss: 0.4749\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4194 - val_loss: 0.4049\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4004 - val_loss: 0.4394\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4371 - val_loss: 0.4113\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4052 - val_loss: 0.3870\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3893 - val_loss: 0.4230\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4648 - val_loss: 0.5630\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.5492 - val_loss: 0.5084\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5068 - val_loss: 0.4943\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4979 - val_loss: 0.4861\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4861 - val_loss: 0.4918\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4846 - val_loss: 0.4655\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4794 - val_loss: 0.4803\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4692 - val_loss: 0.4555\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4620Restoring model weights from the end of the best epoch: 38.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4620 - val_loss: 0.4420\n",
      "Epoch 48: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 105ms/step - loss: 4.5117 - val_loss: 1.2905\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.9343 - val_loss: 0.9391\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.8989 - val_loss: 0.9220\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8841 - val_loss: 0.8972\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.8562 - val_loss: 0.8631\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.8345 - val_loss: 0.8228\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.8021 - val_loss: 0.7980\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.7701 - val_loss: 0.7582\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7285 - val_loss: 0.7082\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.6668 - val_loss: 0.6277\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.6307 - val_loss: 0.6014\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.6037 - val_loss: 0.5960\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.6019 - val_loss: 0.5880\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5905 - val_loss: 0.5707\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5859 - val_loss: 0.5672\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5624 - val_loss: 0.5499\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5559 - val_loss: 0.5388\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5437 - val_loss: 0.5421\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5272 - val_loss: 0.5594\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.5256 - val_loss: 0.5502\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5289 - val_loss: 0.5969\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5245 - val_loss: 0.6057\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5323 - val_loss: 0.5061\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4852 - val_loss: 0.4687\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4763 - val_loss: 0.4604\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4429 - val_loss: 0.4906\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4575 - val_loss: 0.4495\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4351 - val_loss: 0.5427\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5314 - val_loss: 0.4745\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4528 - val_loss: 0.4435\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4334 - val_loss: 0.4549\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4278 - val_loss: 0.3980\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4639 - val_loss: 0.4241\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4252 - val_loss: 0.4093\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4554 - val_loss: 0.4854\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4634 - val_loss: 0.4174\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4009 - val_loss: 0.4279\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3842 - val_loss: 0.3951\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3744 - val_loss: 0.3932\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3666 - val_loss: 0.3523\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3481 - val_loss: 0.3731\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3439 - val_loss: 0.3453\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.4444 - val_loss: 0.4870\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5250 - val_loss: 0.5195\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4909 - val_loss: 0.4786\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4860 - val_loss: 0.4941\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.4827 - val_loss: 0.4490\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4743 - val_loss: 0.4801\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4670 - val_loss: 0.4366\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4425 - val_loss: 0.4138\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4237 - val_loss: 0.4145\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.4022Restoring model weights from the end of the best epoch: 42.\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.4022 - val_loss: 0.3875\n",
      "Epoch 52: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 10s 109ms/step - loss: 3.7115 - val_loss: 0.9804\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.9137 - val_loss: 0.9334\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.8977 - val_loss: 0.9119\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.8797 - val_loss: 0.8914\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.8621 - val_loss: 0.8777\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.8459 - val_loss: 0.8359\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.8213 - val_loss: 0.8254\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.8063 - val_loss: 0.7922\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7759 - val_loss: 0.7792\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.7390 - val_loss: 0.7099\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.6859 - val_loss: 0.6320\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.6327 - val_loss: 0.6191\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.6156 - val_loss: 0.5863\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5975 - val_loss: 0.5748\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5925 - val_loss: 0.5960\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.5821 - val_loss: 0.5584\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.5644 - val_loss: 0.6627\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5759 - val_loss: 0.5728\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5540 - val_loss: 0.5455\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.5429 - val_loss: 0.5388\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.5319 - val_loss: 0.5267\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.5334 - val_loss: 0.5128\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.5097 - val_loss: 0.5206\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.4872 - val_loss: 0.4622\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4967 - val_loss: 0.4644\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4451 - val_loss: 0.4650\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.4321 - val_loss: 0.4424\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3997 - val_loss: 0.4384\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3900 - val_loss: 0.4436\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.3789 - val_loss: 0.4070\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3751 - val_loss: 0.3978\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.3713 - val_loss: 0.4106\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.3643 - val_loss: 0.3844\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3681 - val_loss: 0.3911\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3730 - val_loss: 0.3968\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.3594 - val_loss: 0.4369\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3511 - val_loss: 0.4336\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3629 - val_loss: 0.3834\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 0.3512 - val_loss: 0.3642\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3301 - val_loss: 0.3659\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3306 - val_loss: 0.3493\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3264 - val_loss: 0.3627\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3280 - val_loss: 0.3699\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.3204 - val_loss: 0.3553\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3247 - val_loss: 0.3682\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3173 - val_loss: 0.3516\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.3109 - val_loss: 0.3294\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3220 - val_loss: 0.3411\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3020 - val_loss: 0.3351\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3077 - val_loss: 0.3441\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3111 - val_loss: 0.3405\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3081 - val_loss: 0.3351\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.2965 - val_loss: 0.3278\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.3013 - val_loss: 0.3236\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.3091 - val_loss: 0.3553\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2984 - val_loss: 0.3185\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2854 - val_loss: 0.3351\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2900 - val_loss: 0.3253\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2847 - val_loss: 0.3247\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.2960 - val_loss: 0.3215\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 0.2822 - val_loss: 0.3089\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2837 - val_loss: 0.3209\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2937 - val_loss: 0.3187\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2781 - val_loss: 0.3092\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.2910 - val_loss: 0.3152\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2749 - val_loss: 0.3304\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2947 - val_loss: 0.2995\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2793 - val_loss: 0.3016\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2773 - val_loss: 0.3025\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2721 - val_loss: 0.3280\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2736 - val_loss: 0.3193\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2764 - val_loss: 0.2944\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2709 - val_loss: 0.2968\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2641 - val_loss: 0.3144\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2747 - val_loss: 0.3035\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2749 - val_loss: 0.3152\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.2724 - val_loss: 0.2904\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 0.2711 - val_loss: 0.3302\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2754 - val_loss: 0.2969\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2633 - val_loss: 0.3165\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2736 - val_loss: 0.2906\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2649 - val_loss: 0.2969\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2649 - val_loss: 0.2914\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 0.2566 - val_loss: 0.2910\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2621 - val_loss: 0.2943\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2565 - val_loss: 0.2974\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2594Restoring model weights from the end of the best epoch: 77.\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.2594 - val_loss: 0.2965\n",
      "Epoch 87: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 4.0751 - val_loss: 1.0447\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.9106 - val_loss: 0.9374\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8928 - val_loss: 0.9253\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8773 - val_loss: 0.8865\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8484 - val_loss: 0.8501\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8274 - val_loss: 0.8078\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7872 - val_loss: 0.7657\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.7479 - val_loss: 0.7211\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7049 - val_loss: 0.6799\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6603 - val_loss: 0.6207\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6187 - val_loss: 0.6014\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6042 - val_loss: 0.5986\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5822 - val_loss: 0.5981\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.5921 - val_loss: 0.5631\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.5754 - val_loss: 0.5514\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5554 - val_loss: 0.5615\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.5533 - val_loss: 0.5389\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5364 - val_loss: 0.5225\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5214 - val_loss: 0.5550\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5248 - val_loss: 0.5302\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5173 - val_loss: 0.4836\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5050 - val_loss: 0.5608\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4914 - val_loss: 0.4665\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4894 - val_loss: 0.4406\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4691 - val_loss: 0.5540\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4933 - val_loss: 0.4551\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4524 - val_loss: 0.4637\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4421 - val_loss: 0.4323\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4637 - val_loss: 0.4675\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4534 - val_loss: 0.4024\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4236 - val_loss: 0.5034\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.5071 - val_loss: 0.4893\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4972 - val_loss: 0.4849\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4878 - val_loss: 0.4607\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4747 - val_loss: 0.4332\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4488 - val_loss: 0.4666\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4517 - val_loss: 0.4191\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4510 - val_loss: 0.4188\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4340 - val_loss: 0.4015\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4119 - val_loss: 0.4105\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4201 - val_loss: 0.4471\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4153 - val_loss: 0.3926\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3970 - val_loss: 0.4238\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3950 - val_loss: 0.3824\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3931 - val_loss: 0.3911\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3881 - val_loss: 0.3975\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3925 - val_loss: 0.3591\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3717 - val_loss: 0.3510\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3774 - val_loss: 0.3608\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3626 - val_loss: 0.3375\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3697 - val_loss: 0.3911\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4167 - val_loss: 0.4618\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4055 - val_loss: 0.3910\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3546 - val_loss: 0.3320\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.3362 - val_loss: 0.3736\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.3402 - val_loss: 0.3239\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3406 - val_loss: 0.3833\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3710 - val_loss: 0.3389\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3226 - val_loss: 0.3322\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3204 - val_loss: 0.3424\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3146 - val_loss: 0.3321\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3145 - val_loss: 0.3484\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2960 - val_loss: 0.2973\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3023 - val_loss: 0.3375\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2985 - val_loss: 0.2876\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3057 - val_loss: 0.3377\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2961 - val_loss: 0.3140\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2929 - val_loss: 0.2738\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2905 - val_loss: 0.2841\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.2973 - val_loss: 0.2785\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.3158 - val_loss: 0.2884\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2917 - val_loss: 0.2996\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2837 - val_loss: 0.3116\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3233 - val_loss: 0.3210\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2980 - val_loss: 0.2871\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2757 - val_loss: 0.3125\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3491 - val_loss: 0.3636\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3346Restoring model weights from the end of the best epoch: 68.\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3346 - val_loss: 0.3107\n",
      "Epoch 78: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 101ms/step - loss: 48.0712 - val_loss: 12.4399\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 11.4528 - val_loss: 11.6210\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 11.2645 - val_loss: 11.3379\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 10.9844 - val_loss: 10.9871\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 99ms/step - loss: 10.6331 - val_loss: 10.4843\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 10.3477 - val_loss: 10.0300\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 9.9482 - val_loss: 9.6840\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 9.5567 - val_loss: 9.3639\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 9.0482 - val_loss: 8.5827\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 8.4212 - val_loss: 7.8685\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 8.0173 - val_loss: 7.8938\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 7.9251 - val_loss: 7.5097\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.7444 - val_loss: 7.2812\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 7.5434 - val_loss: 7.1241\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.4293 - val_loss: 7.0882\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.2690 - val_loss: 7.1151\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 7.1805 - val_loss: 6.9367\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.1047 - val_loss: 7.1808\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.0161 - val_loss: 6.9085\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 7.0390 - val_loss: 6.3755\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 6.6338 - val_loss: 6.4418\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.3081 - val_loss: 6.0144\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 6.7277 - val_loss: 6.5843\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4921 - val_loss: 6.1917\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.0692 - val_loss: 5.7540\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 5.9317 - val_loss: 6.2294\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.4090 - val_loss: 6.1858\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.1708 - val_loss: 5.6132\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 5.6558 - val_loss: 5.4690\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.4654 - val_loss: 5.2541\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 5.3770 - val_loss: 5.2300\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 5.2643 - val_loss: 5.0507\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.9370 - val_loss: 5.2331\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.7693 - val_loss: 4.8601\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.6280 - val_loss: 4.6742\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.7752 - val_loss: 5.5784\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.4521 - val_loss: 4.5959\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 4.3492 - val_loss: 4.5390\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.1607 - val_loss: 4.2574\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 4.1026 - val_loss: 4.1407\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9575 - val_loss: 4.0671\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.9016 - val_loss: 4.0914\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.8496 - val_loss: 3.9786\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.8967 - val_loss: 4.1014\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 3.8880 - val_loss: 3.7096\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.7677 - val_loss: 3.9000\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.6521 - val_loss: 3.8338\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.7866 - val_loss: 4.1542\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.7324 - val_loss: 3.9656\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.6212 - val_loss: 3.8672\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.6786 - val_loss: 4.2615\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.6564 - val_loss: 3.8986\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.6200 - val_loss: 3.8292\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.6435 - val_loss: 3.9591\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.5586Restoring model weights from the end of the best epoch: 45.\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5586 - val_loss: 3.7655\n",
      "Epoch 55: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 48.6496 - val_loss: 12.3381\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 11.6085 - val_loss: 11.8094\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 11.4313 - val_loss: 11.5907\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 11.1463 - val_loss: 11.0833\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 10.7092 - val_loss: 10.6484\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 10.2467 - val_loss: 10.0298\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 9.8395 - val_loss: 9.7345\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 9.2485 - val_loss: 8.7535\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 8.6141 - val_loss: 8.0755\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 8.2889 - val_loss: 7.6628\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.8469 - val_loss: 7.6461\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.5765 - val_loss: 7.3866\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.5516 - val_loss: 7.1171\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 7.2837 - val_loss: 6.9211\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 7.1336 - val_loss: 6.8391\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.9493 - val_loss: 7.0952\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.8685 - val_loss: 6.4333\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.5808 - val_loss: 6.1348\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.5371 - val_loss: 6.3856\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.1661 - val_loss: 5.9069\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 6.2643 - val_loss: 7.0417\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 7.1074 - val_loss: 6.4265\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 6.4257 - val_loss: 5.7462\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 6.1986 - val_loss: 5.6094\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.6226 - val_loss: 5.5277\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.4723 - val_loss: 5.8197\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 5.4144 - val_loss: 5.1212\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 5.2120 - val_loss: 5.2081\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 5.0618 - val_loss: 4.8908\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.8826 - val_loss: 5.1771\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.8183 - val_loss: 4.8965\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.7459 - val_loss: 4.7246\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 4.7081 - val_loss: 5.0945\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.6702 - val_loss: 4.8258\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.5011 - val_loss: 4.8824\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.5665 - val_loss: 4.8634\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.6532 - val_loss: 4.6592\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.5585 - val_loss: 4.5355\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.3707 - val_loss: 4.6786\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.2412 - val_loss: 4.3291\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 4.3259 - val_loss: 4.4216\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 4.2112 - val_loss: 4.2499\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 4.2035 - val_loss: 4.2548\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 4.2417 - val_loss: 4.5375\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.1558 - val_loss: 4.2172\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 4.0519 - val_loss: 4.3957\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.0941 - val_loss: 4.2307\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 3.9458 - val_loss: 4.1992\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9361 - val_loss: 3.9153\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 4.1948 - val_loss: 4.2270\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.9559 - val_loss: 4.0679\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 3.8946 - val_loss: 4.2069\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.9010 - val_loss: 4.1794\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.7806 - val_loss: 4.1441\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.9063 - val_loss: 4.1241\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 4.0222 - val_loss: 4.2360\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.9139 - val_loss: 3.8010\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.8223 - val_loss: 3.9686\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.8312 - val_loss: 3.9333\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.6433 - val_loss: 3.7982\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 3.8150 - val_loss: 4.0121\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.6326 - val_loss: 4.0047\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.6657 - val_loss: 3.7647\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.6789 - val_loss: 3.7123\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.6396 - val_loss: 4.0559\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.5898 - val_loss: 4.1709\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 3.6513 - val_loss: 3.7660\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.5238 - val_loss: 4.0202\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.5943 - val_loss: 3.5369\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5778 - val_loss: 3.6756\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 3.6137 - val_loss: 4.1007\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 3.5834 - val_loss: 3.6806\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.5335 - val_loss: 3.7098\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.5180 - val_loss: 3.5987\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.4734 - val_loss: 3.7217\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.4369 - val_loss: 3.6335\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.4369 - val_loss: 3.5892\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.4180 - val_loss: 3.7286\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.5514Restoring model weights from the end of the best epoch: 69.\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 3.5514 - val_loss: 3.7481\n",
      "Epoch 79: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 94ms/step - loss: 54.1281 - val_loss: 16.4916\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 12.0768 - val_loss: 11.9788\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 11.4901 - val_loss: 11.7168\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 11.3170 - val_loss: 11.4466\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.9496 - val_loss: 10.9414\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 10.6942 - val_loss: 10.5909\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 10.3445 - val_loss: 10.2761\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 10.1242 - val_loss: 9.9917\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 9.8313 - val_loss: 9.7047\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 9.3818 - val_loss: 8.9331\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.9327 - val_loss: 8.7144\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 8.4850 - val_loss: 7.8993\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 8.0383 - val_loss: 7.5106\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 7.9191 - val_loss: 7.5999\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 7.6849 - val_loss: 7.3232\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 7.5122 - val_loss: 7.1175\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.2906 - val_loss: 7.0850\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 7.3116 - val_loss: 6.9422\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 7.1249 - val_loss: 7.0042\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 7.0272 - val_loss: 6.8833\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 6.9390 - val_loss: 6.8088\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 6.9204 - val_loss: 6.4538\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.7916 - val_loss: 6.3541\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 6.5621 - val_loss: 6.3270\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4996 - val_loss: 6.3469\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 6.4757 - val_loss: 6.0887\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 6.3870 - val_loss: 6.2855\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 6.3105 - val_loss: 6.1152\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 6.2095 - val_loss: 5.9896\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 6.0166 - val_loss: 5.6289\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 5.8751 - val_loss: 5.5119\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 6.5824 - val_loss: 6.8382\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 6.8762 - val_loss: 6.2313\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 6.4256 - val_loss: 5.8614\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 6.1331 - val_loss: 5.7034\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 6.1402 - val_loss: 5.8939\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 6.1461 - val_loss: 6.1298\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 5.8499 - val_loss: 6.1464\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 5.6509 - val_loss: 5.5962\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 5.6545 - val_loss: 5.5375\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 5.4738Restoring model weights from the end of the best epoch: 31.\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 5.4738 - val_loss: 5.6182\n",
      "Epoch 41: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 130ms/step - loss: 58.5954 - val_loss: 16.0902\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 11.9170 - val_loss: 11.9041\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 11.3662 - val_loss: 11.5313\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 11.0702 - val_loss: 11.0728\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 10.7175 - val_loss: 10.5722\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 10.3584 - val_loss: 10.2542\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 10.0889 - val_loss: 9.8094\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 9.8926 - val_loss: 9.6484\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 9.5260 - val_loss: 9.3373\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 8.9975 - val_loss: 8.5399\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 8.4453 - val_loss: 7.9141\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 8.0616 - val_loss: 7.9242\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 10s 130ms/step - loss: 7.8989 - val_loss: 7.4661\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 10s 139ms/step - loss: 7.8832 - val_loss: 7.5894\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 7.5332 - val_loss: 7.2375\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 7.3561 - val_loss: 7.1094\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 7.1855 - val_loss: 7.2674\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 6.8537 - val_loss: 7.4994\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 6.6607 - val_loss: 6.1206\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.4031 - val_loss: 7.1170\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 6.0279 - val_loss: 6.3487\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 5.9456 - val_loss: 5.6528\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 5.4371 - val_loss: 5.6329\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 5.6389 - val_loss: 5.5730\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 5.4170 - val_loss: 5.4423\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 5.3184 - val_loss: 5.9271\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 10s 139ms/step - loss: 5.2417 - val_loss: 5.4037\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 5.1010 - val_loss: 5.6522\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 5.1889 - val_loss: 5.7344\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.8934 - val_loss: 5.3819\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 4.8862 - val_loss: 5.7597\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 4.8536 - val_loss: 5.1857\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.7968 - val_loss: 5.1836\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 4.7852 - val_loss: 4.8990\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 4.5897 - val_loss: 5.1615\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 4.8620 - val_loss: 6.0923\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 5.0018 - val_loss: 5.0346\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 4.5163 - val_loss: 4.9427\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 4.6314 - val_loss: 4.7376\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.6127 - val_loss: 4.8992\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 10s 139ms/step - loss: 4.5185 - val_loss: 4.8582\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 10s 130ms/step - loss: 4.3965 - val_loss: 5.0547\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 10s 139ms/step - loss: 4.3884 - val_loss: 4.7358\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.3033 - val_loss: 4.8939\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 10s 130ms/step - loss: 4.2977 - val_loss: 4.7103\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 4.2758 - val_loss: 4.6832\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 4.2855 - val_loss: 4.6887\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 4.2543 - val_loss: 4.5455\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 10s 130ms/step - loss: 4.1236 - val_loss: 4.6476\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.1001 - val_loss: 4.5276\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 4.0745 - val_loss: 4.4323\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 4.3468 - val_loss: 4.8330\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 4.2920 - val_loss: 4.3534\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.0585 - val_loss: 4.4832\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 3.9896 - val_loss: 4.2424\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 4.0409 - val_loss: 4.4336\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 3.9377 - val_loss: 4.4169\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.0108 - val_loss: 4.1724\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 3.9657 - val_loss: 4.4077\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 4.1394 - val_loss: 4.1408\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.0989 - val_loss: 4.0783\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 3.9150 - val_loss: 4.1212\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.8616 - val_loss: 4.1469\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 3.8852 - val_loss: 4.4912\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.9605 - val_loss: 4.1973\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 3.8178 - val_loss: 4.3304\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 3.8601 - val_loss: 4.4285\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 3.8130 - val_loss: 4.0746\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 3.8020 - val_loss: 4.0385\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 3.7628 - val_loss: 4.4470\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.9684 - val_loss: 4.4800\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.7986 - val_loss: 4.1875\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.8293 - val_loss: 4.0819\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 3.6416 - val_loss: 3.9775\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 3.7616 - val_loss: 4.0190\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 3.7011 - val_loss: 4.2477\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 3.7707 - val_loss: 4.3213\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 3.7447 - val_loss: 4.0166\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 3.8000 - val_loss: 4.2057\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 3.7686 - val_loss: 4.0847\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 3.6679 - val_loss: 4.1716\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 3.7179 - val_loss: 3.8057\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 3.6135 - val_loss: 4.3668\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.6957 - val_loss: 4.1263\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 3.7491 - val_loss: 3.9302\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 3.6559 - val_loss: 4.1264\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 3.5957 - val_loss: 3.9696\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 3.5783 - val_loss: 4.2316\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 3.6726 - val_loss: 4.0068\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 3.6673 - val_loss: 4.1097\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 3.7312 - val_loss: 3.8679\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.6450Restoring model weights from the end of the best epoch: 82.\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 3.6450 - val_loss: 3.9766\n",
      "Epoch 92: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 12s 131ms/step - loss: 45.5278 - val_loss: 12.1042\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 11.5063 - val_loss: 11.7285\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 11.3977 - val_loss: 11.5502\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 11.1822 - val_loss: 11.2057\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 10.7579 - val_loss: 10.6179\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 10.3137 - val_loss: 10.2431\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 10.0009 - val_loss: 9.9248\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 9.6700 - val_loss: 9.4642\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 9.0765 - val_loss: 8.5139\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 8.3416 - val_loss: 7.8284\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 7.9369 - val_loss: 7.5012\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 7.8537 - val_loss: 7.5981\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 7.5804 - val_loss: 7.2345\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 7.5714 - val_loss: 7.0945\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 7.3770 - val_loss: 7.0096\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 7.2886 - val_loss: 6.8966\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 7.1447 - val_loss: 6.7669\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 7.0091 - val_loss: 6.7777\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 6.8494 - val_loss: 6.5848\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 6.6870 - val_loss: 6.5487\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 6.6121 - val_loss: 6.6373\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 6.4717 - val_loss: 6.1300\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 6.2532 - val_loss: 6.0204\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 6.1571 - val_loss: 5.7951\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 6.0385 - val_loss: 5.8109\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 5.8667 - val_loss: 5.4836\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 5.7273 - val_loss: 5.4239\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 5.5391 - val_loss: 5.2521\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 5.1593 - val_loss: 6.4245\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 5.5333 - val_loss: 5.3461\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 4.8953 - val_loss: 5.4074\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 5.0590 - val_loss: 4.9749\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 4.7262 - val_loss: 4.7548\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 4.7148 - val_loss: 4.3615\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 4.4973 - val_loss: 4.8575\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 4.3312 - val_loss: 4.2482\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 4.1975 - val_loss: 4.3841\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 4.0495 - val_loss: 4.1006\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 4.0177 - val_loss: 4.1146\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 4.2331 - val_loss: 4.2137\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 4.0811 - val_loss: 4.4505\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 4.0074 - val_loss: 4.1658\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 3.9487 - val_loss: 4.3847\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 3.8853 - val_loss: 3.9246\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 4.0094 - val_loss: 4.1928\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 3.9056 - val_loss: 3.8516\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.8720 - val_loss: 4.0234\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.7921 - val_loss: 3.7353\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 3.7599 - val_loss: 3.7702\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 3.8083 - val_loss: 4.7631\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 3.7967 - val_loss: 3.6847\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.6284 - val_loss: 3.6938\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 3.6459 - val_loss: 3.5805\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 3.6349 - val_loss: 3.8405\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 3.5826 - val_loss: 3.8422\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 3.6155 - val_loss: 4.0644\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 3.5593 - val_loss: 4.0505\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.4807 - val_loss: 3.7818\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.5553 - val_loss: 3.6276\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.4558 - val_loss: 3.6673\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 3.5889 - val_loss: 3.9057\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 3.5419 - val_loss: 3.7864\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.4137Restoring model weights from the end of the best epoch: 53.\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 3.4137 - val_loss: 3.8345\n",
      "Epoch 63: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 122ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 200.0000Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 127ms/step - loss: 97.5561 - val_loss: 29.2527\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 13.8251 - val_loss: 11.9732\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 11.6957 - val_loss: 11.9194\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 11.6144 - val_loss: 11.8857\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 11.4985 - val_loss: 11.7151\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 11.2806 - val_loss: 11.4642\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 10s 144ms/step - loss: 10.9719 - val_loss: 10.9246\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 10.6526 - val_loss: 10.4875\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 10.4498 - val_loss: 10.5759\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 10.2436 - val_loss: 10.0976\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 10.0358 - val_loss: 9.7478\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 9.7264 - val_loss: 9.5849\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 9.3226 - val_loss: 8.8134\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 103ms/step - loss: 8.5907 - val_loss: 7.9652\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 8.1657 - val_loss: 8.0253\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 7.9021 - val_loss: 7.5161\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 7.9518 - val_loss: 7.6807\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 7.5814 - val_loss: 7.4648\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 7.4659 - val_loss: 7.3364\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 7.3083 - val_loss: 7.1430\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 7.2902 - val_loss: 7.2320\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 7.3094 - val_loss: 6.9704\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 7.0709 - val_loss: 6.8068\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 7.2002 - val_loss: 6.9056\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 6.9963 - val_loss: 6.6142\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 6.9093 - val_loss: 6.5980\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 6.8012 - val_loss: 6.7627\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 10s 144ms/step - loss: 6.7981 - val_loss: 6.4858\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 6.7394 - val_loss: 6.2641\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 6.5822 - val_loss: 6.2516\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 6.5641 - val_loss: 6.4192\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 6.5388 - val_loss: 6.2072\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 6.4114 - val_loss: 6.1574\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 6.3917 - val_loss: 6.1841\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 6.1821 - val_loss: 6.0093\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 6.1162 - val_loss: 6.1343\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 6.5303 - val_loss: 6.1354\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 6.1502 - val_loss: 6.1591\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 5.9600 - val_loss: 5.6604\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 5.7083 - val_loss: 5.2830\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 6.0495 - val_loss: 5.8007\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 5.6173 - val_loss: 5.3091\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 5.6243 - val_loss: 5.7017\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.6327 - val_loss: 5.3369\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 5.6571 - val_loss: 5.4916\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 5.3163 - val_loss: 5.1048\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 5.3798 - val_loss: 5.9604\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 5.4674 - val_loss: 5.8101\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 5.0967 - val_loss: 5.7720\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 5.0896 - val_loss: 5.6806\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 4.9313 - val_loss: 4.8303\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 4.7952 - val_loss: 5.0148\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 5.1758 - val_loss: 5.0756\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 5.0537 - val_loss: 4.6916\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 4.6611 - val_loss: 4.3793\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 4.2712 - val_loss: 4.2592\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 5.6631 - val_loss: 4.7603\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 4.5233 - val_loss: 4.1922\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 11s 147ms/step - loss: 4.9557 - val_loss: 4.4129\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 11s 144ms/step - loss: 4.1892 - val_loss: 4.5489\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 11s 145ms/step - loss: 4.2710 - val_loss: 4.1979\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 4.2785 - val_loss: 4.2500\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 4.0775 - val_loss: 4.4939\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 4.1249 - val_loss: 4.3577\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 10s 137ms/step - loss: 4.1773 - val_loss: 4.3036\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 11s 144ms/step - loss: 4.0594 - val_loss: 3.7650\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 11s 145ms/step - loss: 3.9918 - val_loss: 6.5188\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 10s 144ms/step - loss: 5.3496 - val_loss: 4.4428\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 4.2484 - val_loss: 4.1296\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 11s 145ms/step - loss: 4.1052 - val_loss: 4.2906\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 11s 148ms/step - loss: 4.0573 - val_loss: 3.5619\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 3.9317 - val_loss: 3.7140\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 10s 139ms/step - loss: 4.2640 - val_loss: 3.8931\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 11s 147ms/step - loss: 3.6940 - val_loss: 3.5312\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 3.7420 - val_loss: 4.0107\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 3.9483 - val_loss: 3.7385\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 3.7937 - val_loss: 3.6913\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 4.3519 - val_loss: 4.4079\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.8334 - val_loss: 4.0383\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 3.6639 - val_loss: 3.4781\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.7006 - val_loss: 3.8151\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 3.6239 - val_loss: 5.1947\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 4.1535 - val_loss: 3.4282\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 3.5496 - val_loss: 3.5294\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 3.5214 - val_loss: 3.3720\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 3.5946 - val_loss: 3.2557\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 11s 145ms/step - loss: 3.5137 - val_loss: 4.1117\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 3.5167 - val_loss: 3.5289\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 3.3493 - val_loss: 3.5137\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.3747 - val_loss: 3.1814\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 3.4495 - val_loss: 3.6903\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 10s 142ms/step - loss: 3.4029 - val_loss: 3.9564\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 10s 141ms/step - loss: 3.3731 - val_loss: 3.2324\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 3.4031 - val_loss: 3.5202\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 3.4277 - val_loss: 3.4587\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 3.3887 - val_loss: 3.6000\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 3.3883 - val_loss: 3.2518\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.3942 - val_loss: 3.2690\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 10s 140ms/step - loss: 3.4524 - val_loss: 3.4926\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.4549Restoring model weights from the end of the best epoch: 90.\n",
      "73/73 [==============================] - 10s 143ms/step - loss: 3.4549 - val_loss: 4.3875\n",
      "Epoch 100: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 15s 155ms/step - loss: 159.1508 - val_loss: 62.7926\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 10s 138ms/step - loss: 21.1604 - val_loss: 11.9450\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 11s 144ms/step - loss: 11.6742 - val_loss: 11.9334\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 11.6024 - val_loss: 11.9171\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 11.5173 - val_loss: 11.7428\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 11.3608 - val_loss: 11.4770\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 11.1854 - val_loss: 11.1807\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 10.8853 - val_loss: 10.7838\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 10.5788 - val_loss: 10.6625\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 10.4889 - val_loss: 10.2371\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 10.2200 - val_loss: 9.9436\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 9.8552 - val_loss: 9.7920\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 9.5703 - val_loss: 9.2123\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 9.1263 - val_loss: 8.6558\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 8.5711 - val_loss: 8.1467\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 8.0805 - val_loss: 8.1840\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 7.9184 - val_loss: 7.6233\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 7.7894 - val_loss: 7.6126\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 7.5970 - val_loss: 7.3023\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 7.4367 - val_loss: 7.1865\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 7.2745 - val_loss: 7.0444\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 7.3945 - val_loss: 7.0505\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 7.1476 - val_loss: 6.9235\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 6.9412 - val_loss: 6.8874\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 6.8723 - val_loss: 6.4723\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.5599 - val_loss: 6.3695\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.7520 - val_loss: 6.6602\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 6.5103 - val_loss: 6.4252\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.3870 - val_loss: 6.0280\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 6.1169 - val_loss: 5.7478\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.1433 - val_loss: 6.1562\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 6.1754 - val_loss: 6.0992\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 5.8953 - val_loss: 5.5958\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.7327 - val_loss: 5.9716\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.7043 - val_loss: 5.7654\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 5.5681 - val_loss: 5.4714\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 5.4230 - val_loss: 5.4240\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 5.4495 - val_loss: 5.3377\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.1971 - val_loss: 5.1355\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 5.0652 - val_loss: 5.2154\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.9492 - val_loss: 4.7972\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.8740 - val_loss: 4.8503\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.6381 - val_loss: 4.5657\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.5477 - val_loss: 4.7837\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.3717 - val_loss: 4.3609\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.3969 - val_loss: 4.2252\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.3252 - val_loss: 4.2514\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.4104 - val_loss: 4.2940\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.0824 - val_loss: 3.9856\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4.1167 - val_loss: 4.3154\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.9865 - val_loss: 4.1166\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 3.9642 - val_loss: 4.1887\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 4.5310 - val_loss: 3.9223\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 10s 132ms/step - loss: 3.7518 - val_loss: 3.7060\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.7865 - val_loss: 3.7320\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.6539 - val_loss: 3.7319\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 3.6652 - val_loss: 3.6401\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.7724 - val_loss: 3.8137\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 10s 136ms/step - loss: 3.6682 - val_loss: 3.7353\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.6816 - val_loss: 3.6475\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.5514 - val_loss: 3.7445\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 3.5839 - val_loss: 3.7514\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.8116 - val_loss: 3.9299\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 10s 135ms/step - loss: 3.5691 - val_loss: 3.3819\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.5198 - val_loss: 3.5485\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 3.4422 - val_loss: 3.4371\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 3.3858 - val_loss: 3.9665\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.5230 - val_loss: 3.4552\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.4381 - val_loss: 3.6507\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.4119 - val_loss: 3.5270\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 10s 133ms/step - loss: 3.4011 - val_loss: 3.5449\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 10s 134ms/step - loss: 3.3541 - val_loss: 3.6183\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.3428 - val_loss: 4.1562\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.4166Restoring model weights from the end of the best epoch: 64.\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 3.4166 - val_loss: 3.4053\n",
      "Epoch 74: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 11s 115ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 101ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 8s 105ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 200.0000Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 200.0000 - val_loss: 200.0000\n",
      "Epoch 11: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 13s 135ms/step - loss: 80.4160 - val_loss: 18.7064\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 12.2080 - val_loss: 11.8438\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 11.5187 - val_loss: 11.8477\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 11.4265 - val_loss: 11.6574\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 11.1849 - val_loss: 11.4347\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 11.0178 - val_loss: 10.9529\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 10.7079 - val_loss: 10.6108\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 10.4647 - val_loss: 10.2551\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 10.2689 - val_loss: 9.9953\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 9.9434 - val_loss: 9.6269\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 9.5673 - val_loss: 9.2434\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 9.1514 - val_loss: 8.6987\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 8.7111 - val_loss: 8.8115\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 9s 130ms/step - loss: 8.2824 - val_loss: 7.9364\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 8.0104 - val_loss: 8.0052\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 7.8183 - val_loss: 7.5213\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 7.6711 - val_loss: 7.9048\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 7.5475 - val_loss: 7.1378\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 7.4359 - val_loss: 7.1055\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 7.2218 - val_loss: 7.1014\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 7.2560 - val_loss: 6.8560\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 7.1309 - val_loss: 6.9986\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 7.0879 - val_loss: 7.0982\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 7.0510 - val_loss: 6.7261\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 6.9280 - val_loss: 6.6826\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 6.7849 - val_loss: 6.5960\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 6.6224 - val_loss: 6.5932\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 6.5138 - val_loss: 6.5989\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 6.4343 - val_loss: 6.1346\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 6.3016 - val_loss: 6.0813\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 6.0219 - val_loss: 5.9602\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 5.9822 - val_loss: 5.9546\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 9s 128ms/step - loss: 5.9158 - val_loss: 6.4312\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 6.2354 - val_loss: 5.9723\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 6.1149 - val_loss: 6.0221\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 6.6442 - val_loss: 6.7596\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 6.3808 - val_loss: 5.8096\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 5.6852 - val_loss: 5.2942\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 5.7934 - val_loss: 5.4390\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.8323 - val_loss: 5.1390\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 5.4730 - val_loss: 5.3578\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.2811 - val_loss: 5.2781\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 6.0063 - val_loss: 5.8516\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.5555 - val_loss: 5.0024\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 5.1685 - val_loss: 4.9746\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 5.0377 - val_loss: 5.1345\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 5.1201 - val_loss: 5.0813\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.9077 - val_loss: 5.0449\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 5.0542 - val_loss: 4.6899\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 4.7911 - val_loss: 4.7419\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.8113 - val_loss: 4.8433\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 98ms/step - loss: 4.7445 - val_loss: 4.9208\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.7147 - val_loss: 4.7654\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.7468 - val_loss: 5.3098\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 4.6232 - val_loss: 4.2073\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.9301 - val_loss: 4.8357\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.6300 - val_loss: 4.3051\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.6914 - val_loss: 4.5886\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 4.6188 - val_loss: 4.8786\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 8s 108ms/step - loss: 4.4187 - val_loss: 4.3065\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.3069 - val_loss: 4.0021\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 4.0683 - val_loss: 3.9136\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.0887 - val_loss: 3.7179\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.8717 - val_loss: 4.2439\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.1415 - val_loss: 3.9634\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 3.9894 - val_loss: 4.0433\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 3.9922 - val_loss: 3.9254\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.0289 - val_loss: 3.8104\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 3.9623 - val_loss: 3.6250\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 100ms/step - loss: 3.6997 - val_loss: 3.7616\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 3.7792 - val_loss: 3.5697\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.7519 - val_loss: 3.5285\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 3.6837 - val_loss: 3.7379\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 5.3207 - val_loss: 4.6425\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 4.3187 - val_loss: 4.5124\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 3.9343 - val_loss: 3.9597\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 3.6491 - val_loss: 3.8878\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 3.6675 - val_loss: 3.6979\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 3.8218 - val_loss: 4.3088\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 102ms/step - loss: 3.5852 - val_loss: 3.3474\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 3.5733 - val_loss: 3.4926\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 3.4097 - val_loss: 3.4659\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 3.4702 - val_loss: 3.3429\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 3.4979 - val_loss: 3.6906\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.5858 - val_loss: 3.3370\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.7468 - val_loss: 3.6622\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.5374 - val_loss: 3.4376\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 3.4456 - val_loss: 3.3660\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 3.5558 - val_loss: 3.4816\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 3.4882 - val_loss: 3.2745\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.3910 - val_loss: 3.4993\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 3.4065 - val_loss: 4.0168\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 4.0554 - val_loss: 3.7006\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 3.3936 - val_loss: 3.1766\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.3619 - val_loss: 4.0879\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 3.4582 - val_loss: 3.3051\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 3.5460 - val_loss: 3.5204\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.3324 - val_loss: 3.3166\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.5037 - val_loss: 3.5370\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 3.3541 - val_loss: 3.6984\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 3.3165 - val_loss: 3.3715\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 3.2986 - val_loss: 4.0338\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 3.3369 - val_loss: 3.4667\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 3.2782Restoring model weights from the end of the best epoch: 94.\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 3.2782 - val_loss: 3.4016\n",
      "Epoch 104: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 3.7793 - val_loss: 0.9594\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7588 - val_loss: 0.7845\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.7422 - val_loss: 0.7645\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7280 - val_loss: 0.7354\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7058 - val_loss: 0.7214\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6880 - val_loss: 0.6819\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.6643 - val_loss: 0.6566\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6447 - val_loss: 0.6440\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.6244 - val_loss: 0.6013\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5848 - val_loss: 0.5557\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5435 - val_loss: 0.5319\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5193 - val_loss: 0.4955\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5020 - val_loss: 0.4950\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4920 - val_loss: 0.4946\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4789 - val_loss: 0.4601\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4677 - val_loss: 0.4674\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4626 - val_loss: 0.4679\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4616 - val_loss: 0.4420\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4493 - val_loss: 0.4529\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4501 - val_loss: 0.4264\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4295 - val_loss: 0.4074\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4129 - val_loss: 0.3840\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4433 - val_loss: 0.4318\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4285 - val_loss: 0.4242\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4106 - val_loss: 0.3942\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.3932 - val_loss: 0.3647\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3714 - val_loss: 0.3560\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3619 - val_loss: 0.3573\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3560 - val_loss: 0.3917\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3580 - val_loss: 0.3740\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3631 - val_loss: 0.4239\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4120 - val_loss: 0.3857\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3657 - val_loss: 0.3486\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3439 - val_loss: 0.3195\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3222 - val_loss: 0.3379\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3962 - val_loss: 0.4210\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4123 - val_loss: 0.3760\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3375 - val_loss: 0.3659\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3298 - val_loss: 0.3259\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3170 - val_loss: 0.3348\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3184 - val_loss: 0.3505\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2991 - val_loss: 0.3334\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3155 - val_loss: 0.3027\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2953 - val_loss: 0.2886\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2886 - val_loss: 0.2912\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2930 - val_loss: 0.2706\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2695 - val_loss: 0.2779\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3021 - val_loss: 0.2806\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2799 - val_loss: 0.3522\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3764 - val_loss: 0.3172\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3333 - val_loss: 0.3310\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3168 - val_loss: 0.3209\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3065 - val_loss: 0.2899\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2865 - val_loss: 0.2748\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2686 - val_loss: 0.2790\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2679Restoring model weights from the end of the best epoch: 46.\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.2679 - val_loss: 0.3232\n",
      "Epoch 56: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 99ms/step - loss: 3.4764 - val_loss: 1.0176\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.7859 - val_loss: 0.7844\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.7487 - val_loss: 0.7718\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7372 - val_loss: 0.7688\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7237 - val_loss: 0.7281\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.6922 - val_loss: 0.6839\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6669 - val_loss: 0.6582\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.6317 - val_loss: 0.6116\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.5863 - val_loss: 0.5480\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.5319 - val_loss: 0.5061\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.5159 - val_loss: 0.4908\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4981 - val_loss: 0.4906\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4897 - val_loss: 0.4768\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4766 - val_loss: 0.4772\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4730 - val_loss: 0.4643\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4702 - val_loss: 0.4534\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4685 - val_loss: 0.4554\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.4594 - val_loss: 0.4565\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4468 - val_loss: 0.4445\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4509 - val_loss: 0.4436\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4500 - val_loss: 0.4474\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4436 - val_loss: 0.4310\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4312 - val_loss: 0.4228\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4263 - val_loss: 0.4131\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.4249 - val_loss: 0.4307\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4186 - val_loss: 0.4100\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4163 - val_loss: 0.3965\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4071 - val_loss: 0.4150\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3966 - val_loss: 0.3928\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3966 - val_loss: 0.3780\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3922 - val_loss: 0.4088\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3919 - val_loss: 0.3714\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3830 - val_loss: 0.3892\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3772 - val_loss: 0.3580\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3604 - val_loss: 0.3459\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3570 - val_loss: 0.3636\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3525 - val_loss: 0.3405\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3516 - val_loss: 0.3967\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3524 - val_loss: 0.3677\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3474 - val_loss: 0.3311\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3341 - val_loss: 0.3773\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3376 - val_loss: 0.3864\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3311 - val_loss: 0.3376\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3358 - val_loss: 0.3334\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3322 - val_loss: 0.3242\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3196 - val_loss: 0.3094\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3250 - val_loss: 0.3594\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3242 - val_loss: 0.3140\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3225 - val_loss: 0.3257\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3050 - val_loss: 0.3278\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3037 - val_loss: 0.3045\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3022 - val_loss: 0.3043\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2950 - val_loss: 0.2917\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2958 - val_loss: 0.3324\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2850 - val_loss: 0.3435\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2879 - val_loss: 0.2958\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.2833 - val_loss: 0.2883\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2918 - val_loss: 0.3069\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2658 - val_loss: 0.2921\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2685 - val_loss: 0.2723\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.2566 - val_loss: 0.2664\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2594 - val_loss: 0.2616\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2537 - val_loss: 0.2587\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2451 - val_loss: 0.2848\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2472 - val_loss: 0.2555\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2482 - val_loss: 0.2605\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2443 - val_loss: 0.2612\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2370 - val_loss: 0.2682\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2347 - val_loss: 0.2543\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2420 - val_loss: 0.2451\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2307 - val_loss: 0.2363\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2309 - val_loss: 0.2420\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2336 - val_loss: 0.2440\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2226 - val_loss: 0.2487\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2366 - val_loss: 0.2297\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2198 - val_loss: 0.2376\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2177 - val_loss: 0.2315\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.2169 - val_loss: 0.2329\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2235 - val_loss: 0.2458\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2135 - val_loss: 0.2229\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2234 - val_loss: 0.2325\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.2160 - val_loss: 0.2239\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2095 - val_loss: 0.2331\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2091 - val_loss: 0.2431\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2152 - val_loss: 0.2355\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2112 - val_loss: 0.2386\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2151 - val_loss: 0.2240\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2054 - val_loss: 0.2197\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2043 - val_loss: 0.2343\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2118 - val_loss: 0.2470\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2090 - val_loss: 0.2231\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2058 - val_loss: 0.2200\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2330 - val_loss: 0.2395\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2203 - val_loss: 0.2151\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2023 - val_loss: 0.2202\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2073 - val_loss: 0.2219\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2033 - val_loss: 0.2366\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2085 - val_loss: 0.2224\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2047 - val_loss: 0.2113\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2034 - val_loss: 0.2131\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2005 - val_loss: 0.2106\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.1983 - val_loss: 0.2079\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1980 - val_loss: 0.2130\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2005 - val_loss: 0.2126\n",
      "Epoch 105/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2005 - val_loss: 0.2252\n",
      "Epoch 106/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2004 - val_loss: 0.2187\n",
      "Epoch 107/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2027 - val_loss: 0.2227\n",
      "Epoch 108/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2038 - val_loss: 0.2122\n",
      "Epoch 109/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2047 - val_loss: 0.2117\n",
      "Epoch 110/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.1989 - val_loss: 0.2214\n",
      "Epoch 111/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.1996 - val_loss: 0.2301\n",
      "Epoch 112/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2006Restoring model weights from the end of the best epoch: 102.\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2006 - val_loss: 0.2226\n",
      "Epoch 112: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 95ms/step - loss: 3.5535 - val_loss: 1.1791\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.8019 - val_loss: 0.7918\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7524 - val_loss: 0.7768\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.7408 - val_loss: 0.7715\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7167 - val_loss: 0.7522\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6852 - val_loss: 0.6748\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.6655 - val_loss: 0.6567\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6409 - val_loss: 0.6265\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6024 - val_loss: 0.5767\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.5585 - val_loss: 0.5324\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.5208 - val_loss: 0.5076\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5084 - val_loss: 0.4854\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4988 - val_loss: 0.4982\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4936 - val_loss: 0.4889\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4868 - val_loss: 0.4742\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4672 - val_loss: 0.4525\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4679 - val_loss: 0.4529\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4507 - val_loss: 0.4421\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4460 - val_loss: 0.4384\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4439 - val_loss: 0.5464\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4786 - val_loss: 0.4329\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4312 - val_loss: 0.4174\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4205 - val_loss: 0.4208\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4060 - val_loss: 0.3930\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.3886 - val_loss: 0.3675\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4175 - val_loss: 0.3857\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3863 - val_loss: 0.3816\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4278 - val_loss: 0.3919\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3622 - val_loss: 0.3894\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3613 - val_loss: 0.3986\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3481 - val_loss: 0.3597\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3337 - val_loss: 0.3429\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3277 - val_loss: 0.3355\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3145 - val_loss: 0.3240\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3296 - val_loss: 0.3791\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3297 - val_loss: 0.3527\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3142 - val_loss: 0.3093\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3025 - val_loss: 0.2865\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3109 - val_loss: 0.3141\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2936 - val_loss: 0.3514\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3241 - val_loss: 0.3898\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3051 - val_loss: 0.2802\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3586 - val_loss: 0.4225\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4026 - val_loss: 0.3660\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3267 - val_loss: 0.3526\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3044 - val_loss: 0.3210\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2954 - val_loss: 0.3235\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2944 - val_loss: 0.3217\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3485 - val_loss: 0.3442\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3275 - val_loss: 0.3116\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3023 - val_loss: 0.2783\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.2751 - val_loss: 0.2631\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2779 - val_loss: 0.2536\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2710 - val_loss: 0.2665\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2926 - val_loss: 0.2663\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2613 - val_loss: 0.2596\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2657 - val_loss: 0.2520\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2523 - val_loss: 0.2568\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.2499 - val_loss: 0.2539\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2385 - val_loss: 0.2534\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2525 - val_loss: 0.2628\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2496 - val_loss: 0.2379\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2445 - val_loss: 0.2636\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2322 - val_loss: 0.2320\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2412 - val_loss: 0.2301\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2720 - val_loss: 0.2765\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2643 - val_loss: 0.2429\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2406 - val_loss: 0.2927\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2516 - val_loss: 0.2790\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2310 - val_loss: 0.2318\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2283 - val_loss: 0.2555\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2346 - val_loss: 0.2650\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2305 - val_loss: 0.2532\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2228 - val_loss: 0.2369\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2267Restoring model weights from the end of the best epoch: 65.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2267 - val_loss: 0.2362\n",
      "Epoch 75: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 100ms/step - loss: 3.5335 - val_loss: 0.9663\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7594 - val_loss: 0.7800\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.7502 - val_loss: 0.7695\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7391 - val_loss: 0.7643\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.7270 - val_loss: 0.7360\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6977 - val_loss: 0.6907\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6800 - val_loss: 0.6670\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.6627 - val_loss: 0.6400\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6262 - val_loss: 0.6060\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.5934 - val_loss: 0.5720\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.5459 - val_loss: 0.5084\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5093 - val_loss: 0.4920\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4871 - val_loss: 0.4780\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4901 - val_loss: 0.4848\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4762 - val_loss: 0.4614\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4633 - val_loss: 0.4577\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.4532 - val_loss: 0.4560\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4512 - val_loss: 0.4334\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.4418 - val_loss: 0.4254\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.4349 - val_loss: 0.4306\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4264 - val_loss: 0.4164\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4132 - val_loss: 0.3974\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4043 - val_loss: 0.4060\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3997 - val_loss: 0.4086\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3984 - val_loss: 0.4057\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.3883 - val_loss: 0.4261\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4198 - val_loss: 0.4133\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.4250 - val_loss: 0.4023\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3967 - val_loss: 0.3882\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3877 - val_loss: 0.3852\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3793 - val_loss: 0.3670\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.3729 - val_loss: 0.3725\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3657 - val_loss: 0.3735\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3563 - val_loss: 0.3689\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3494 - val_loss: 0.3581\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3555 - val_loss: 0.3576\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3287 - val_loss: 0.3409\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.3429 - val_loss: 0.3463\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.3325 - val_loss: 0.3224\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3165 - val_loss: 0.3357\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3020 - val_loss: 0.2903\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2855 - val_loss: 0.2720\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2693 - val_loss: 0.2766\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2778 - val_loss: 0.2750\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2689 - val_loss: 0.2757\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2633 - val_loss: 0.2761\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2721 - val_loss: 0.2559\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2631 - val_loss: 0.2876\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2530 - val_loss: 0.2696\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2505 - val_loss: 0.2647\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2490 - val_loss: 0.2941\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2436 - val_loss: 0.2613\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2400 - val_loss: 0.2491\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2356 - val_loss: 0.2522\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 97ms/step - loss: 0.2418 - val_loss: 0.2618\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2279 - val_loss: 0.2386\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2331 - val_loss: 0.2534\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2277 - val_loss: 0.2698\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2255 - val_loss: 0.2396\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2231 - val_loss: 0.2611\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2307 - val_loss: 0.2662\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2258 - val_loss: 0.2424\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 95ms/step - loss: 0.2176 - val_loss: 0.2417\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2199 - val_loss: 0.2488\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2196 - val_loss: 0.2757\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2248Restoring model weights from the end of the best epoch: 56.\n",
      "73/73 [==============================] - 7s 96ms/step - loss: 0.2248 - val_loss: 0.2666\n",
      "Epoch 66: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 96ms/step - loss: 3.4773 - val_loss: 0.9537\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.7606 - val_loss: 0.7828\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.7499 - val_loss: 0.7731\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.7366 - val_loss: 0.7617\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.7150 - val_loss: 0.7109\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.6924 - val_loss: 0.7145\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.6751 - val_loss: 0.6703\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.6531 - val_loss: 0.6382\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.6203 - val_loss: 0.6119\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5659 - val_loss: 0.5249\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5238 - val_loss: 0.5141\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5085 - val_loss: 0.4826\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4879 - val_loss: 0.4758\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4785 - val_loss: 0.4662\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4741 - val_loss: 0.4908\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4634 - val_loss: 0.4518\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4601 - val_loss: 0.4586\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4538 - val_loss: 0.4514\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4524 - val_loss: 0.4308\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4436 - val_loss: 0.4228\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4385 - val_loss: 0.4354\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4425 - val_loss: 0.4292\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.4343 - val_loss: 0.4174\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4343 - val_loss: 0.4361\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4213 - val_loss: 0.4036\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4048 - val_loss: 0.4258\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.4093 - val_loss: 0.3883\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3917 - val_loss: 0.4174\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3903 - val_loss: 0.3970\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3796 - val_loss: 0.3745\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3665 - val_loss: 0.3712\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3581 - val_loss: 0.3721\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3487 - val_loss: 0.3760\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3366 - val_loss: 0.3540\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3356 - val_loss: 0.3322\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3188 - val_loss: 0.3162\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3352 - val_loss: 0.3590\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3101 - val_loss: 0.3208\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2914 - val_loss: 0.3403\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2923 - val_loss: 0.3046\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2842 - val_loss: 0.3153\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2964 - val_loss: 0.2822\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2803 - val_loss: 0.3007\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2817 - val_loss: 0.3010\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2670 - val_loss: 0.2942\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.2736 - val_loss: 0.2971\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2627 - val_loss: 0.2765\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2687 - val_loss: 0.3030\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2597 - val_loss: 0.2836\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2607 - val_loss: 0.2704\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2537 - val_loss: 0.2746\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2527 - val_loss: 0.3287\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2637 - val_loss: 0.2827\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2485 - val_loss: 0.2639\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2547 - val_loss: 0.2565\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2497 - val_loss: 0.2619\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2411 - val_loss: 0.2746\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2422 - val_loss: 0.2537\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2431 - val_loss: 0.2607\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2427 - val_loss: 0.2506\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2404 - val_loss: 0.2628\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2403 - val_loss: 0.2694\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2378 - val_loss: 0.2524\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2390 - val_loss: 0.2549\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2413 - val_loss: 0.2561\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.2318 - val_loss: 0.2617\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2325 - val_loss: 0.2371\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2292 - val_loss: 0.2548\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2388 - val_loss: 0.2500\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2274 - val_loss: 0.2498\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2240 - val_loss: 0.2473\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2281 - val_loss: 0.2607\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2329 - val_loss: 0.2547\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2290 - val_loss: 0.2316\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2255 - val_loss: 0.2319\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2221 - val_loss: 0.2382\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2242 - val_loss: 0.2364\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2229 - val_loss: 0.2709\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2173 - val_loss: 0.2402\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2192 - val_loss: 0.2309\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2142 - val_loss: 0.2306\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2160 - val_loss: 0.2179\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2172 - val_loss: 0.2263\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2160 - val_loss: 0.2278\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2143 - val_loss: 0.2588\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2125 - val_loss: 0.2289\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2115 - val_loss: 0.2410\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2227 - val_loss: 0.2421\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2134 - val_loss: 0.2289\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2159 - val_loss: 0.2554\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.2171 - val_loss: 0.2426\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.2148Restoring model weights from the end of the best epoch: 83.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2148 - val_loss: 0.2312\n",
      "Epoch 93: early stopping\n",
      "'########################################################Model4\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 93ms/step - loss: 18.5233 - val_loss: 3.8226\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 1.4554 - val_loss: 0.9454\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8581 - val_loss: 0.9247\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8467 - val_loss: 0.9011\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8203 - val_loss: 0.8792\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7824 - val_loss: 0.8449\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7453 - val_loss: 0.8190\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.7274 - val_loss: 0.7031\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.6880 - val_loss: 0.7592\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.7125 - val_loss: 0.7090\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.7017 - val_loss: 0.6830\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6868 - val_loss: 0.7285\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7050 - val_loss: 0.7199\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.6712 - val_loss: 0.6444\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.7245 - val_loss: 0.7989\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7094 - val_loss: 0.7218\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6786 - val_loss: 0.7073\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.6502 - val_loss: 0.6520\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6317 - val_loss: 0.6496\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6297 - val_loss: 0.6559\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.6018 - val_loss: 0.6084\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.5801 - val_loss: 0.6012\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5689 - val_loss: 0.5633\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5451 - val_loss: 0.5401\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5163 - val_loss: 0.5203\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5011 - val_loss: 0.4740\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5160 - val_loss: 0.5198\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4915 - val_loss: 0.4785\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4606 - val_loss: 0.4740\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4503 - val_loss: 0.4539\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4453 - val_loss: 0.4532\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.4411 - val_loss: 0.4268\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4179 - val_loss: 0.4266\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4353 - val_loss: 0.4362\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4169 - val_loss: 0.4169\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4093 - val_loss: 0.4045\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4052 - val_loss: 0.4102\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.4036 - val_loss: 0.4193\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4003 - val_loss: 0.4188\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3958 - val_loss: 0.3882\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3880 - val_loss: 0.3800\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3850 - val_loss: 0.3809\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3815 - val_loss: 0.3811\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3817 - val_loss: 0.3715\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3795 - val_loss: 0.3677\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3803 - val_loss: 0.3658\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3724 - val_loss: 0.3622\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3760 - val_loss: 0.3603\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3600 - val_loss: 0.4066\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3605 - val_loss: 0.3573\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3674 - val_loss: 0.3519\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3630 - val_loss: 0.3493\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3664 - val_loss: 0.3507\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3640 - val_loss: 0.3523\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3505 - val_loss: 0.3527\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3511 - val_loss: 0.3437\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3552 - val_loss: 0.3459\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3549 - val_loss: 0.3665\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3552 - val_loss: 0.4066\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3501 - val_loss: 0.3445\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.3679 - val_loss: 0.3963\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3488 - val_loss: 0.3452\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3485 - val_loss: 0.3361\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3470 - val_loss: 0.3369\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3497 - val_loss: 0.3569\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3457 - val_loss: 0.3560\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3459 - val_loss: 0.3362\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3379 - val_loss: 0.3339\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3351 - val_loss: 0.3390\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3318 - val_loss: 0.3195\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3234 - val_loss: 0.3197\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3108 - val_loss: 0.2928\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.3050 - val_loss: 0.2992\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3207 - val_loss: 0.3369\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3304 - val_loss: 0.2999\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2936 - val_loss: 0.2935\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2834 - val_loss: 0.3085\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3359 - val_loss: 0.3386\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2962 - val_loss: 0.2699\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2410 - val_loss: 0.2584\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2534 - val_loss: 0.2588\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2589 - val_loss: 0.2898\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2652 - val_loss: 0.2686\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2471 - val_loss: 0.2888\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2349 - val_loss: 0.2615\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2269 - val_loss: 0.2277\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2162 - val_loss: 0.2531\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1998 - val_loss: 0.2092\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2817 - val_loss: 0.2821\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2653 - val_loss: 0.2730\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.2208 - val_loss: 0.2533\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2045 - val_loss: 0.2689\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1977 - val_loss: 0.2530\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1868 - val_loss: 0.1985\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1837 - val_loss: 0.2311\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2021 - val_loss: 0.2143\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1762 - val_loss: 0.1938\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1826 - val_loss: 0.1789\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1832 - val_loss: 0.1952\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1672 - val_loss: 0.1844\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.1586 - val_loss: 0.1940\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1588 - val_loss: 0.2468\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1670 - val_loss: 0.1747\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1555 - val_loss: 0.1660\n",
      "Epoch 105/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1461 - val_loss: 0.1642\n",
      "Epoch 106/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.1405 - val_loss: 0.2108\n",
      "Epoch 107/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1524 - val_loss: 0.1876\n",
      "Epoch 108/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1430 - val_loss: 0.1777\n",
      "Epoch 109/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1417 - val_loss: 0.1575\n",
      "Epoch 110/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1382 - val_loss: 0.1738\n",
      "Epoch 111/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1471 - val_loss: 0.1611\n",
      "Epoch 112/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1375 - val_loss: 0.1526\n",
      "Epoch 113/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1264 - val_loss: 0.1683\n",
      "Epoch 114/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1315 - val_loss: 0.1501\n",
      "Epoch 115/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1408 - val_loss: 0.1943\n",
      "Epoch 116/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.1292 - val_loss: 0.1640\n",
      "Epoch 117/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.1395 - val_loss: 0.1637\n",
      "Epoch 118/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1317 - val_loss: 0.1596\n",
      "Epoch 119/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1291 - val_loss: 0.1460\n",
      "Epoch 120/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1262 - val_loss: 0.1531\n",
      "Epoch 121/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.1222 - val_loss: 0.1648\n",
      "Epoch 122/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1339 - val_loss: 0.1512\n",
      "Epoch 123/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1244 - val_loss: 0.1452\n",
      "Epoch 124/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1423 - val_loss: 0.1781\n",
      "Epoch 125/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1432 - val_loss: 0.1503\n",
      "Epoch 126/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1242 - val_loss: 0.1463\n",
      "Epoch 127/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1200 - val_loss: 0.1567\n",
      "Epoch 128/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1232 - val_loss: 0.1449\n",
      "Epoch 129/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.1220 - val_loss: 0.1546\n",
      "Epoch 130/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.1236 - val_loss: 0.1508\n",
      "Epoch 131/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1245 - val_loss: 0.1991\n",
      "Epoch 132/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1387 - val_loss: 0.1832\n",
      "Epoch 133/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1221 - val_loss: 0.1446\n",
      "Epoch 134/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1162 - val_loss: 0.1508\n",
      "Epoch 135/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1118 - val_loss: 0.1506\n",
      "Epoch 136/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1559 - val_loss: 0.1915\n",
      "Epoch 137/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.1367 - val_loss: 0.1480\n",
      "Epoch 138/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1283 - val_loss: 0.1578\n",
      "Epoch 139/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1126 - val_loss: 0.1431\n",
      "Epoch 140/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1110 - val_loss: 0.1440\n",
      "Epoch 141/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1135 - val_loss: 0.1484\n",
      "Epoch 142/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1167 - val_loss: 0.1585\n",
      "Epoch 143/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1206 - val_loss: 0.1497\n",
      "Epoch 144/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.1116 - val_loss: 0.1558\n",
      "Epoch 145/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.1321 - val_loss: 0.1779\n",
      "Epoch 146/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1345 - val_loss: 0.1448\n",
      "Epoch 147/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1135 - val_loss: 0.1502\n",
      "Epoch 148/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1087 - val_loss: 0.1453\n",
      "Epoch 149/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.1089Restoring model weights from the end of the best epoch: 139.\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1089 - val_loss: 0.1538\n",
      "Epoch 149: early stopping\n",
      "'########################################################Model0\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 86ms/step - loss: 17.0918 - val_loss: 2.3060\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 1.0097 - val_loss: 0.9277\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.8519 - val_loss: 0.9188\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.8308 - val_loss: 0.8790\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.7961 - val_loss: 0.8255\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.7481 - val_loss: 0.7644\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7258 - val_loss: 0.7244\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6818 - val_loss: 0.6932\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6649 - val_loss: 0.6681\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.6479 - val_loss: 0.7153\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.6397 - val_loss: 0.6295\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6122 - val_loss: 0.6325\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5898 - val_loss: 0.5858\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.5717 - val_loss: 0.5621\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5445 - val_loss: 0.5403\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5256 - val_loss: 0.5124\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5111 - val_loss: 0.4874\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4747 - val_loss: 0.4617\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4604 - val_loss: 0.4521\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4538 - val_loss: 0.4364\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.4393 - val_loss: 0.4350\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.4309 - val_loss: 0.4166\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4303 - val_loss: 0.4214\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4192 - val_loss: 0.4077\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4220 - val_loss: 0.4399\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4173 - val_loss: 0.4072\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4064 - val_loss: 0.3853\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3969 - val_loss: 0.4033\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.4066 - val_loss: 0.4013\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3993 - val_loss: 0.3849\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3866 - val_loss: 0.3850\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3927 - val_loss: 0.3856\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3827 - val_loss: 0.3769\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3764 - val_loss: 0.3914\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3755 - val_loss: 0.3724\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3910 - val_loss: 0.3738\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3753 - val_loss: 0.3730\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3710 - val_loss: 0.3592\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3665 - val_loss: 0.3763\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3641 - val_loss: 0.3979\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3660 - val_loss: 0.3673\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3720 - val_loss: 0.3545\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3661 - val_loss: 0.3601\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3585 - val_loss: 0.3789\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3583 - val_loss: 0.3440\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3538 - val_loss: 0.3582\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3598 - val_loss: 0.3421\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3529 - val_loss: 0.3421\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3494 - val_loss: 0.3491\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3495 - val_loss: 0.3406\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3490 - val_loss: 0.3385\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3435 - val_loss: 0.3377\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3426 - val_loss: 0.3435\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.3444 - val_loss: 0.3393\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3467 - val_loss: 0.3365\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3424 - val_loss: 0.3546\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3393 - val_loss: 0.3492\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3376 - val_loss: 0.3257\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.3290 - val_loss: 0.3156\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3320 - val_loss: 0.3439\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3382 - val_loss: 0.3485\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3492 - val_loss: 0.3423\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3414 - val_loss: 0.3313\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3452 - val_loss: 0.3354\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3366 - val_loss: 0.3440\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3468 - val_loss: 0.3292\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3421 - val_loss: 0.3294\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3386 - val_loss: 0.3353\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3529Restoring model weights from the end of the best epoch: 59.\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3529 - val_loss: 0.3264\n",
      "Epoch 69: early stopping\n",
      "'########################################################Model1\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 9s 98ms/step - loss: 21.6348 - val_loss: 3.8574\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 1.3583 - val_loss: 0.9627\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8796 - val_loss: 0.9517\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.8725 - val_loss: 0.9414\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.8564 - val_loss: 0.9190\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.8325 - val_loss: 0.8979\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.8037 - val_loss: 0.8325\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.7581 - val_loss: 0.7659\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7066 - val_loss: 0.7128\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.6779 - val_loss: 0.6749\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.6507 - val_loss: 0.7022\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.6303 - val_loss: 0.6399\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.6173 - val_loss: 0.5918\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.5865 - val_loss: 0.6127\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.5619 - val_loss: 0.5842\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5759 - val_loss: 0.5867\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5426 - val_loss: 0.5325\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5105 - val_loss: 0.5456\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4977 - val_loss: 0.4789\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4602 - val_loss: 0.4532\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4525 - val_loss: 0.4698\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4494 - val_loss: 0.4759\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4388 - val_loss: 0.4249\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.4301 - val_loss: 0.4308\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4319 - val_loss: 0.4120\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.4174 - val_loss: 0.4289\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4159 - val_loss: 0.4171\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4073 - val_loss: 0.4144\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.4043 - val_loss: 0.4067\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4070 - val_loss: 0.3950\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3970 - val_loss: 0.4019\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3899 - val_loss: 0.3803\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3842 - val_loss: 0.3826\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3832 - val_loss: 0.3749\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3799 - val_loss: 0.3744\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3753 - val_loss: 0.3782\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3734 - val_loss: 0.3740\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3692 - val_loss: 0.3519\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3538 - val_loss: 0.3460\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3664 - val_loss: 0.3452\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3437 - val_loss: 0.3411\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3406 - val_loss: 0.3379\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3380 - val_loss: 0.3631\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3506 - val_loss: 0.3448\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3369 - val_loss: 0.3400\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3355 - val_loss: 0.3426\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3333 - val_loss: 0.3220\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3118 - val_loss: 0.3189\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3098 - val_loss: 0.3178\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.3057 - val_loss: 0.3123\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.3082 - val_loss: 0.3496\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3171 - val_loss: 0.3531\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3168 - val_loss: 0.3040\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2904 - val_loss: 0.3015\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2931 - val_loss: 0.2901\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2794 - val_loss: 0.2765\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2838 - val_loss: 0.3033\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2719 - val_loss: 0.2922\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2634 - val_loss: 0.2820\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2623 - val_loss: 0.2735\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2552 - val_loss: 0.2942\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2574 - val_loss: 0.2712\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.2612 - val_loss: 0.2742\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2527 - val_loss: 0.2698\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2464 - val_loss: 0.2834\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2491 - val_loss: 0.2731\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2387 - val_loss: 0.2566\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2323 - val_loss: 0.2858\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2416 - val_loss: 0.2686\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2636 - val_loss: 0.3082\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2460 - val_loss: 0.2622\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2344 - val_loss: 0.2627\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2302 - val_loss: 0.2526\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2350 - val_loss: 0.2466\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.2248 - val_loss: 0.2333\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2240 - val_loss: 0.2213\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2166 - val_loss: 0.2438\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2164 - val_loss: 0.2492\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2249 - val_loss: 0.2702\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2242 - val_loss: 0.2247\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2194 - val_loss: 0.2483\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2190 - val_loss: 0.2560\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2172 - val_loss: 0.2574\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.2097 - val_loss: 0.2244\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.2133 - val_loss: 0.2322\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2205 - val_loss: 0.2116\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.2139 - val_loss: 0.2122\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2088 - val_loss: 0.2141\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.2031 - val_loss: 0.2115\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2189 - val_loss: 0.2419\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2193 - val_loss: 0.2190\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1995 - val_loss: 0.2152\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1983 - val_loss: 0.1874\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2075 - val_loss: 0.1852\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.2278 - val_loss: 0.2274\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2026 - val_loss: 0.2156\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1921 - val_loss: 0.2638\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2242 - val_loss: 0.1973\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1924 - val_loss: 0.2094\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1834 - val_loss: 0.2008\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1873 - val_loss: 0.1869\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1865 - val_loss: 0.2328\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1855 - val_loss: 0.1859\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.1846 - val_loss: 0.1791\n",
      "Epoch 105/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1776 - val_loss: 0.2007\n",
      "Epoch 106/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1721 - val_loss: 0.2032\n",
      "Epoch 107/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1754 - val_loss: 0.1772\n",
      "Epoch 108/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1681 - val_loss: 0.1801\n",
      "Epoch 109/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1756 - val_loss: 0.1716\n",
      "Epoch 110/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1681 - val_loss: 0.1707\n",
      "Epoch 111/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1561 - val_loss: 0.1590\n",
      "Epoch 112/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1554 - val_loss: 0.1925\n",
      "Epoch 113/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1600 - val_loss: 0.1601\n",
      "Epoch 114/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1459 - val_loss: 0.1886\n",
      "Epoch 115/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.1498 - val_loss: 0.1792\n",
      "Epoch 116/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1440 - val_loss: 0.1829\n",
      "Epoch 117/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1475 - val_loss: 0.1565\n",
      "Epoch 118/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1386 - val_loss: 0.1475\n",
      "Epoch 119/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1378 - val_loss: 0.1728\n",
      "Epoch 120/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1328 - val_loss: 0.1351\n",
      "Epoch 121/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.1296 - val_loss: 0.1442\n",
      "Epoch 122/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1406 - val_loss: 0.1516\n",
      "Epoch 123/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1319 - val_loss: 0.1628\n",
      "Epoch 124/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1234 - val_loss: 0.1655\n",
      "Epoch 125/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1279 - val_loss: 0.1476\n",
      "Epoch 126/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1244 - val_loss: 0.1418\n",
      "Epoch 127/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1287 - val_loss: 0.1285\n",
      "Epoch 128/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1240 - val_loss: 0.1358\n",
      "Epoch 129/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1500 - val_loss: 0.1236\n",
      "Epoch 130/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1207 - val_loss: 0.1272\n",
      "Epoch 131/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.1185 - val_loss: 0.1197\n",
      "Epoch 132/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1158 - val_loss: 0.1551\n",
      "Epoch 133/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1264 - val_loss: 0.1208\n",
      "Epoch 134/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1181 - val_loss: 0.1174\n",
      "Epoch 135/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1170 - val_loss: 0.1211\n",
      "Epoch 136/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1126 - val_loss: 0.1083\n",
      "Epoch 137/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1098 - val_loss: 0.1173\n",
      "Epoch 138/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1122 - val_loss: 0.1508\n",
      "Epoch 139/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1129 - val_loss: 0.1149\n",
      "Epoch 140/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1076 - val_loss: 0.1183\n",
      "Epoch 141/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1111 - val_loss: 0.1122\n",
      "Epoch 142/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1049 - val_loss: 0.1159\n",
      "Epoch 143/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1028 - val_loss: 0.1105\n",
      "Epoch 144/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1081 - val_loss: 0.1067\n",
      "Epoch 145/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.0987 - val_loss: 0.1077\n",
      "Epoch 146/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1003 - val_loss: 0.1098\n",
      "Epoch 147/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1014 - val_loss: 0.1183\n",
      "Epoch 148/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1092 - val_loss: 0.1291\n",
      "Epoch 149/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1213 - val_loss: 0.1209\n",
      "Epoch 150/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1013 - val_loss: 0.1133\n",
      "Epoch 151/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1039 - val_loss: 0.1087\n",
      "Epoch 152/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.1030 - val_loss: 0.1142\n",
      "Epoch 153/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.0959 - val_loss: 0.1078\n",
      "Epoch 154/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0947Restoring model weights from the end of the best epoch: 144.\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.0947 - val_loss: 0.1172\n",
      "Epoch 154: early stopping\n",
      "'########################################################Model2\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 88ms/step - loss: 12.7918 - val_loss: 1.5226\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.9308 - val_loss: 0.9258\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8465 - val_loss: 0.9060\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8244 - val_loss: 0.8681\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7789 - val_loss: 0.8121\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7425 - val_loss: 0.7551\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.7043 - val_loss: 0.7217\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6866 - val_loss: 0.6931\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6863 - val_loss: 0.6883\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.6573 - val_loss: 0.6630\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6545 - val_loss: 0.6499\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6288 - val_loss: 0.6286\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6202 - val_loss: 0.6185\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.5979 - val_loss: 0.5886\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.5735 - val_loss: 0.5678\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.5548 - val_loss: 0.5442\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.5414 - val_loss: 0.5291\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.5119 - val_loss: 0.4977\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4872 - val_loss: 0.4707\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4641 - val_loss: 0.4539\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4540 - val_loss: 0.4364\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4395 - val_loss: 0.4761\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4480 - val_loss: 0.4256\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4374 - val_loss: 0.4238\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.4240 - val_loss: 0.4200\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.4220 - val_loss: 0.4283\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.4353 - val_loss: 0.4112\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.4229 - val_loss: 0.4103\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.4084 - val_loss: 0.4042\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.4123 - val_loss: 0.4086\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4089 - val_loss: 0.4329\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4056 - val_loss: 0.3932\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3894 - val_loss: 0.3730\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4330 - val_loss: 0.4222\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.4039 - val_loss: 0.4028\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4051 - val_loss: 0.4513\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3952 - val_loss: 0.4452\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3878 - val_loss: 0.3808\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3911 - val_loss: 0.3749\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.3781 - val_loss: 0.3913\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.3764 - val_loss: 0.3801\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3794 - val_loss: 0.3870\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3676 - val_loss: 0.3644\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3707 - val_loss: 0.3591\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3863 - val_loss: 0.3639\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3613 - val_loss: 0.3717\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3604 - val_loss: 0.3608\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3609 - val_loss: 0.3611\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3513 - val_loss: 0.3881\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3639 - val_loss: 0.3450\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3496 - val_loss: 0.3494\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3434 - val_loss: 0.3605\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3520 - val_loss: 0.3323\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3232 - val_loss: 0.3403\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3295 - val_loss: 0.3646\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3606 - val_loss: 0.3669\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3511 - val_loss: 0.3576\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3311 - val_loss: 0.3208\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2939 - val_loss: 0.3212\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2763 - val_loss: 0.3422\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.3778 - val_loss: 0.3557\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3594 - val_loss: 0.3423\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3491 - val_loss: 0.3378\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3538 - val_loss: 0.3371\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.3206 - val_loss: 0.3231\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2695 - val_loss: 0.2784\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2742 - val_loss: 0.2583\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2542 - val_loss: 0.2468\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2512 - val_loss: 0.2867\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.2649 - val_loss: 0.2856\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2319 - val_loss: 0.2930\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2355 - val_loss: 0.2975\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2222 - val_loss: 0.2693\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2179 - val_loss: 0.2435\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2070 - val_loss: 0.2553\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2302 - val_loss: 0.2708\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2265 - val_loss: 0.2548\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2010 - val_loss: 0.2650\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1968 - val_loss: 0.2126\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1870 - val_loss: 0.2012\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1833 - val_loss: 0.2345\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1813 - val_loss: 0.2173\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.1854 - val_loss: 0.2022\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.2226 - val_loss: 0.2636\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1988 - val_loss: 0.1997\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1755 - val_loss: 0.1741\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1688 - val_loss: 0.2349\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1731 - val_loss: 0.2323\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1988 - val_loss: 0.2246\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1753 - val_loss: 0.1818\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1554 - val_loss: 0.1702\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1740 - val_loss: 0.1762\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1816 - val_loss: 0.1983\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1620 - val_loss: 0.1601\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1495 - val_loss: 0.1805\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1483 - val_loss: 0.1921\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1553 - val_loss: 0.1827\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1438 - val_loss: 0.1730\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1580 - val_loss: 0.1709\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1575 - val_loss: 0.2901\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1699 - val_loss: 0.1682\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1468 - val_loss: 0.1759\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1406 - val_loss: 0.1791\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1562 - val_loss: 0.1537\n",
      "Epoch 105/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2176 - val_loss: 0.2276\n",
      "Epoch 106/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.1506 - val_loss: 0.1722\n",
      "Epoch 107/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.1627 - val_loss: 0.1867\n",
      "Epoch 108/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1436 - val_loss: 0.1791\n",
      "Epoch 109/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1426 - val_loss: 0.1610\n",
      "Epoch 110/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1434 - val_loss: 0.1739\n",
      "Epoch 111/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1318 - val_loss: 0.1614\n",
      "Epoch 112/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.1333 - val_loss: 0.1447\n",
      "Epoch 113/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1327 - val_loss: 0.1468\n",
      "Epoch 114/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1383 - val_loss: 0.1874\n",
      "Epoch 115/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1463 - val_loss: 0.1458\n",
      "Epoch 116/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1268 - val_loss: 0.1681\n",
      "Epoch 117/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1477 - val_loss: 0.1465\n",
      "Epoch 118/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1300 - val_loss: 0.1645\n",
      "Epoch 119/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1327 - val_loss: 0.1905\n",
      "Epoch 120/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1328 - val_loss: 0.1430\n",
      "Epoch 121/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1211 - val_loss: 0.1315\n",
      "Epoch 122/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1224 - val_loss: 0.1333\n",
      "Epoch 123/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1199 - val_loss: 0.1308\n",
      "Epoch 124/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1297 - val_loss: 0.1544\n",
      "Epoch 125/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1179 - val_loss: 0.1355\n",
      "Epoch 126/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1208 - val_loss: 0.1396\n",
      "Epoch 127/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1155 - val_loss: 0.1226\n",
      "Epoch 128/2000\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.1162 - val_loss: 0.1424\n",
      "Epoch 129/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1191 - val_loss: 0.1300\n",
      "Epoch 130/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1196 - val_loss: 0.1557\n",
      "Epoch 131/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1133 - val_loss: 0.1225\n",
      "Epoch 132/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.1270 - val_loss: 0.1222\n",
      "Epoch 133/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.1130 - val_loss: 0.1488\n",
      "Epoch 134/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1210 - val_loss: 0.1194\n",
      "Epoch 135/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1210 - val_loss: 0.1410\n",
      "Epoch 136/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1208 - val_loss: 0.1388\n",
      "Epoch 137/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1139 - val_loss: 0.1323\n",
      "Epoch 138/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1198 - val_loss: 0.1382\n",
      "Epoch 139/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1143 - val_loss: 0.1313\n",
      "Epoch 140/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1079 - val_loss: 0.1356\n",
      "Epoch 141/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1055 - val_loss: 0.1242\n",
      "Epoch 142/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1084 - val_loss: 0.1359\n",
      "Epoch 143/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1056 - val_loss: 0.1158\n",
      "Epoch 144/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1176 - val_loss: 0.1100\n",
      "Epoch 145/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1050 - val_loss: 0.1132\n",
      "Epoch 146/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1046 - val_loss: 0.1147\n",
      "Epoch 147/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1048 - val_loss: 0.1202\n",
      "Epoch 148/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1039 - val_loss: 0.1319\n",
      "Epoch 149/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1063 - val_loss: 0.1264\n",
      "Epoch 150/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1051 - val_loss: 0.1185\n",
      "Epoch 151/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.1263 - val_loss: 0.1134\n",
      "Epoch 152/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1016 - val_loss: 0.1122\n",
      "Epoch 153/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1006 - val_loss: 0.1175\n",
      "Epoch 154/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.1004Restoring model weights from the end of the best epoch: 144.\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1004 - val_loss: 0.1127\n",
      "Epoch 154: early stopping\n",
      "'########################################################Model3\n",
      "Epoch 1/2000\n",
      "73/73 [==============================] - 8s 97ms/step - loss: 17.0666 - val_loss: 3.1670\n",
      "Epoch 2/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 1.2445 - val_loss: 0.9265\n",
      "Epoch 3/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.8482 - val_loss: 0.9082\n",
      "Epoch 4/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.8347 - val_loss: 0.8977\n",
      "Epoch 5/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.8196 - val_loss: 0.8834\n",
      "Epoch 6/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7907 - val_loss: 0.8298\n",
      "Epoch 7/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7545 - val_loss: 0.7699\n",
      "Epoch 8/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.7077 - val_loss: 0.7312\n",
      "Epoch 9/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.6821 - val_loss: 0.6829\n",
      "Epoch 10/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.6708 - val_loss: 0.6626\n",
      "Epoch 11/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.6440 - val_loss: 0.6650\n",
      "Epoch 12/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.6393 - val_loss: 0.6497\n",
      "Epoch 13/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.5978 - val_loss: 0.6155\n",
      "Epoch 14/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.6231 - val_loss: 0.7241\n",
      "Epoch 15/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.6090 - val_loss: 0.6379\n",
      "Epoch 16/2000\n",
      "73/73 [==============================] - 6s 80ms/step - loss: 0.5759 - val_loss: 0.5739\n",
      "Epoch 17/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.5459 - val_loss: 0.5629\n",
      "Epoch 18/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.5248 - val_loss: 0.5202\n",
      "Epoch 19/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.5091 - val_loss: 0.4926\n",
      "Epoch 20/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4790 - val_loss: 0.4712\n",
      "Epoch 21/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4571 - val_loss: 0.4473\n",
      "Epoch 22/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4423 - val_loss: 0.4327\n",
      "Epoch 23/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4316 - val_loss: 0.4235\n",
      "Epoch 24/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4222 - val_loss: 0.4289\n",
      "Epoch 25/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4123 - val_loss: 0.4133\n",
      "Epoch 26/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.4145 - val_loss: 0.4071\n",
      "Epoch 27/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.4018 - val_loss: 0.3940\n",
      "Epoch 28/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3988 - val_loss: 0.3905\n",
      "Epoch 29/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3913 - val_loss: 0.3837\n",
      "Epoch 30/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3850 - val_loss: 0.3796\n",
      "Epoch 31/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3945 - val_loss: 0.3838\n",
      "Epoch 32/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.3923 - val_loss: 0.3770\n",
      "Epoch 33/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.3796 - val_loss: 0.3693\n",
      "Epoch 34/2000\n",
      "73/73 [==============================] - 6s 79ms/step - loss: 0.3753 - val_loss: 0.3768\n",
      "Epoch 35/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3738 - val_loss: 0.3650\n",
      "Epoch 36/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3683 - val_loss: 0.3584\n",
      "Epoch 37/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3687 - val_loss: 0.3709\n",
      "Epoch 38/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.3719 - val_loss: 0.3586\n",
      "Epoch 39/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3619 - val_loss: 0.3646\n",
      "Epoch 40/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3684 - val_loss: 0.3548\n",
      "Epoch 41/2000\n",
      "73/73 [==============================] - 7s 94ms/step - loss: 0.3628 - val_loss: 0.3476\n",
      "Epoch 42/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.3596 - val_loss: 0.3452\n",
      "Epoch 43/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3591 - val_loss: 0.3585\n",
      "Epoch 44/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3602 - val_loss: 0.3428\n",
      "Epoch 45/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3537 - val_loss: 0.3667\n",
      "Epoch 46/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3548 - val_loss: 0.3419\n",
      "Epoch 47/2000\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 0.3517 - val_loss: 0.3488\n",
      "Epoch 48/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3478 - val_loss: 0.3561\n",
      "Epoch 49/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3519 - val_loss: 0.3372\n",
      "Epoch 50/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.3487 - val_loss: 0.3413\n",
      "Epoch 51/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3432 - val_loss: 0.4341\n",
      "Epoch 52/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.3563 - val_loss: 0.3595\n",
      "Epoch 53/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.3415 - val_loss: 0.3337\n",
      "Epoch 54/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.3402 - val_loss: 0.3431\n",
      "Epoch 55/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3490 - val_loss: 0.3423\n",
      "Epoch 56/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.3412 - val_loss: 0.3380\n",
      "Epoch 57/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.3320 - val_loss: 0.3241\n",
      "Epoch 58/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.3141 - val_loss: 0.3300\n",
      "Epoch 59/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.3080 - val_loss: 0.3038\n",
      "Epoch 60/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.2996 - val_loss: 0.3260\n",
      "Epoch 61/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2898 - val_loss: 0.2947\n",
      "Epoch 62/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2805 - val_loss: 0.3027\n",
      "Epoch 63/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2840 - val_loss: 0.2767\n",
      "Epoch 64/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.2643 - val_loss: 0.2569\n",
      "Epoch 65/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2498 - val_loss: 0.2690\n",
      "Epoch 66/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.2302 - val_loss: 0.2402\n",
      "Epoch 67/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.2241 - val_loss: 0.2335\n",
      "Epoch 68/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.2117 - val_loss: 0.2366\n",
      "Epoch 69/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.2101 - val_loss: 0.2140\n",
      "Epoch 70/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1971 - val_loss: 0.2371\n",
      "Epoch 71/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1986 - val_loss: 0.2015\n",
      "Epoch 72/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1920 - val_loss: 0.1895\n",
      "Epoch 73/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.2007 - val_loss: 0.1902\n",
      "Epoch 74/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1924 - val_loss: 0.1776\n",
      "Epoch 75/2000\n",
      "73/73 [==============================] - 6s 85ms/step - loss: 0.1780 - val_loss: 0.2374\n",
      "Epoch 76/2000\n",
      "73/73 [==============================] - 6s 78ms/step - loss: 0.1649 - val_loss: 0.2066\n",
      "Epoch 77/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1644 - val_loss: 0.1779\n",
      "Epoch 78/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1702 - val_loss: 0.1676\n",
      "Epoch 79/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1671 - val_loss: 0.1761\n",
      "Epoch 80/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1617 - val_loss: 0.1655\n",
      "Epoch 81/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1528 - val_loss: 0.1728\n",
      "Epoch 82/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1481 - val_loss: 0.1615\n",
      "Epoch 83/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1471 - val_loss: 0.1638\n",
      "Epoch 84/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1520 - val_loss: 0.1544\n",
      "Epoch 85/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1613 - val_loss: 0.1570\n",
      "Epoch 86/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1474 - val_loss: 0.1448\n",
      "Epoch 87/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1483 - val_loss: 0.2274\n",
      "Epoch 88/2000\n",
      "73/73 [==============================] - 7s 89ms/step - loss: 0.1680 - val_loss: 0.1487\n",
      "Epoch 89/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1431 - val_loss: 0.1524\n",
      "Epoch 90/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1571 - val_loss: 0.2039\n",
      "Epoch 91/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1481 - val_loss: 0.1508\n",
      "Epoch 92/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1439 - val_loss: 0.1385\n",
      "Epoch 93/2000\n",
      "73/73 [==============================] - 6s 83ms/step - loss: 0.1409 - val_loss: 0.1512\n",
      "Epoch 94/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1435 - val_loss: 0.1555\n",
      "Epoch 95/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1548 - val_loss: 0.1413\n",
      "Epoch 96/2000\n",
      "73/73 [==============================] - 6s 87ms/step - loss: 0.1390 - val_loss: 0.1404\n",
      "Epoch 97/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1477 - val_loss: 0.1310\n",
      "Epoch 98/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1369 - val_loss: 0.1692\n",
      "Epoch 99/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1417 - val_loss: 0.1478\n",
      "Epoch 100/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1363 - val_loss: 0.1395\n",
      "Epoch 101/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1370 - val_loss: 0.1446\n",
      "Epoch 102/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1801 - val_loss: 0.1483\n",
      "Epoch 103/2000\n",
      "73/73 [==============================] - 7s 91ms/step - loss: 0.1411 - val_loss: 0.1597\n",
      "Epoch 104/2000\n",
      "73/73 [==============================] - 6s 89ms/step - loss: 0.1454 - val_loss: 0.2010\n",
      "Epoch 105/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1583 - val_loss: 0.1407\n",
      "Epoch 106/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1280 - val_loss: 0.1306\n",
      "Epoch 107/2000\n",
      "73/73 [==============================] - 7s 90ms/step - loss: 0.1686 - val_loss: 0.2051\n",
      "Epoch 108/2000\n",
      "73/73 [==============================] - 7s 93ms/step - loss: 0.1788 - val_loss: 0.1524\n",
      "Epoch 109/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1497 - val_loss: 0.1784\n",
      "Epoch 110/2000\n",
      "73/73 [==============================] - 7s 92ms/step - loss: 0.1439 - val_loss: 0.1479\n",
      "Epoch 111/2000\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 0.1320 - val_loss: 0.1679\n",
      "Epoch 112/2000\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1976 - val_loss: 0.1698\n",
      "Epoch 113/2000\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 0.1395 - val_loss: 0.1451\n",
      "Epoch 114/2000\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 0.1317 - val_loss: 0.1459\n",
      "Epoch 115/2000\n",
      "73/73 [==============================] - 6s 84ms/step - loss: 0.1264 - val_loss: 0.1411\n",
      "Epoch 116/2000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.1261Restoring model weights from the end of the best epoch: 106.\n",
      "73/73 [==============================] - 6s 77ms/step - loss: 0.1261 - val_loss: 0.1357\n",
      "Epoch 116: early stopping\n",
      "'########################################################Model4\n"
     ]
    }
   ],
   "source": [
    "model_num = 5\n",
    "\n",
    "\n",
    "mase_models = train_bagging_models(model_num, MASE(y_train,24),2000,10,8,0.0001)\n",
    "mape_models = train_bagging_models(model_num,'mape',2000,10,8,0.0001)\n",
    "smape_models = train_bagging_models(model_num, SMAPE(),2000,10,8,0.0001)\n",
    "mae_models = train_bagging_models(model_num, 'mae',2000,10,8,0.0001)\n",
    "mse_models = train_bagging_models(model_num, 'mse',2000,10,8,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35de203-07f8-48f9-8ede-0a0bcba1ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 29ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 30ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 33ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n",
      "12/12 [==============================] - 1s 32ms/step\n",
      "12/12 [==============================] - 1s 30ms/step\n",
      "12/12 [==============================] - 1s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1,_=smape_models\n",
    "pred2,_=mase_models\n",
    "pred3,_=mape_models\n",
    "pred4,_=mae_models\n",
    "pred5,_=mse_models\n",
    "\n",
    "smape_predictions = bagging_predict2(pred1, test_X)\n",
    "mase_predictions = bagging_predict2(pred2, test_X)\n",
    "mape_predictions = bagging_predict2(pred3, test_X)\n",
    "mae_predictions = bagging_predict2(pred4, test_X)\n",
    "mse_predictions = bagging_predict2(pred5, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed0c7ce-ad6f-47e0-b18a-7e5a1e0a7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(fin_pred.reshape(-1,24)).to_csv(\"pred/lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fd3e88-6523-4042-a996-ad1f65d9fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34226954, 0.22443897)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([smape_predictions, mase_predictions,mape_predictions,mae_predictions,mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "pd.DataFrame(fin_pred.flatten()).to_csv(\"result7/lstm.csv\")\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce6054d-bd3d-4981-90dc-91c0ab363196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34471175, 0.22660251)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([mae_predictions, mase_predictions,mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "pd.DataFrame(fin_pred.flatten()).to_csv(\"best7/LSTM.csv\")\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6a62d0-8e46-487c-a3a5-b525b4d8f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34499446, 0.22593692)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([smape_predictions, mase_predictions,mape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1291c905-5ea5-4b7f-9852-aed9fb1b361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38135096, 0.24693182)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "concat = np.concatenate([smape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccfe8c32-66e3-41f3-81d4-fdf27365d9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3615845, 0.2373837)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mape_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f869349b-d29e-489f-97bc-d9b6df52b835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3558516, 0.23579495)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mase_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5f77bb-cc32-44a1-8384-6ff5cb753cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3508729, 0.2302761)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mae_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb0678ca-6617-4379-bc86-4b882056dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35632575, 0.24217606)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat = np.concatenate([mse_predictions],axis=0)\n",
    "fin_pred = np.median(concat,axis=0)\n",
    "#pd.DataFrame(fin_pred).to_csv(\"freezing_I.csv\")\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred.flatten())),mean_absolute_error(test_y.flatten(),fin_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30cdd8e4-0d65-4983-ad5c-5773907577b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34584\n"
     ]
    }
   ],
   "source": [
    "concat_mase = np.concatenate([np.nan_to_num(np.array(mase_predictions), nan=0)])\n",
    "fin_pred_mase = np.median(concat_mase,axis=0)\n",
    "MASE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mase .flatten())).round(5)\n",
    "\n",
    "concat_mape = np.concatenate([np.nan_to_num(np.array(mape_predictions), nan=0)])\n",
    "fin_pred_mape = np.median(concat_mape,axis=0)\n",
    "MAPE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mape .flatten())).round(5)\n",
    "\n",
    "concat_smape = np.concatenate([np.nan_to_num(np.array(smape_predictions), nan=0)])\n",
    "fin_pred_smape = np.median(concat_smape,axis=0)\n",
    "sMAPE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_smape .flatten())).round(5)\n",
    "\n",
    "concat_mae = np.concatenate([np.nan_to_num(np.array(mae_predictions), nan=0)])\n",
    "fin_pred_mae = np.median(concat_mae,axis=0)\n",
    "MAE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mae .flatten())).round(5)\n",
    "\n",
    "concat_mse = np.concatenate([np.nan_to_num(np.array(mse_predictions), nan=0)])\n",
    "fin_pred_mse = np.median(concat_mse,axis=0)\n",
    "MSE= np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_mse .flatten())).round(5)\n",
    "\n",
    "performance = np.array([MASE, MAPE,sMAPE,MAE,MSE])\n",
    "beta = 1  # 조정 파라미터\n",
    "weights = np.exp(-beta * performance)\n",
    "\n",
    "gd= np.concatenate([fin_pred_mase.flatten().reshape(1,-1),\n",
    "                    fin_pred_mape.flatten().reshape(1,-1),\n",
    "                   fin_pred_smape.flatten().reshape(1,-1),\n",
    "                   fin_pred_mae.flatten().reshape(1,-1),\n",
    "                   fin_pred_mse.flatten().reshape(1,-1)],axis=0)\n",
    "\n",
    "normalized_weights = weights / np.sum(weights)\n",
    "\n",
    "# 각 모델의 예측값에 가중치를 부여하여 앙상블 예측 생성\n",
    "ensemble_prediction = np.dot(normalized_weights, gd)\n",
    "print(np.sqrt(mean_squared_error(test_y.flatten(),ensemble_prediction)).round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48d539f-953b-4d30-9c12-9b1195de25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ensemble_prediction).to_csv(\"exp7/vanila_LSTM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
