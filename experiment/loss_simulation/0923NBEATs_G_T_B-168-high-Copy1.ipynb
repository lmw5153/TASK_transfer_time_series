{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7f5360-ea1a-4d72-99c6-a200aa3893ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34721, 168)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ebab801-077c-43dc-9f56-b335c1d5fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nbeats_keras.model import NBeatsNet as NBeatsKeras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from nbeats_pytorch.model import NBeatsNet as NBeatsPytorch\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import time\n",
    "from keras.models import load_model\n",
    "#from target_data_electronic70_7 import target_X, target_y ,test_X, test_y\n",
    "#from m4databasis21_7 import base_domain,zt_in,zt_out,M4Meta,inputsize,train_12,train_12_y\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error,r2_score\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "#from m4databasis35_7_70_7 import train_35,train_35_y,train_70,train_70_y\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Add, Concatenate,Flatten,Reshape\n",
    "#import pandas as pd\n",
    "from keras.utils import custom_object_scope\n",
    "\n",
    "#####################################################################################\n",
    "X_train = pd.read_csv(\"../task_transfer_solor_24/data/M4_train.csv\").iloc[:,(1):].values\n",
    "y_train = pd.read_csv(\"../task_transfer_solor_24/data/M4_test.csv\").iloc[:,1:].values\n",
    "X_train.shape[1], y_train.shape[1]\n",
    "\n",
    "target_X= pd.read_csv(\"high_X.csv\").iloc[:,(1+24*0):].values\n",
    "target_y =pd.read_csv(\"high_y.csv\").iloc[:,1:].values\n",
    "#test_X= pd.read_csv(\"../data/solor_val_input_7.csv\").iloc[:,(1+24*0):].values\n",
    "#test_y =pd.read_csv(\"../data/solor_val_output_7.csv\").iloc[:,1:].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "target_X,test_X,target_y,test_y=train_test_split(target_X,target_y,random_state=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X_train=target_X\n",
    "#y_train=target_y\n",
    "\n",
    "backcast_length = X_train.shape[1]\n",
    "forecast_length = y_train.shape[1]\n",
    "\n",
    "X_train.shape,test_X.shape,y_train.shape,test_y.shape\n",
    "\n",
    "target_X.shape,test_X.shape\n",
    "#################################################################################\n",
    "# loss SMAPE\n",
    "class SMAPE(Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 예측 값의 차원을 맞춤\n",
    "       # y_pred=tf.clip_by_value(y_pred, 1e-10, tf.reduce_max(y_pred))\n",
    "       # y_true = tf.clip_by_value(y_true, 1e-10, tf.reduce_max(y_true))\n",
    "        \n",
    "        numerator = 100 * tf.abs(y_true- y_pred )\n",
    "        denominator =  (tf.abs(y_true ) + tf.abs(y_pred))/2\n",
    "        smape =  numerator /  denominator #tf.clip_by_value(denominator, 1e-10, tf.reduce_max(denominator))\n",
    "        return tf.reduce_mean(smape)\n",
    "\n",
    "#################################################################################\n",
    "# loss MASE\n",
    "class MASE(Loss):\n",
    "    def __init__(self, training_data, period, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.scale = self.calculate_scale(training_data, period)\n",
    "    def seasonal_diff(data, period):\n",
    "        return data[period:] - data[:-period]\n",
    "\n",
    "    def calculate_scale(self, training_data, period):\n",
    "        # 주기 차분 계산\n",
    "        diff = seasonal_diff(training_data, period)\n",
    "        scale = np.mean(np.abs(diff))\n",
    "        return scale\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.reshape(y_pred, tf.shape(y_true))  # 차원 맞추기\n",
    "        error = tf.abs(y_true - y_pred)\n",
    "        return tf.reduce_mean(error / self.scale)\n",
    "\n",
    "def seasonal_diff(data, period):\n",
    "    return data[period:] - data[:-period]\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "# 하이퍼파라미터 인자 설정\n",
    "def hyperparameter():\n",
    "    # 1 backcast\n",
    "    # 2 forecast\n",
    "    # 3 inputdim\n",
    "    # 4 outputdim\n",
    "    # 5 unit\n",
    "    # 6 bacth size\n",
    "    return X_train.shape[1], y_train.shape[1],1,1,128\n",
    "\n",
    "#################################################################################\n",
    "# nbeats + I모델 생성 함수\n",
    "def bulid_model(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.TREND_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK,\n",
    "                                    NBeatsKeras.SEASONALITY_BLOCK)\n",
    "                   ,nb_blocks_per_stack=1, thetas_dim=(1,2,2,4,4,4),\n",
    "                   share_weights_in_stack=True, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + G모델 생성 함수    \n",
    "def bulid_model_G(backcast_,forecast_,input_dim,output_dim,unit):\n",
    "    model= NBeatsKeras(backcast_length=backcast_, \n",
    "                       forecast_length=forecast_,\n",
    "                       input_dim=input_dim,\n",
    "                       output_dim=output_dim,\n",
    "                       stack_types=(NBeatsKeras.GENERIC_BLOCK,NBeatsKeras.GENERIC_BLOCK)\n",
    "                   ,nb_blocks_per_stack=5, thetas_dim=(4,4),\n",
    "                   share_weights_in_stack=False, hidden_layer_units=unit)\n",
    "    return model \n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, verbose = 0, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "#################################################################################\n",
    "# nbeats + I모델 부트스트랩 샘플링 배깅\n",
    "\n",
    "def train_bagging_models_G(num_models, loss_fn , epochs_, patience_,batch_size_,lr):\n",
    "    models = {}\n",
    "    backcast,forecast,in_dim,out_dim,unit = hyperparameter()\n",
    "    historys = []\n",
    "    for n in range(num_models):\n",
    "        K.clear_session()\n",
    "        model = bulid_model_G(backcast,forecast,in_dim,out_dim,unit)\n",
    "       # model.set_weights(pretrained_weights)  # 전이 학습 가중치 적용\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer , loss=loss_fn)\n",
    "        \n",
    "        # 부트스트랩 샘플링\n",
    "        select = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_bootstrap = X_train[select]\n",
    "        y_bootstrap = y_train[select]\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience = patience_, restore_best_weights=True)\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, batch_size = batch_size_,\n",
    "                  epochs=epochs_, verbose=0, \n",
    "                  callbacks=[early_stop],\n",
    "                 validation_split = 0.2)\n",
    "        models[f'model_{n+1}'] = model\n",
    "        historys.append(history)\n",
    "        #models.append(model)\n",
    "        print(f\"'########################################################Model{n}\")\n",
    "    return models,historys\n",
    "\n",
    "#################################################################################\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[np.newaxis, ...]\n",
    "\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.pe[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)\n",
    "##########################################################################################\n",
    "# 트랜스퍼 레이어\n",
    "def create_model(fn,d_model, nlayers, nhead, dropout, iw, ow,lr,pretrained_output_reshaped,inputs):\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(pretrained_output_reshaped)\n",
    "    x = layers.Dense(d_model, activation='relu')(x)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "    x = pos_encoding(x)\n",
    "    \n",
    "    for _ in range(nlayers):\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model, dropout=dropout)(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        ffn_output = layers.Dense(d_model, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    x = layers.Dense(d_model // 2, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    \n",
    "    outputs = layers.Dense((iw + ow) // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(ow)(outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    target_model = Model(inputs=inputs, outputs=outputs)\n",
    "    target_model.compile(optimizer=optimizer, loss=fn)\n",
    "    \n",
    "    return target_model\n",
    "########################################################################################################################\n",
    "def transfer_(model_num,models,trainable,\n",
    "              lossf,epochs_,batch_size_,pt,lr_):\n",
    "    #model_mapes_G = {}\n",
    "    #history_mapes_G = []\n",
    "    model_pred = []\n",
    "    for i in range(1, model_num+1):\n",
    "        K.clear_session()\n",
    "        model_name = f'model_{i}'\n",
    "        m,_ = models#[model_name]\n",
    "        model1= m[model_name]\n",
    "        # 모든 레이어를 학습 불가능하게 설정\n",
    "        for layer in model1.layers[:-1]:  # 새로운 레이어 추가된 부분은 학습 가능하도록 설정\n",
    "            layer.trainable = trainable\n",
    "        pretrained_layers = model1.layers[:-1]\n",
    "        \n",
    "        pretrained_model = Model(inputs=model1.input, outputs=pretrained_layers[-1].output)\n",
    "        \n",
    "        inputs = Input(shape=(X_train.shape[1], 1))\n",
    "        pretrained_output = pretrained_model(inputs)\n",
    "        normalized_output = layers.BatchNormalization()(pretrained_output)\n",
    "\n",
    "        pretrained_output_reshaped = layers.Reshape((y_train.shape[1], -1))(normalized_output)\n",
    "            \n",
    "        model_instance = create_model(lossf ,d_model=64, nlayers=1,nhead=1, dropout=0.1, iw=X_train.shape[1], ow=y_train.shape[1],lr=lr_\n",
    "                                      ,pretrained_output_reshaped=pretrained_output_reshaped,inputs=inputs)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=pt, verbose=0, restore_best_weights=True)\n",
    "    \n",
    "        history = model_instance.fit(target_X, target_y, batch_size = batch_size_,\n",
    "                      epochs=epochs_, verbose=0, \n",
    "                      callbacks=[early_stop],\n",
    "                     validation_split = 0.2)\n",
    "        pred = model_instance.predict(test_X)\n",
    "        pred = pred.reshape(-1,y_train.shape[1])\n",
    "        model_pred.append(pred)\n",
    "        #history_mapes_G.append(history)\n",
    "        #model_mapes_G[f'model_{i}'] =model_instance\n",
    "        #del model_instance\n",
    "        print(f\"'########################################################fitted{i}\")\n",
    "    return model_pred\n",
    "\n",
    "def transfer_save(model_num, model_paths, trainable, lossf, epochs_, batch_size_, pt, lr_):\n",
    "    history_mapes_G = []\n",
    "    model_pred = []\n",
    "    model_path = model_paths\n",
    "    for i in range(1, model_num + 1):\n",
    "        K.clear_session()\n",
    "        \n",
    "        # 저장된 모델 경로에서 모델 불러오기\n",
    "        #model_path = model_paths[i - 1]  # 각 모델의 경로를 리스트로 받아 처리\n",
    "        #model_paths = model_path #+ f'_model_G_{i}.h5'\n",
    "        model1 = load_model(model_path+ f'_model_G_{i}.h5')  # 저장된 모델 불러오기\n",
    "        print(f\"{model_path}_model_G_{i}.h5\")\n",
    "        \n",
    "        # 모든 레이어를 학습 불가능하게 설정 (필요한 경우)\n",
    "        for layer in model1.layers[:-1]:  # 마지막 레이어를 제외하고 학습 가능 여부 설정\n",
    "            layer.trainable = trainable\n",
    "        \n",
    "        pretrained_layers = model1.layers[:-1]\n",
    "        \n",
    "        # 전이 학습을 위한 새로운 입력과 모델 구성\n",
    "        pretrained_model = Model(inputs=model1.input, outputs=pretrained_layers[-1].output)\n",
    "        inputs = Input(shape=(X_train.shape[1], 1))\n",
    "        pretrained_output = pretrained_model(inputs)\n",
    "        pretrained_output_reshaped = layers.Reshape((y_train.shape[1], -1))(pretrained_output)\n",
    "        \n",
    "        # 새로운 모델 생성 및 전이 학습\n",
    "        model_instance = create_model(\n",
    "            lossf, d_model=64, nlayers=1, nhead=1, dropout=0.1, iw=X_train.shape[1], \n",
    "            ow=y_train.shape[1], lr=lr_, pretrained_output_reshaped=pretrained_output_reshaped, inputs=inputs\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=pt, verbose=0, restore_best_weights=True)\n",
    "    \n",
    "        history = model_instance.fit(\n",
    "            target_X, target_y, batch_size=batch_size_, epochs=epochs_, verbose=0,\n",
    "            callbacks=[early_stop], validation_split=0.2\n",
    "        )\n",
    "        \n",
    "        pred = model_instance.predict(test_X)\n",
    "        pred = pred.reshape(-1, y_train.shape[1])\n",
    "        model_pred.append(pred)\n",
    "        history_mapes_G.append(history)\n",
    "        \n",
    "        print(f\"######################################################## fitted {i}\")\n",
    "    \n",
    "    return model_pred\n",
    "\n",
    "def transfer_SMAPE(model_num, model_paths, trainable, lossf, epochs_, batch_size_, pt, lr_):\n",
    "    history_mapes_G = []\n",
    "    model_pred = []\n",
    "    model_path = model_paths\n",
    "    \n",
    "    for i in range(1, model_num + 1):\n",
    "        K.clear_session()\n",
    "        \n",
    "        with custom_object_scope({'SMAPE': SMAPE}):  # SMAPE 등록\n",
    "            model1 = load_model(model_path + f'_model_G_{i}.h5')  # 저장된 모델 불러오기\n",
    "        \n",
    "        print(f\"{model_path}_model_G_{i}.h5\")\n",
    "        \n",
    "        # 모든 레이어를 학습 불가능하게 설정 (필요한 경우)\n",
    "        for layer in model1.layers[:-1]:  # 마지막 레이어를 제외하고 학습 가능 여부 설정\n",
    "            layer.trainable = trainable\n",
    "        \n",
    "        pretrained_layers = model1.layers[:-1]\n",
    "        \n",
    "        # 전이 학습을 위한 새로운 입력과 모델 구성\n",
    "        pretrained_model = Model(inputs=model1.input, outputs=pretrained_layers[-1].output)\n",
    "        inputs = Input(shape=(X_train.shape[1], 1))\n",
    "        pretrained_output = pretrained_model(inputs)\n",
    "        pretrained_output_reshaped = layers.Reshape((y_train.shape[1], -1))(pretrained_output)\n",
    "        \n",
    "        # 새로운 모델 생성 및 전이 학습\n",
    "        model_instance = create_model(\n",
    "            lossf, d_model=64, nlayers=1, nhead=1, dropout=0.1, iw=X_train.shape[1], \n",
    "            ow=y_train.shape[1], lr=lr_, pretrained_output_reshaped=pretrained_output_reshaped, inputs=inputs\n",
    "        )\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=pt, verbose=0, restore_best_weights=True)\n",
    "    \n",
    "        history = model_instance.fit(\n",
    "            target_X, target_y, batch_size=batch_size_, epochs=epochs_, verbose=0,\n",
    "            callbacks=[early_stop], validation_split=0.2\n",
    "        )\n",
    "        \n",
    "        pred = model_instance.predict(test_X)\n",
    "        pred = pred.reshape(-1, y_train.shape[1])\n",
    "        model_pred.append(pred)\n",
    "        history_mapes_G.append(history)\n",
    "        \n",
    "        print(f\"######################################################## fitted {i}\")\n",
    "    \n",
    "    return model_pred\n",
    "#################################################################################\n",
    "# 예측\n",
    "\n",
    "def bagging_predict(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return np.median(predictions, axis=0)\n",
    "\n",
    "def bagging_predict2(models, X):\n",
    "    predictions = np.array([model.predict(X) for model in models.values()])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27e2cd6-1f6f-4c32-94ef-a199729c9e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'########################################################Model0\n",
      "'########################################################Model1\n",
      "'########################################################Model2\n",
      "'########################################################Model3\n",
      "'########################################################Model4\n",
      "'########################################################Model5\n",
      "'########################################################Model6\n",
      "'########################################################Model7\n",
      "'########################################################Model8\n",
      "'########################################################Model9\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted1\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted2\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "'########################################################fitted3\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted4\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted6\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted7\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "'########################################################fitted8\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "'########################################################fitted9\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "'########################################################fitted10\n"
     ]
    }
   ],
   "source": [
    "#mape_models = train_bagging_models_G(10,'mape',100,10,1024,0.001)\n",
    "#smape_models = train_bagging_models_G(10,SMAPE(),100,10,1024,0.001)\n",
    "mase_models = train_bagging_models_G(10,MASE(y_train,y_train.shape[1]),100,10,1024,0.001)\n",
    "mase_pred = transfer_(10,mase_models,True, MASE(target_y,y_train.shape[1]),1000,8,30,0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc3de2-b399-4314-bd60-74b77fcba0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0574cf8-8fdc-4c23-be47-7bfb89881baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_1.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 1\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_2.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 2\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_3.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 3\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_4.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 4\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_5.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 5\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_6.h5\n",
      "4/4 [==============================] - 1s 4ms/step\n",
      "######################################################## fitted 6\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_7.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 7\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_8.h5\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "######################################################## fitted 8\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_9.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 9\n",
      "../task_transfer_ele_24/train7/mape_models_G/mape_model_G_10.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 10\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_1.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 1\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_2.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 2\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_3.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 3\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_4.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 4\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_5.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 5\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_6.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 6\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_7.h5\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "######################################################## fitted 7\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_8.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 8\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_9.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 9\n",
      "../task_transfer_ele_24/train7/mae_models_G/mae_model_G_10.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 10\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_1.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 1\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_2.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 2\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_3.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 3\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_4.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 4\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_5.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 5\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_6.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 6\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_7.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 7\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_8.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 8\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_9.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 9\n",
      "../task_transfer_ele_24/train7/mse_models_G/mse_model_G_10.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 10\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_1.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/losses.py\", line 2663, in get\n        return deserialize(identifier)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/losses.py\", line 2617, in deserialize\n        return deserialize_keras_object(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/saving/legacy/serialization.py\", line 557, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'smape'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m mae_pred \u001b[38;5;241m=\u001b[39m transfer_save(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../task_transfer_ele_24/train7/mae_models_G/mae\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m0.00005\u001b[39m)\n\u001b[1;32m      3\u001b[0m mse_pred \u001b[38;5;241m=\u001b[39m transfer_save(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../task_transfer_ele_24/train7/mse_models_G/mse\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m0.00005\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m smape_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtransfer_SMAPE\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../task_transfer_ele_24/train7/smape_models_G/smape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.00005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 354\u001b[0m, in \u001b[0;36mtransfer_SMAPE\u001b[0;34m(model_num, model_paths, trainable, lossf, epochs_, batch_size_, pt, lr_)\u001b[0m\n\u001b[1;32m    347\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[1;32m    348\u001b[0m     lossf, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, nlayers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, iw\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m    349\u001b[0m     ow\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], lr\u001b[38;5;241m=\u001b[39mlr_, pretrained_output_reshaped\u001b[38;5;241m=\u001b[39mpretrained_output_reshaped, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    352\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpt, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 354\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m    357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m pred \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mpredict(test_X)\n\u001b[1;32m    360\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filetbux9y5e.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/losses.py\", line 2663, in get\n        return deserialize(identifier)\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/losses.py\", line 2617, in deserialize\n        return deserialize_keras_object(\n    File \"/home/student/anaconda3/envs/py310/lib/python3.10/site-packages/keras/saving/legacy/serialization.py\", line 557, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'smape'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "mape_pred = transfer_save(10,'../task_transfer_ele_24/train7/mape_models_G/mape',True, 'mape',1000,8,30,0.00005)\n",
    "mae_pred = transfer_save(10,'../task_transfer_ele_24/train7/mae_models_G/mae',True, 'mae',1000,8,30,0.00005)\n",
    "mse_pred = transfer_save(10,'../task_transfer_ele_24/train7/mse_models_G/mse',True, 'mse',1000,8,30,0.00005)\n",
    "smape_pred = transfer_SMAPE(10,'../task_transfer_ele_24/train7/smape_models_G/smape',True, 'smape',1000,8,30,0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9260ad7-f452-46b8-bee4-bbe17f19cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_1.h5\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "######################################################## fitted 1\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_2.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 2\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_3.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 3\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_4.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 4\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_5.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 5\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_6.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 6\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_7.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 7\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_8.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 8\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_9.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 9\n",
      "../task_transfer_ele_24/train7/smape_models_G/smape_model_G_10.h5\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "######################################################## fitted 10\n"
     ]
    }
   ],
   "source": [
    "smape_pred = transfer_SMAPE(10,'../task_transfer_ele_24/train7/smape_models_G/smape',True, SMAPE(),1000,8,30,0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eca183b8-2e10-4729-acea-3e36a61ac7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.986486223308427, 0.2770277349331334)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mase_pred),np.array(mape_pred),np.array(mae_pred),np.array(mse_pred),np.array(smape_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2622f647-8322-406d-af47-61e89bd681b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.972275383459936, 0.282172983297178)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mase_pred),np.array(mae_pred),np.array(mse_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4147ec0-4875-4fe9-98f9-2751533c1a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.978242221739376, 0.2800148375403604)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mase_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab1e3f64-00a3-4c64-a41b-920a72cbf520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.673584727598899, 0.0063319114343457494)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mape_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36086c09-242b-48b1-b51b-0a4cfc261a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.710211224007162, -0.00930370596154173)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(smape_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b5151c1-3af5-4133-b8df-257f214c8d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9815612345780544, 0.27881298169924695)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mae_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e93bd38-b5e0-4859-ad67-d827477c9e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.981429781796751, 0.2788606014488112)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_G = np.concatenate([np.array(mse_pred)])\n",
    "\n",
    "fin_pred_G = np.median(concat_G,axis=0)\n",
    "\n",
    "np.sqrt(mean_squared_error(test_y.flatten(),fin_pred_G.flatten())),r2_score(test_y.flatten(),fin_pred_G.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1ebab7-ad59-433f-8bf6-fa1479bea6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(fin_pred_G.reshape(-1,24)).to_csv(\"../result7_new/NBEATs_G_T/pred_mid_G.csv\")\n",
    "for i in range(10):\n",
    "    pd.DataFrame(concat_G[i].reshape(-1,24)).to_csv(f\"../result7_new/NBEATs_G_T/pred_G{i}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
